[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Quick access to frequently used links. Add/edit freely."
  },
  {
    "objectID": "resources.html#daily-weekly",
    "href": "resources.html#daily-weekly",
    "title": "Resources",
    "section": "Daily / Weekly",
    "text": "Daily / Weekly\n\nLatent Space\nSam Altman’s Blog\nNews from Wes Roth"
  },
  {
    "objectID": "resources.html#llms-genai",
    "href": "resources.html#llms-genai",
    "title": "Resources",
    "section": "LLMs / GenAI",
    "text": "LLMs / GenAI\n\nAndrej Karpathy’s Youtube\n3 Blue 1 Brown Youtube\nHugging Face Youtube"
  },
  {
    "objectID": "resources.html#python-data",
    "href": "resources.html#python-data",
    "title": "Resources",
    "section": "Python / Data",
    "text": "Python / Data"
  },
  {
    "objectID": "resources.html#research-learning",
    "href": "resources.html#research-learning",
    "title": "Resources",
    "section": "Research & Learning",
    "text": "Research & Learning"
  },
  {
    "objectID": "resources.html#tools-platforms",
    "href": "resources.html#tools-platforms",
    "title": "Resources",
    "section": "Tools & Platforms",
    "text": "Tools & Platforms\n\nRun local AI models like gpt-oss, Qwen, Gemma, DeepSeek and many more on your computer, privately and for free\nlmarena\nInference Playground"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Notes, experiments, and updates.\nWelcome to my blog. I write about frontend engineering, product development, and building with AI.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nAn llm that tweaks itself - Evolutionary based LLM\n\n\nOct 21, 2025\n\n\n\n\n\n\nWhy I’m Building an AI Engineer Curriculum in Public\n\n\nOct 21, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-10-21-brain.html",
    "href": "blog/posts/2025-10-21-brain.html",
    "title": "An llm that tweaks itself - Evolutionary based LLM",
    "section": "",
    "text": "The core idea is an LLM that can tweak its own parameters. An LLM that has core drivers, motivations, and goals."
  },
  {
    "objectID": "learning-journey/2025-10-23.html",
    "href": "learning-journey/2025-10-23.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "Intermediate Python - Frontend Masters - Nina Zakharenko"
  },
  {
    "objectID": "learning-journey/2025-10-20.html",
    "href": "learning-journey/2025-10-20.html",
    "title": "Transformers and Attention",
    "section": "",
    "text": "This video is very information dense and heavy and far beyond currrent full understanding. Just watch to familiarise yourself with the concepts loosely.\n\n\nAttention pattern, Weighted Sum, Dot Product, Sequence of Vectors, Multi-Headed Attention, Multilayer Perceptron\nContext is revelant to updating meanings.\nWhy is this technique as effective as it is? One lesson is that scale alone matters. Simply making things bigger and simply giving them more training data can sometimes give qualititivie improvements to the model performance.\nFor a given size of the model, for a given amount of training that you do — what’s the cost function going to look like? If that cost function is going down, that typically responds to improvements in the model performance that are qualitativley visable - a chatbot that behaves better.\nThe attention mechanism allows things to talk to eachother without sequential processing - it means they can take in the whole text passage at once, letting all the different embeddings talk to eachother as it does. This way it can effectively do more floating point operations in a given amount of time.\nYou can alos train on a huge amount of data that doesnt require human labelling. You can just give it massive amounts of data wihtout being restrained by human feedback.\nYou can Tokenise essentially anything and then embed those as vectors. This means you can have lots of distinct data types working in conjunction with each other.\n\n\n\nUnder the hood of ChatGP(Transformer)\nEach Transformer performs a set of fixed matrix operations on an input matrix of data and typically returns an output matrix of the same size.\nTo figure out what it’s going to say next, ChatGPT breaks apart what you ask it into words and word fragments, maps each of these to a vector and stacks all these vectors together into a matrix.\nThis matrix is then passed into the first transformer block which returns a new matrix of the same size. This operation is repeated again and again.\nThe next word is the final column of its final output matrix mapped from a vector back to text.\nThen this final word is appended to the end of the input sequence and the process is repeated again and again. With one new column appended to the end of the input matrix each time.\nChatGPT slowly morphs the input you give it into the output it returns.\nConvolutional Blocks, Kernel, Activation Maps. Activations maps are stacked together to form a Tensor that become the input to the Convolutional Compute Block.\nDot Product can be thought of as a similarity score.\n\n\n\nConvolutional Block Architecture\n\n\nLatent/Embedding Space is the space of all the possible vectors that the model can output. Distance but Directionality in these latent spaces is meaningful.\nTo de-age an image for example, you can use the latent space to find the closest image to the original image and then apply a transformation (literally moving the point in the age direction) to the latent space to make the image younger/older. Then mapping the modified vector back into an image."
  },
  {
    "objectID": "learning-journey/2025-10-13.html",
    "href": "learning-journey/2025-10-13.html",
    "title": "Practical Python: Beginner’s Guide",
    "section": "",
    "text": "Frontend Masters — Practical Guide to Python (full course) by Nina Zakharenko: https://frontendmasters.com/courses/practical-python/\n\n\nNumbers and booleans\n\na = 12\nb = 3.5\ntotal = a + b\nis_equal = (a == 2)\ntruthy = True and (1 &lt; 2)\nfalsy = False or (2 &gt; 5)\nnot_val = not False\n\nprint(a, type(a))\nprint(b, type(b))\nprint(total)\nprint(is_equal, truthy, falsy, not_val)\n\n12 &lt;class 'int'&gt;\n3.5 &lt;class 'float'&gt;\n15.5\nFalse True False True\n\n\n\n\nStrings and f-strings\n\nname = \"Charlotte\"\nlang = \"Python\"\nmsg = f\"Hi {name}, welcome to {lang}!\"\n\nprint(name.upper())\nprint(len(name))\nprint(\"thon\" in lang)\nprint(msg)\n\nCHARLOTTE\n9\nTrue\nHi Charlotte, welcome to Python!\n\n\n\n\nLists\n\nnums = [1, 2, 3]\nnums.append(4)\nsliced = nums[1:3]\ndoubled = [x * 2 for x in nums]\n\nprint(nums)\nprint(sliced)\nprint(doubled)\n\n[1, 2, 3, 4]\n[2, 3]\n[2, 4, 6, 8]\n\n\n\n\nTuples\n\nt = (1, \"a\", True)\nsingle = (42,)\npacked = 1, 2  # tuple without parentheses\nx, y = (10, 20)  # unpacking\n\nprint(t)\nprint(single)\nprint(packed)\nprint(x, y)\n\n# t[0] = 99  # TypeError: 'tuple' object does not support item assignment\n\n(1, 'a', True)\n(42,)\n(1, 2)\n10 20\n\n\n\n\nDictionaries\n\nuser = {\"name\": \"Charlotte\", \"role\": \"AI Engineer\"}\nuser[\"city\"] = \"Ipswich\"\n\nprint(user[\"name\"])       # indexing by key\nprint(\"role\" in user)      # membership check on keys\nprint(user)\n\nCharlotte\nTrue\n{'name': 'Charlotte', 'role': 'AI Engineer', 'city': 'Ipswich'}\n\n\n\n\nSets\n\nnames = [\"alice\", \"bob\", \"alice\"]\ns = set(names)\ns.add(\"carol\")\n\nprint(s)\nprint(\"alice\" in s)\n\n{'alice', 'carol', 'bob'}\nTrue\n\n\n\n\nHash function and hashability\n\nprint(hash((\"a\", 1)))  # tuples are hashable if their items are hashable\nprint(hash(\"abc\"))\n\n# hash([1, 2])   # TypeError: unhashable type: 'list'\n# hash({\"k\": 1}) # TypeError: unhashable type: 'dict'\n\n8263313310410063139\n8525419185965012929\n\n\n\n\nLogic: and / or / in\n\nprint(True and False)\nprint(True or False)\nprint(False or 0)\nprint(0 or \"fallback\")\nprint(\"x\" and \"y\")      # returns last truthy operand\nprint(\"py\" in \"python\")  # substring membership\n\nFalse\nTrue\n0\nfallback\ny\nTrue\n\n\n\n\nFunctions\n\ndef my_function(x=4):\n   return x + 2\n\nmy_function()\n\n6\n\n\n\ndef my_other_function(x):\n   return x * 2\n\nmy_other_function(21312)\n\n42624\n\n\n\ndef another_cat(x, y, z=12):\n   return z + (x + y)\n\nanother_cat(1, 2)\n\n15\n\n\n\n\nConditionals\n\ndef my_dogs(x, one, two):\n    if x == 2:\n        return f\"my doggos are {one} and {two}\"\n    elif x &gt; 2:\n        return \"Soon! soon we will have them all!\"\n    else: \n        return \"Got no doggos\"\n\nmy_dogs(1, \"Annie\", \"Anubis\")\n\n'Got no doggos'\n\n\n\ndef fizzbuzz(number):\n    if (number % 3 == 0) and (number % 5 == 0):\n        print(\"fizz\")\n    else: print(\"buzz\")\n\nfizzbuzz(15)\nfizzbuzz(5)\n\nfizz\nbuzz\n\n\n\n\nLoops\n\nfamily = [\"Annie\", \"Anubis\", \"Alex\", \"Charlotte\"]\n\nfor family_member in family:\n    print(f\"My name is {family_member}!\")\n\nprint(f\"outside of the loop {family_member}\")\n\nlist(enumerate(family))\n\nMy name is Annie!\nMy name is Anubis!\nMy name is Alex!\nMy name is Charlotte!\noutside of the loop Charlotte\n\n\n[(0, 'Annie'), (1, 'Anubis'), (2, 'Alex'), (3, 'Charlotte')]\n\n\nenumerate returns list of tuples , first item index, second item the value\n\ncolours = [\"Red\", \"Yellow\", \"Pink\", \"Green\", \"Orange\", \"Purple\", \"Blue\"]\n\nfor index, colour in enumerate(colours):\n    print(f\"this is the greatest colour {colour} number {index}\")\n\nthis is the greatest colour Red number 0\nthis is the greatest colour Yellow number 1\nthis is the greatest colour Pink number 2\nthis is the greatest colour Green number 3\nthis is the greatest colour Orange number 4\nthis is the greatest colour Purple number 5\nthis is the greatest colour Blue number 6\n\n\n\nconcepts_to_learn = {\n    \"Python\": \"Language\",\n    \"TensorFlow\": \"Execution Env\",\n    \"Pytorch\": \"differnt exectuion place\",\n    \"Deep Learning\": \"Neural networks and that\"\n}\n\nfor foo in concepts_to_learn:\n    print(foo)\n\nPython\nTensorFlow\nPytorch\nDeep Learning\n\n\n\nconcepts_to_learn.items()\n\ndict_items([('Python', 'Language'), ('TensorFlow', 'Execution Env'), ('Pytorch', 'differnt exectuion place'), ('Deep Learning', 'Neural networks and that')])\n\n\n\nfor key, value in concepts_to_learn.items():\n    print(key) \n    print(\"----\")\n    print(value)\n\nPython\n----\nLanguage\nTensorFlow\n----\nExecution Env\nPytorch\n----\ndiffernt exectuion place\nDeep Learning\n----\nNeural networks and that\n\n\n\nx = 0\nwhile x &lt; 5:\n    print(x)\n    x += 1\n\n0\n1\n2\n3\n4\n\n\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\n\ndef return_target(target=\"Jeremy\"):\n    for name in names:\n        print(name)\n        if name == target:\n            print(f\"we found {target}!\")\n            return name\n\n\n\nList comprehensions\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\nmy_list = []\n\nfor name in names:\n    my_list.append(len(name))\n\nprint(\"First way: \", my_list)\n\nprint(\"Shorter way:\", [len(name) for name in names])\n\nFirst way:  [4, 3, 6, 6, 5]\nShorter way: [4, 3, 6, 6, 5]\n\n\n\nnums = [0, 1, 2, 3, 4]\n\n[num * 2 for num in nums] \n\n[0, 2, 4, 6, 8]\n\n\n\n\nSlicing\n\nmy_cake = \"Hey this is a big cake!\"\nmy_cake[14:22]\n\nmy_cake[:18]\nmy_cake[19:]\nmy_cake[-1]\n\n'!'\n\n\n\n\nfiles - reading, writing, appending, and JSON\nBelow are executable examples that will run in this page’s kernel. They demonstrate different open() modes and working with a small JSON file stored alongside this page at learning-journey/data/example.json.\n\n# Write: creates or truncates file\nwith open(\"my_file.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"first line\\n\")\n    f.write(\"second line\\n\")\n\n# Append: adds to end of file\nwith open(\"my_file.txt\", \"a\", encoding=\"utf-8\") as f:\n    f.write(\"appended line\\n\")\n\n# Read entire file\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    contents = f.read()\ncontents\n\n'first line\\nsecond line\\nappended line\\n'\n\n\n\n# Read line-by-line\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        print(line.rstrip())\n\nfirst line\nsecond line\nappended line\n\n\n\n# Explicit open/close (less preferred vs. context manager)\nf = open(\"my_file.txt\", \"r\", encoding=\"utf-8\")\ntry:\n    print(f.readline().rstrip())\nfinally:\n    f.close()\n\nfirst line\n\n\n\n# pathlib usage\nfrom pathlib import Path\n\npath = Path(\"my_file.txt\")\npath.write_text(\"overwritten via pathlib\\n\", encoding=\"utf-8\")\nprint(path.read_text(encoding=\"utf-8\"))\n\noverwritten via pathlib\n\n\n\n\n\nClasses\n\nclass Car:\n    runs = True\n\n    def start(self):\n        if self.runs:\n            print(\"The car starts.\")\n        else:\n            print(\"The car is broken.\")\n\nmy_car = Car()\nmy_car.start()\nmy_car.runs = False\nmy_car.start()\n\nmy_other_car = Car()\nmy_other_car.start()\n\nThe car starts.\nThe car is broken.\nThe car starts.\n\n\n\n\nisinstance\n\nprint(isinstance(my_car, Car))\nprint(isinstance(my_car, str))\nprint(isinstance(\"Hallo there\", str))\nprint(isinstance(12, int))\n\nTrue\nFalse\nTrue\nTrue\n\n\n\n\ninitializing classes\n\nclass Supersupercar:\n    runs = True\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def start(self):\n        if self.runs:\n            print(f\"The {self.make} {self.model} {self.year} starts.\")\n        else:\n            print(f\"The {self.make} {self.model} {self.year} is broken.\")\n\nmy_car = Supersupercar(\"Toyota\", \"Corolla\", 2020)\nmy_car.start()\n\nmy_better_car = Supersupercar(\"Mustang\", \"GT\", 2025)\nmy_better_car.start()\n\nThe Toyota Corolla 2020 starts.\nThe Mustang GT 2025 starts.\n\n\n\n\n\nMustang GT\n\n\n\n\ninheritance\n\nclass Mustang(Supersupercar):\n    def __init__(self, make, model, year, color):\n        super().__init__(make, model, year)\n        self.color = color\n\nmy_jaguar = Mustang(\"Jaguar\", \"2027\", 2027, \"Green\")\nmy_jaguar.start()\n\nThe Jaguar 2027 2027 starts.\n\n\n\n\n\nJaguar 2027\n\n\n\n\nExceptions\n\ntry:\n    print(10 / 0)\nexcept ZeroDivisionError:\n    print(\"You can't divide by zero!\")\n\ntry:\n    print(10 / 2)\nexcept ValueError:\n    print(\"You can divide by two!\")\n\nYou can't divide by zero!\n5.0\n\n\n\n\nrequests\n\n# import requests\n\n# response = requests.get(\"https://ur-api.....\")\n\n# print(response.status_code)\n# print(response.json())"
  },
  {
    "objectID": "learning-journey/2025-10-14.html",
    "href": "learning-journey/2025-10-14.html",
    "title": "Intro to Large Language Models",
    "section": "",
    "text": "Mathematically there is a very close relationship between prediction and compression\nparameters bytes artifact Neural Network compressed into the weights sample model inference feeding back in perform inference\nRun the neural network - or as we say perform inference\ndreaming/mimicking/hallucinating\nIt’s parroting the training set distribution\nlossy compression of the internet\n\nTransformer Neural Net Architecture\n\n\n\nTransformer Neural Net Architecture - https://deeprevision.github.io/posts/001-transformer/\n\n\n100 billion parameters are dispersed throughout the entire Neural Net and all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task - BUT we don’t actually know what these 100 billion parameters are doing.\nWe can measure that it’s getting better to next word prediction, but we don’t know how these parameters collaborate to actually perform that\nknowledge isnt stored in traditional sense\nAll we can really measure is whether it works or not and the probability that it works.\ncome from a long process of optimization interpretability or mechanistic interpretability empirical artifacts\nWe can give them some inputs and can measure the outputs. We can measure their behaviour - this requires corresponsively sophisticated evaluations to work with these models because they’re mostly empirical (evidence based).\n\n\nTraining the assistant - Finetuning\nQuality is prefered over quantity in this stage.\nFine tuning creates an Assistant Model\nThis assisstant model now suscribes to the form of its new training documents.\nRetrieval Augmented Generation - ChatGPT can browse the files that you upload and can use them as reference inforamtion for creating its answers\nThink of LLMs as the kernel process of an emerging operating system - this process is coordiatng many different processes be they memory or computational tools for problemsolving.\nContext window suffix jailbreaking prompt injection Poisioned model/Corrupted model"
  },
  {
    "objectID": "learning-journey/2025-10-17.html",
    "href": "learning-journey/2025-10-17.html",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "",
    "text": "🍷 FineWeb: decanting the web for the finest text data at scale"
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-1.-pretraining",
    "href": "learning-journey/2025-10-17.html#step-1.-pretraining",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 1. Pretraining",
    "text": "Step 1. Pretraining\nDownload and preprocess the internet. We want a huge quantity of high quality documents, with large diversity. Huggingface made FineWeb which is a new, large-scale (15-trillion tokens, 44TB disk space) dataset for LLM pretraining. They used Common Crawl as their source of data. OpenAI and Anthrophic crawl themselves OpenAI Crawlers, heres Claudebot\n\n\nURL filtering - Firstly data needs filtering. Loads of websites are not included from categories like adult stuff. BLocklists are lists of urls to block.\ntext extraction - Raw HTML is what the crawlers save. We only want the text content.\nlanguage filtering - There’s a guess (using a classifier) which rules out non english pages, keeping pages that score above 65% confidence it is English.\ngopher filtering Gopher is an LLM Transformer model. The architecture is same as GPT2. It uses a huge dataset. Templates were used to prompt the model to try to stop biases in the data, sentiment analysers were used to stop biased content.\n\n\n\n\nMinhash deduplication - This is a technique to remove duplicate documents from the dataset. It’s a hashing technique that is used to identify duplicate documents.\nC4 filters\nCustom fitlers\nPII Removal - Personal Identifiable Information is detected and removed like addresses, phone numbers, emails, etc."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-2.-tokenization",
    "href": "learning-journey/2025-10-17.html#step-2.-tokenization",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 2. Tokenization",
    "text": "Step 2. Tokenization\nThe way the technology works for these neural nets is that they expect a one dimesional sequence of symbols. They want a finite set of symbols that are possible. We have to decide what the symbols are, then have to represent our data as a one dimensional sequence of symbols.\n\nText → bytes → bits (roundtrip, compact and readable)\n\ntext = \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n\n# Text → bytes (UTF-8)\nutf8_bytes = text.encode(\"utf-8\")\nprint(\"chars:\", len(text))\nprint(\"bytes:\", len(utf8_bytes))\nprint(\"first 16 bytes (int):\", list(utf8_bytes[:16]))\nprint(\"first 16 bytes (hex):\", utf8_bytes[:16].hex())\n\n# Bytes → bits (grouped in 8)\ndef bytes_to_bits(data: bytes) -&gt; str:\n    return \" \".join(f\"{b:08b}\" for b in data)\n\nbits = bytes_to_bits(utf8_bytes)\npreview_bits = \" \".join(bits.split(\" \")[:12])  # first 12 bytes as bits\nprint(\"bits (first 12 bytes):\", preview_bits, \"...\")\nprint(\"total bits:\", len(utf8_bytes) * 8)\n\n# Bits → bytes → text (roundtrip)\ndef bits_to_bytes(bits_str: str) -&gt; bytes:\n    cleaned = bits_str.replace(\" \", \"\")\n    assert len(cleaned) % 8 == 0\n    return bytes(int(cleaned[i:i+8], 2) for i in range(0, len(cleaned), 8))\n\nroundtrip_bytes = bits_to_bytes(bits)\nprint(\"roundtrip matches bytes:\", roundtrip_bytes == utf8_bytes)\nprint(\"decoded:\", roundtrip_bytes.decode(\"utf-8\"))\n\nchars: 167\nbytes: 167\nfirst 16 bytes (int): [73, 39, 109, 32, 101, 110, 106, 111, 121, 105, 110, 103, 32, 108, 101, 97]\nfirst 16 bytes (hex): 49276d20656e6a6f79696e67206c6561\nbits (first 12 bytes): 01001001 00100111 01101101 00100000 01100101 01101110 01101010 01101111 01111001 01101001 01101110 01100111 ...\ntotal bits: 1336\nroundtrip matches bytes: True\ndecoded: I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\n\n\n\n\nPer-character view (code point → UTF‑8 bytes)\nWe want more symbols and shorter sequences. Let’s compress the binary sequence. A group of 8 bits are a byte.\n\nsample = text[:8]\nprint(f\"{'char':&lt;6}{'codepoint':&lt;12}{'hex':&lt;20}{'bin'}\")\nfor ch in sample:\n    b = ch.encode('utf-8')\n    hx = \" \".join(f\"{x:02x}\" for x in b)\n    bn = \" \".join(f\"{x:08b}\" for x in b)\n    print(f\"{repr(ch):&lt;6}{ord(ch):&lt;12}{hx:&lt;20}{bn}\")\n\nchar  codepoint   hex                 bin\n'I'   73          49                  01001001\n\"'\"   39          27                  00100111\n'm'   109         6d                  01101101\n' '   32          20                  00100000\n'e'   101         65                  01100101\n'n'   110         6e                  01101110\n'j'   106         6a                  01101010\n'o'   111         6f                  01101111\n\n\n\n\nByte Pair Encoding (BPE) — learn merges and tokenize\n\n\nWe’ll train a tiny BPE on a short corpus, learn a few merges, then tokenize a sentence.\n\nfrom collections import Counter\n\ncorpus = (\n    \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n).lower()\n\ndef build_vocab(text: str):\n    vocab = Counter()\n    for word in text.split():\n        symbols = tuple(list(word) + ['&lt;/w&gt;'])\n        vocab[symbols] += 1\n    return vocab\n\ndef get_stats(vocab: Counter):\n    pairs = Counter()\n    for symbols, freq in vocab.items():\n        for a, b in zip(symbols, symbols[1:]):\n            pairs[(a, b)] += freq\n    return pairs\n\ndef merge_vocab(pair, vocab: Counter):\n    a, b = pair\n    merged = Counter()\n    for symbols, freq in vocab.items():\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == (a, b):\n                new.append(a + b)\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        merged[tuple(new)] += freq\n    return merged\n\nvocab = build_vocab(corpus)\nmerges = []\nfor _ in range(20):  # learn up to 20 merges\n    stats = get_stats(vocab)\n    if not stats:\n        break\n    best = max(stats, key=stats.get)\n    merges.append(best)\n    vocab = merge_vocab(best, vocab)\n\nprint(\"top merges:\", merges[:10])\n\nrank = {pair: i for i, pair in enumerate(merges)}\n\ndef bpe_tokenize(word: str):\n    symbols = list(word) + ['&lt;/w&gt;']\n    while True:\n        pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols)-1)]\n        ranked = [(rank.get(p, 1e9), p) for p in pairs]\n        best_rank, best_pair = min(ranked, default=(1e9, None))\n        if best_pair is None or best_rank == 1e9:\n            break\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == best_pair:\n                new.append(symbols[i] + symbols[i+1])\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        symbols = new\n    return [s for s in symbols if s != '&lt;/w&gt;']\n\nsentence = \"Viewing single post from Spoilers of the week Lil\".lower()\nchar_tokens = sum((list(w) for w in sentence.split()), [])\nbpe_tokens = []\nfor w in sentence.split():\n    bpe_tokens.extend(bpe_tokenize(w))\n\nprint(\"char-level token count:\", len(char_tokens))\nprint(\"bpe token count:\", len(bpe_tokens))\nprint(\"bpe tokens (first 30):\", bpe_tokens[:30])\n\ntop merges: [('t', '&lt;/w&gt;'), ('n', 'd'), ('i', \"'\"), ('m', '&lt;/w&gt;'), ('i', 'n'), ('g', '&lt;/w&gt;'), ('a', 'r'), ('e', '&lt;/w&gt;'), (\"i'\", 'm&lt;/w&gt;'), ('in', 'g&lt;/w&gt;')]\nchar-level token count: 41\nbpe token count: 36\nbpe tokens (first 30): ['v', 'i', 'e', 'w', 'ing&lt;/w&gt;', 's', 'in', 'g', 'l', 'e&lt;/w&gt;', 'p', 'o', 's', 't&lt;/w&gt;', 'f', 'r', 'o', 'm&lt;/w&gt;', 's', 'p', 'o', 'i', 'l', 'e', 'r', 's&lt;/w&gt;', 'o', 'f', 'the&lt;/w&gt;', 'w']\n\n\nWe mint a symbol for each unique byte pair in the corpus. There are 100,277 symbols in GPT4\n\nGPT2 Tokenizer\n\n\n\n\nTikTokenizer\n\n\nThis token sequence is what GPT4 will ‘see’ the text as."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-3-neural-network-training",
    "href": "learning-journey/2025-10-17.html#step-3-neural-network-training",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 3: Neural Network Training",
    "text": "Step 3: Neural Network Training\nNow we are trying to predict the next token in the sequence. Currently there are 100,277 probabilities for the next token. The neural network is going to output exactly 100,277 numbers, and of those numbers, correspond to the probablility of that token as coming next in the sequence\nIn the beginning the Nural Network is randomly initialised, random probabilities. We’ve sampled this window from our dataset.\nWe know the correct next token for this sentence, so we need a mathematical process to update the weights on the network - tuning it. (Making the probability of the correct next token as high as possible, and making the other potential answers lower.)\nWe mathematically adjust the neural network so that the correct answer has a slightly higher probability.\ninput sequence tokens\niteratively updating the neural network = training the neural network."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#visualization-of-llm-in-3d",
    "href": "learning-journey/2025-10-17.html#visualization-of-llm-in-3d",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Visualization of LLM in 3D",
    "text": "Visualization of LLM in 3D\nTry not to think of these LLM neurons like the ones in our brain, our biological ones have complex dynamical processes that have memory. There’sno memory in LLM neurons, it’s stateless input and output.\nThe LLM in basic terms is a mathematical function. It it parameterised by some fixed set of parameters (85,584) it is a way of transforming inputs to outputs as we twiddle the parameters we are getting different kinds of predictions, and then we need to find a good setting of these parameters so they match up with the patterns seen in the training set"
  },
  {
    "objectID": "learning-journey/index.html",
    "href": "learning-journey/index.html",
    "title": "AI Engineer Learning Journey",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAi Engineering\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI with Python and PyTorch — Second Edition\n\n\n\npython\n\npytorch\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\ntransformers\n\n\n\nStudy outline, exercises, and examples based on Babcock & Bali (O’Reilly, 2025).\n\n\n\n\n\nOct 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\nconvolutional-neural-networks\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Python\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we mean by a network ‘learning’ is the minimisation of the cost function\n\n\n\nneural-networks\n\nmachine-learning\n\ndeep-learning\n\n\n\nGetting through my tabs\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers and Attention\n\n\n\ntransformers\n\nattention\n\n\n\n\n\n\n\n\n\nOct 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into LLMs like ChatGPT\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Python: Beginner’s Guide\n\n\n\npython\n\ndata-structures\n\ntuples\n\nsets\n\ndictionaries\n\nfundamentals\n\n\n\nsets, tuples, dictionaries, mutability & hashing.\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learning-journey/2025-10-22.html",
    "href": "learning-journey/2025-10-22.html",
    "title": "What we mean by a network ‘learning’ is the minimisation of the cost function",
    "section": "",
    "text": "Neurons , Activation, Layer, Hidden Layers, Parameters\na Weight is assigned to each connection between neurons. These weights are floating point numbers. Then take all the activations from the previous layer and compute their weighted sum according to the weights.\nWe want the Activations to be a value between 0 and 1. This is called a sigmoid function.\n\nThe Activation of the Neuron is a measure of how positive the weighted sum is.\n\nWe also want a bias for inactivity. (bias - how high the weighted sum needs to be before the neuron is meaningfully active)\n\nLearning: Getting the computer to find a valid settings for all the weights and biases so that it will actually solve the problem at hand\n\nVector, Matrix, Transistion of Activations\nTaking the Weighted sum of the Activations in the first layer according to these weights corresponds to one of the turns in a Matrix Vector Product\nThink of Neurons as Functions that take in the outputs of all the neurons of the previous layer and spits out a number between 0 and 1. The entire network is a complex function that is a composition of many simple functions.\nThe Network learns the appropriate weights and biases to solve the problem just by looking at data.\nReLU is used now moreso than Sigmoid because its much easier to train. Rather than a wavy line its a straight line which reflects the idea ‘is it active or not’ rather than warmth or coldness. If the value passes a certain threshold it is active, otherwise it is not. - ReLU is Rectified Linear Unit, a simplification\n\n\nGradient Descent\nA Cost Function is how far off (in values) the prediction is from the actual value. Average cost - a measure for how bad the network performs.\nThe Cost Function takes the weights and biases as its input, it outputs a single number (the cost), and the way its defined depends on the networks behaviour over thousands of pieces of training data.\n\nFinding an input that minimises the cost function is called Gradient Descent. Remember it could be 13,394 inputs… how do you find the input value that minimises the output value of the cost function.\nLocal minima = doable. Global minima = crazy hard.\n\nGradient - the direction of steepest increase\nGradient Descent - Robert Constable\n\nGradient descent is an iterative optimisation algorithm, which is used in machine learning to train models, by finding the parameters which minimise a cost function. Gradient descent is essential where the exhaustive calculation of best parameters would be unfeasible, for example in the training of neural networks.\n\nWhat we mean by a network ‘learning’ is the minimisation of the cost function\nArtificial neurons have continously ranging neuron activations (0.45….0.93). They are not binary (0, 1).\nGradient Descent - A process of repeatedly nudging an input of a function by some multiple of the negative gradient is called Gradient Descent. It is a way to converge towards some local minimum of a cost function.\nThe sin tells us if the number should be nudged up or down the gradient.\nThe network itself is a function with inputs and outputs, defined in terms of weighted sums:\nFor example:\n\nNeural Network Function\n\ninput: 784 numbers (pixels)\noutput: 10 numbers (digits)\nparameters: 13,002 weights/biases\n\n\n\nCost Function\n\ninputs: 13,002 weights/biases\noutput 1 number (the cost)\nparameters: many, many training examples\n\n\nThe Gradient of the Cost Function tells us what nudges to all the weights/biases cause the fastest change to the value of the cost function. Which changes to which weights matter the most."
  },
  {
    "objectID": "learning-journey/2025-10-24.html",
    "href": "learning-journey/2025-10-24.html",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-24.html#learning-outcomes",
    "href": "learning-journey/2025-10-24.html#learning-outcomes",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-24.html#todays-syllabus",
    "href": "learning-journey/2025-10-24.html#todays-syllabus",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/2025-10-24.html#resources",
    "href": "learning-journey/2025-10-24.html#resources",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Resources",
    "text": "Resources\n\nGenerative AI with Python and PyTorch — Second Edition, Joseph Babcock & Raghav Bali, 2025: https://learning.oreilly.com/library/view/generative-ai-with/9781835884447/"
  },
  {
    "objectID": "learning-journey/2025-10-24.html#exercises",
    "href": "learning-journey/2025-10-24.html#exercises",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/2025-10-24.html#readiness-checklist",
    "href": "learning-journey/2025-10-24.html#readiness-checklist",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/2025-10-24.html#examples",
    "href": "learning-journey/2025-10-24.html#examples",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['the', 'the', 'cat', 'cat', 'cat']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A roadmap to becoming an AI Engineer",
    "section": "",
    "text": "This is a living, outcome-driven curriculum for becoming an AI Engineer. The dates are simply the day I began that course. Some parts can take a long time to complete, often I went back and forth into different topics to get a better understanding.\nCheck out what I’ve been learning this week, or change the ordering to ‘Oldest’ to start from scratch too.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAI Engineer Learning Journey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAi Engineering\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI with Python and PyTorch — Second Edition\n\n\n\npython\n\npytorch\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\ntransformers\n\n\n\nStudy outline, exercises, and examples based on Babcock & Bali (O’Reilly, 2025).\n\n\n\n\n\nOct 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\nconvolutional-neural-networks\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Python\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we mean by a network ‘learning’ is the minimisation of the cost function\n\n\n\nneural-networks\n\nmachine-learning\n\ndeep-learning\n\n\n\nGetting through my tabs\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers and Attention\n\n\n\ntransformers\n\nattention\n\n\n\n\n\n\n\n\n\nOct 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into LLMs like ChatGPT\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Python: Beginner’s Guide\n\n\n\npython\n\ndata-structures\n\ntuples\n\nsets\n\ndictionaries\n\nfundamentals\n\n\n\nsets, tuples, dictionaries, mutability & hashing.\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-10-21-first-post.html",
    "href": "blog/posts/2025-10-21-first-post.html",
    "title": "Why I’m Building an AI Engineer Curriculum in Public",
    "section": "",
    "text": "I want to retrain to make AI systems\nWhy a change in career? I’ve been finding fullstack engineering un-impactful and un-fulfilling. Rendering out data on a page; designing that page to pixel perfection, and getting millions of clicks on buttons I have created doesn’t feel awesome to me.\nI’m so glad I went down this route and tried it out… however when I tell people what they should do with their lives, I tell them to study AI and get involved.\nIt is the next technological revolution and I want to be part of it. I like how it’s the closest thing to understand the human brain and trying to emulate it. I like how we become gods in a way of our own simulation. - It’s becoming clearer that we are within a simulation, so I want to be a part of the people able to bend the matrix and understand it.\nAI isn’t only interesting for making lots of money. It could do so much for us. Neuralink comes to mind. That’s directly some of the most incredible impactful outcomes I’ve seen in my life.\nIn another sense I want to get to see AGI. To understand the human mind, truly. I find human intelligence fascinating.\n\n\nMy Dream job\n\nTitle: AGI Engineer\nTeam: The Artificial General Intelligence (AGI) team\nWorking for a massive well known company like OpenAI, Meta, Amazon…\nprototype new technology\nWork on creating systems better than the human brain\nProven track record of designing, building, and shipping real-time ML products\nStrong foundation in signal processing, algorithms, and software engineering principles\n\n\nI need to find the most fascinating stage of training Neural Networks to me, then I’ll find the right specialisation.\n\n\n\nMust-Haves from listings\n\nComputer Science Degree or equivalent\nPossess strong programming skills in Python\nImplementing LLM finetuning algorithms, such as RLHF\nLarge scale LLM training\nExperience with open-source ML toolkits and frameworks (eg. PyTorch, TensorFlow, etc)\nPublished work on hallucination prevention, factual grounding, or knowledge integration in language models\nExperience with fact-grounding techniques\nBackground in developing confidence estimation or calibration methods for ML models\nA track record of creating and maintaining factual knowledge bases\nFamiliarity with RLHF specifically applied to improving model truthfulness\nWorked with crowd-sourcing platforms and human feedback collection systems\nExperience developing evaluations of model accuracy or hallucinations\nHave industry experience with language model finetuning and classifier training\nShow proficiency in experimental design and statistical analysis for measuring improvements in calibration and accuracy\nCare about AI safety and the accuracy and honesty of both current and future AI systems\nHave experience in data science or the creation and curation of datasets for finetuning LLMs\nAn understanding of various metrics of uncertainty, calibration, and truthfulness in model outputs\ndemonstrable track record of success in delivering new features and products\nCreating reliable, scalable, and high performance AI products\nKnowledge of design or architecture (design patterns, reliability and scaling) of new and existing systems experience\ndevelopment of techniques to minimize hallucinations and enhance truthfulness in language models\nDesign and implement novel data curation pipelines to identify, verify, and filter training data for accuracy given the model’s knowledge\nDevelop specialized classifiers to detect potential hallucinations or miscalibrated claims made by the mode\nCreate and maintain comprehensive honesty benchmarks and evaluation frameworks\nImplement techniques to ground model outputs in verified information, such as search and retrieval-augmented generation (RAG) systems\nDesign and deploy human feedback collection specifically for identifying and correcting miscalibrated responses\nDesign and implement prompting pipelines to generate data that improves model accuracy and honesty\nDevelop and test novel RL environments that reward truthful outputs and penalize fabricated claims\nCreate tools to help human evaluators efficiently assess model outputs for accuracy\nDesign, develop, and maintain tokenization systems used across Pretraining and Finetuning workflows\nOptimize encoding techniques to improve model training efficiency and performance\nCollaborate closely with research teams to understand their evolving needs around data representation\nBuild infrastructure that enables researchers to experiment with novel tokenization approaches\nImplement systems for monitoring and debugging tokenization-related issues in the model training pipeline\nCreate robust testing frameworks to validate tokenization systems across diverse languages and data types\nIdentify and address bottlenecks in data processing pipelines related to tokenization\nDocument systems thoroughly and communicate technical decisions clearly to stakeholders across teams\nWorking with machine learning data processing pipelines\nBuilding or optimizing data encodings for ML applications\nImplementing or working with BPE, WordPiece, or other tokenization algorithms\nPerformance optimization of ML data processing systems\nMulti-language tokenization challenges and solutions\nResearch environments where engineering directly enables scientific progress\nDistributed systems and parallel computing for ML workflows\nLarge language models or other transformer-based architectures (not required)\nProfiling our reinforcement learning pipeline to find opportunities for improvement Building a system that regularly launches training jobs in a test environment so that we can quickly detect problems in the training pipeline Making changes to our finetuning systems so they work on new model architectures Building instrumentation to detect and eliminate Python GIL contention in our training code Diagnosing why training runs have started slowing down after some number of steps, and fixing it Implementing a stable, fast version of a new training algorithm proposed by a researcher ##### Ideal working environment\nWork from home, flexible working hours\nLearning days and study days\nOpen sharing environment\n\n\nListings\n\nAnthropic - Machine Learning Systems Engineer, Research Tools\nAnthrophic - [Expression of Interest] Research Scientist/Engineer, Honesty\nAnthropic - Machine Learning Systems Engineer, RL Engineering\nAmazon - Software Development Engineer (ML), AGI Customization\nAmazon - Sr. Research Engineer, Machine Learning, AGI Foundations"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "charlotteai.engineer",
    "section": "",
    "text": "Hey there — I’m Charlotte, a Frontend‑focused Product Developer who loves building fast, elegant, and accessible web experiences. I dive deep, move quickly, and use strong workflows to ship pixel‑perfect UI.\nI started by asking “can I build this?” and dove head‑first into full‑stack projects. Today I lean frontend, collaborate closely with design, and sweat the details that make products feel alive.\nFocus areas - Pixel‑perfect UI and component systems - Rapid, feedback‑driven iteration - Performance, accessibility, and DX\n\n\nIpswich, UK\nLinkedIn · GitHub\n\n\n\n\nSkills\nCore: TypeScript, React, Next.js, JavaScript, UI/UX, Testing (Playwright)\nUI Systems: shadcn/ui, TailwindCSS, design systems, atomic design\nBackend/Infra: Node.js, GraphQL/REST, Serverless, AWS (S3, Lambda, DynamoDB, Amplify), Docker, Netlify\nCMS: Contentful, Payload CMS\nData/DB: MongoDB, SQL, PostgreSQL\nTooling: Git, Figma, Linear, Adobe Analytics, Prisma, Heroku, Railway\n\n\nExperience\nFrontend Engineer · Hargreaves Lansdown (03/2023 — current)\n- Led rebuild of main and dropdown menus on hl.co.uk with Next.js and Contentful\n- Built new Help Docs and News & Insights from scratch; promoted to Pensions team\n- Owned calculators and high‑traffic apps; integrated Adobe Analytics\n- Migrated UI to an internal library; TDD; sole manager of three repos\n- Mentored two apprentices; taught Playwright/Next.js; wrote technical docs\nFrontend Engineer · Munch (01/2021 — 01/2023)\n- Mobile‑first website builder with complex multi‑touch interactions\n- Rebuilt the entire menu system for responsiveness and configurability\n- Improved productivity via modular architecture and clear routing/naming\n- Migrated frontend from Craft.js to Google Web Stories in one weekend; desktop/mobile editor up within a week\nCPO · Kynk (04/2019 — 08/2020)\n- Product discovery and UX; user research into user stories and flows\n- Pitched scope and product vision to engineering team\n- Explored payments for creator‑safe platform\nWordPress Developer · Lella.co (01/2019 — 06/2019)\n- Built and maintained the site; advised on product positioning\n\n\nSelected projects\nMarketplace buildout — ultra‑fast marketplaces with non‑technical creator tools\nStack: Medusa.js + Next.js; Vendure; later Qwik; Docker; Railway; PostgreSQL\n- Rebuilt three times to optimize DX/perf; pitched to VCs; interviewed early users\nEngineer Learning Roadmap — curated learning hub\nPrior: Clerk auth, Next.js + TypeScript, Payload CMS (MongoDB, PlanetScale, Prisma)\nCurrent: Docusaurus; planned progression tracking and starter code\nSocial media prototype — deep dive first project\nGetStream‑based social feeds; Amplify, Serverless, Heroku, Mantine, Next.js\n- Solved media‑hosting costs via IPFS/web3.storage; working prototype with auth/feeds\nRealtime character rig — motion‑tracked 3D avatar\nUnity, Blender, Rokoko; custom retargeting, facial rig mapping, live web integration\n\n\nCurrently exploring\n\nAI companion to support autistic people (React/Next.js)\n\nIn‑browser OS (Tauri/React Native)\n\nAliveUI — rapidly composable UI library\n\nProduct planning with Figma and Linear\n\n\n\nGet in touch\nIf you’re building ambitious, design‑led products and want to ship fast with quality, let’s talk."
  },
  {
    "objectID": "resources.html#newsletters",
    "href": "resources.html#newsletters",
    "title": "Resources",
    "section": "Newsletters",
    "text": "Newsletters\n\nLatent Space\nnews.smol.ai"
  },
  {
    "objectID": "learning-journey/2025-10-23-cnn.html",
    "href": "learning-journey/2025-10-23-cnn.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Deep Learning CNN: Convolutional Neural Networks with Python AI Sciences - Published by Packt Publishing\n\n\nA CNN is for any structural data. Images, Video, Audio, Text, etc.\nColours in digital screens are represented as a combination of Red, Green, and Blue in different intensities. There are three image ‘Planes’ stacked ontop of eachother.\nHigh spectral images have more stacks ontop of the basic 3. (Geospacial images from satellites for example get height maps too)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nim = plt.imread('../images/owl.jpg')\nplt.imshow(im)\n\n\n\n\n\n\n\n\n\nim.shape # (height, width, channels)\n\n(4800, 7200, 3)\n\n\n\nR = im[:,:,0]\nG = im[:,:,1]\nB = im[:,:,2]\nplt.imshow(R)\nplt.imshow(G)\nplt.imshow(B)\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nplt.imshow(R, cmap='Reds')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nplt.imshow(R, cmap='gray')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nfig,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 15))\nax1.imshow(R, cmap='Reds')\nax1.axis('off')\nax2.imshow(G, cmap='Greens')\nax2.axis('off')\nax3.imshow(B, cmap='Blues')\nax3.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nmodified_im = im.copy()\nmodified_im[:,:,2] = 255 # Blue channel\nplt.imshow(modified_im)\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nmodified_im2 = im.copy()\nmodified_im2[1000:3600,1400:3600,0] = 0 # A position in the image\nmodified_im2[1000:3600,1400:3600,1] = 255\nmodified_im2[1000:3600,1400:3600,0] = 0\nplt.imshow(modified_im2)    \n\n\n\n\n\n\n\n\n\ngrayscale_im = 0.299 * R + 0.587 * G + 0.114 * B # not equal (0.333) values because a more realistic grayscale image would have these values\nplt.imshow(grayscale_im, cmap='gray')\nplt.axis('off')\n\n\n\n\n\n\n\n\nA filter is a function grid. To blur the image, each pixel is put through an averaging function. The filter function moves along each pixel, outputting into another image. it takes the average value of the pixels in the filter and applies it to the pixel in the image.\nIt takes all the input pixels and multiplies them by the filter values and sums them up to get the output pixel value. This is an image filtering function.\nThe output image has had convolution applied to it."
  }
]