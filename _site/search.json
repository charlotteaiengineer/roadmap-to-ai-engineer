[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Quick access to frequently used links. Add/edit freely."
  },
  {
    "objectID": "resources.html#daily-weekly",
    "href": "resources.html#daily-weekly",
    "title": "Resources",
    "section": "Daily / Weekly",
    "text": "Daily / Weekly\n\nLatent Space\nSam Altman’s Blog\nNews from Wes Roth"
  },
  {
    "objectID": "resources.html#llms-genai",
    "href": "resources.html#llms-genai",
    "title": "Resources",
    "section": "LLMs / GenAI",
    "text": "LLMs / GenAI\n\nAndrej Karpathy’s Youtube\n3 Blue 1 Brown Youtube\nHugging Face Youtube"
  },
  {
    "objectID": "resources.html#python-data",
    "href": "resources.html#python-data",
    "title": "Resources",
    "section": "Python / Data",
    "text": "Python / Data\n\nTensorflow examples"
  },
  {
    "objectID": "resources.html#model-training",
    "href": "resources.html#model-training",
    "title": "Resources",
    "section": "Model Training",
    "text": "Model Training\n\nNatural Language Toolkit\nSpacy"
  },
  {
    "objectID": "resources.html#research-learning",
    "href": "resources.html#research-learning",
    "title": "Resources",
    "section": "Research & Learning",
    "text": "Research & Learning\n\narc prize"
  },
  {
    "objectID": "resources.html#tools-platforms",
    "href": "resources.html#tools-platforms",
    "title": "Resources",
    "section": "Tools & Platforms",
    "text": "Tools & Platforms\n\nTensorFlow Playground\nRun local AI models like gpt-oss, Qwen, Gemma, DeepSeek and many more on your computer, privately and for free\nlmarena\nInference Playground"
  },
  {
    "objectID": "resources.html#newsletters",
    "href": "resources.html#newsletters",
    "title": "Resources",
    "section": "Newsletters",
    "text": "Newsletters\n\nLatent Space\nnews.smol.ai"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Notes, experiments, and updates.\nWelcome to my blog. I write about frontend engineering, product development, and building with AI.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nDopamine Addiction - the true killer of progression\n\n\nNov 1, 2025\n\n\n\n\n\n\nAn llm that tweaks itself - Evolutionary based LLM\n\n\nOct 29, 2025\n\n\n\n\n\n\nThis weeks Inspirations & Interesting Finds\n\n\nOct 22, 2025\n\n\n\n\n\n\nReality and De-realisation\n\n\nOct 22, 2025\n\n\n\n\n\n\nWhy I’m Building an AI Engineer Curriculum in Public\n\n\nOct 21, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-10-21-first-post.html",
    "href": "blog/posts/2025-10-21-first-post.html",
    "title": "Why I’m Building an AI Engineer Curriculum in Public",
    "section": "",
    "text": "I want to retrain to make AI systems\nWhy a change in career? I’ve been finding fullstack engineering un-impactful and un-fulfilling. Rendering out data on a page; designing that page to pixel perfection, and getting millions of clicks on buttons I have created doesn’t feel awesome to me.\nI’m so glad I went down this route and tried it out… however when I tell people what they should do with their lives, I tell them to study AI and get involved.\nIt is the next technological revolution and I want to be part of it. I like how it’s the closest thing to understand the human brain and trying to emulate it. I like how we become gods in a way of our own simulation. - It’s becoming clearer that we are within a simulation, so I want to be a part of the people able to bend the matrix and understand it.\nAI isn’t only interesting for making lots of money. It could do so much for us. Neuralink comes to mind. That’s directly some of the most incredible impactful outcomes I’ve seen in my life.\nIn another sense I want to get to see AGI. To understand the human mind, truly. I find human intelligence fascinating.\n\n\nMy Dream job\n\nTitle: AGI Engineer\nTeam: The Artificial General Intelligence (AGI) team\nWorking for a massive well known company like OpenAI, Meta, Amazon…\nprototype new technology\nWork on creating systems better than the human brain\nProven track record of designing, building, and shipping real-time ML products\nStrong foundation in signal processing, algorithms, and software engineering principles\n\n\nI need to find the most fascinating stage of training Neural Networks to me, then I’ll find the right specialisation.\n\n\n\nSample Projects must be in portfolio\nOptimizing the throughput of novel attention mechanisms Comparing compute efficiency of different Transformer variants Preparing large-scale datasets for efficient model consumption Scaling distributed training jobs to thousands of GPUs Designing fault tolerance strategies for our training infrastructure Creating interactive visualizations of model internals, such as attention patterns\n\n\nMust-Haves from listings\n\nComputer Science Degree or equivalent\nPossess strong programming skills in Python\nExpertise in Python and experience with deep learning frameworks (PyTorch preferred)\nImplementing LLM finetuning algorithms, such as RLHF\nExperience with open-source ML toolkits and frameworks (eg. PyTorch, TensorFlow, etc)\nHave experience with training, evaluating, or monitoring large language models\nLanguage modeling with transformers\nLarge-scale ETL Work on high-performance, large-scale ML systems Familiarity with GPUs, Kubernetes, and OS internals Experience with language modeling using transformer architectures Knowledge of reinforcement learning techniques Background in large-scale ETL processes Are eager to learn more about machine learning research Are working to align state of the art models with human values and preferences, understand and interpret deep neural networks, or develop new models to support these areas of research View research and engineering as two sides of the same coin, and seek to understand all aspects of our research program as well as possible, to maximize the impact of your insights Have ambitious goals for AI safety and general progress in the next few years, and you’re working to create the best outcomes over the long-term.\nGPUs, Kubernetes, Pytorch, or OS internals\nIndependently lead small research projects\nConduct research and implement solutions in areas such as model architecture, algorithms, data processing, and optimizer development\nDesign, run, and analyze scientific experiments to advance our understanding of large language models\nOptimize and scale our training infrastructure to improve efficiency and reliability\nPublished work on hallucination prevention, factual grounding, or knowledge integration in language models\nExperience with fact-grounding techniques\nBackground in developing confidence estimation or calibration methods for ML models\nA track record of creating and maintaining factual knowledge bases\nFamiliarity with RLHF specifically applied to improving model truthfulness\nWorked with crowd-sourcing platforms and human feedback collection systems\nExperience developing evaluations of model accuracy or hallucinations\nHave industry experience with language model finetuning and classifier training\nShow proficiency in experimental design and statistical analysis for measuring improvements in calibration and accuracy\nCare about AI safety and the accuracy and honesty of both current and future AI systems\nHave experience in data science or the creation and curation of datasets for finetuning LLMs\nAn understanding of various metrics of uncertainty, calibration, and truthfulness in model outputs\ndemonstrable track record of success in delivering new features and products\nCreating reliable, scalable, and high performance AI products\nKnowledge of design or architecture (design patterns, reliability and scaling) of new and existing systems experience\ndevelopment of techniques to minimize hallucinations and enhance truthfulness in language models\nDesign and implement novel data curation pipelines to identify, verify, and filter training data for accuracy given the model’s knowledge\nDevelop specialized classifiers to detect potential hallucinations or miscalibrated claims made by the mode\nCreate and maintain comprehensive honesty benchmarks and evaluation frameworks\nImplement techniques to ground model outputs in verified information, such as search and retrieval-augmented generation (RAG) systems\nDesign and deploy human feedback collection specifically for identifying and correcting miscalibrated responses\nDesign and implement prompting pipelines to generate data that improves model accuracy and honesty\nDevelop and test novel RL environments that reward truthful outputs and penalize fabricated claims\nCreate tools to help human evaluators efficiently assess model outputs for accuracy\nDesign, develop, and maintain tokenization systems used across Pretraining and Finetuning workflows\nOptimize encoding techniques to improve model training efficiency and performance\nCollaborate closely with research teams to understand their evolving needs around data representation\nBuild infrastructure that enables researchers to experiment with novel tokenization approaches\nImplement systems for monitoring and debugging tokenization-related issues in the model training pipeline\nCreate robust testing frameworks to validate tokenization systems across diverse languages and data types\nIdentify and address bottlenecks in data processing pipelines related to tokenization\nDocument systems thoroughly and communicate technical decisions clearly to stakeholders across teams\nWorking with machine learning data processing pipelines\nBuilding or optimizing data encodings for ML applications\nImplementing or working with BPE, WordPiece, or other tokenization algorithms\nPerformance optimization of ML data processing systems\nMulti-language tokenization challenges and solutions\nResearch environments where engineering directly enables scientific progress\nDistributed systems and parallel computing for ML workflows\nLarge language models or other transformer-based architectures (not required)\nProfiling our reinforcement learning pipeline to find opportunities for improvement Building a system that regularly launches training jobs in a test environment so that we can quickly detect problems in the training pipeline Making changes to our finetuning systems so they work on new model architectures Building instrumentation to detect and eliminate Python GIL contention in our training code Diagnosing why training runs have started slowing down after some number of steps, and fixing it Implementing a stable, fast version of a new training algorithm proposed by a researcher\n\nDevelop next-generation evaluation frameworks - Move beyond traditional benchmarks to create evaluations that capture real-world utility\nCreate automated quality assessment pipelines - Build custom classifiers to continuously monitor RL transcripts for complex issues\nBuild comprehensive training observability systems - Design and implement monitoring infrastructure to keep an eye on how model behaviors evolve throughout training.\nBridge research and production - Partner with research teams to translate cutting-edge evaluation techniques into production-ready systems, and work with engineering teams to ensure our monitoring infrastructure scales with increasingly complex training workflows.\n\nIdeal working environment\n\nWork from home, flexible working hours\nLearning days and study days\nOpen sharing environment\n\n\n\nListings\n\nResearch Engineer, Pre-training\nAnthropic - Machine Learning Systems Engineer, Research Tools\nAnthrophic - [Expression of Interest] Research Scientist/Engineer, Honesty\nAnthropic - Machine Learning Systems Engineer, RL Engineering\nAmazon - Software Development Engineer (ML), AGI Customization\nAmazon - Sr. Research Engineer, Machine Learning, AGI Foundations"
  },
  {
    "objectID": "blog/posts/2025-10-22-reality.html",
    "href": "blog/posts/2025-10-22-reality.html",
    "title": "Reality and De-realisation",
    "section": "",
    "text": "Coherence (2013) was a slow burn however it ended with some truly thought provoking ideas, and I’m visited by this feeling of derealisation.\nLife feels like a simulation, or a dream. There is no way of proving whenever I close my eyes and wake up in the morning that my life wasn’t just preloaded memories of a past that may or may not have happened.\nLiving day by day is beneficial, it makes sense to the majority of the globe. A diary keeps us aware of our current point in time, where and when we are existing.\nHowever; I mean perhaps these feelings arise from an ape species only supposed to live to around 100 years, each decade feeling like a lifetime.\nHowever… those around me seem not to change all that much. Again perhaps there’s a bias there that I’m not privvy to their new theories and ideas and novel questions - though…I’m yet to hear them.\nNot long ago I was so different, so very different. Different enough to have deep discussions and a full on fight with myself about views I had or actions I did.\nI do feel special in this way, snowflaky in the main character syndrome way I suppose - though how else am I supposed to view this ride? The world does revolve around me because I’m the one percieving the world. When you or I say “the world” that world in which we are in reference to is our own ‘worlds’.\nI suppose a large part of pursuing a career in AI is to better understand the simulation in which we exist. These are my beliefs after all."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A roadmap to becoming an AI Engineer",
    "section": "",
    "text": "This is a living, outcome-driven curriculum for becoming an AI Engineer. The dates are simply the day I began that course. Some parts can take a long time to complete, often I went back and forth into different topics to get a better understanding.\nCheck out what I’ve been learning this week, or change the ordering to ‘Oldest’ to start from scratch too.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAI Engineer Learning Journey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSELF-TUNING: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching\n\n\n\nself-tuning\n\nllm\n\npaper\n\n\n\n\n\n\n\n\n\nOct 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 2 - Neural Nets As Universal Approximators\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Layer Perceptrons\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Lectures - History of Neural Networks\n\n\n\nneural-networks\n\n\n\nand other cool videos I found as I wanted to hear other perspectives\n\n\n\n\n\nOct 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Python\n\n\n\npython\n\n\n\nfrontendmaster course\n\n\n\n\n\nOct 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\nconvolutional-neural-networks\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we mean by a network ‘learning’ is the minimisation of the cost function\n\n\n\nneural-networks\n\nmachine-learning\n\ndeep-learning\n\n\n\nGetting through my tabs\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers and Attention\n\n\n\ntransformers\n\nattention\n\n\n\n\n\n\n\n\n\nOct 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into LLMs like ChatGPT\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Python: Beginner’s Guide\n\n\n\npython\n\ndata-structures\n\ntuples\n\nsets\n\ndictionaries\n\nfundamentals\n\n\n\nsets, tuples, dictionaries, mutability & hashing.\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2019-11-realtime-rig.html",
    "href": "projects/2019-11-realtime-rig.html",
    "title": "Realtime character rig",
    "section": "",
    "text": "Unity, Blender, Rokoko; custom retargeting, facial rig mapping, live web integration.\nLinks:\n\nPlaceholder link"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Selected projects with short write‑ups. Click through for details.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMarketplace buildout\n\n\n\nmarketplace\n\nnextjs\n\necommerce\n\n\n\nUltra‑fast marketplaces with non‑technical creator tools.\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEngineer Learning Roadmap\n\n\n\nlearning\n\ndocs\n\n\n\nCurated learning hub with progression tracking.\n\n\n\n\n\nSep 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSocial media prototype\n\n\n\nsocial\n\nprototype\n\n\n\nDeep‑dive first project; IPFS media, auth/feeds prototype.\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRealtime character rig\n\n\n\n3d\n\nrealtime\n\n\n\nMotion‑tracked 3D avatar with custom retargeting and live web integration.\n\n\n\n\n\nNov 20, 2019\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "roadmap/machine-learning.html",
    "href": "roadmap/machine-learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "A Practical Guide to Machine Learning with TensorFlow 2.0 & Keras by Vadim Karpusenko"
  },
  {
    "objectID": "roadmap/machine-learning.html#linear-regression",
    "href": "roadmap/machine-learning.html#linear-regression",
    "title": "Machine Learning",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nimport tensorflow as tf\n\ntf.random.uniform.__doc__\n\n'Outputs random values from a uniform distribution.\\n\\n  The generated values follow a uniform distribution in the range\\n  `[minval, maxval)`. The lower bound `minval` is included in the range, while\\n  the upper bound `maxval` is excluded.\\n\\n  For floats, the default range is `[0, 1)`.  For ints, at least `maxval` must\\n  be specified explicitly.\\n\\n  In the integer case, the random integers are slightly biased unless\\n  `maxval - minval` is an exact power of two.  The bias is small for values of\\n  `maxval - minval` significantly smaller than the range of the output (either\\n  `2**32` or `2**64`).\\n\\n  Examples:\\n\\n  &gt;&gt;&gt; tf.random.uniform(shape=[2])\\n  &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)&gt;\\n  &gt;&gt;&gt; tf.random.uniform(shape=[], minval=-1., maxval=0.)\\n  &lt;tf.Tensor: shape=(), dtype=float32, numpy=-...&gt;\\n  &gt;&gt;&gt; tf.random.uniform(shape=[], minval=5, maxval=10, dtype=tf.int64)\\n  &lt;tf.Tensor: shape=(), dtype=int64, numpy=...&gt;\\n\\n  The `seed` argument produces a deterministic sequence of tensors across\\n  multiple calls. To repeat that sequence, use `tf.random.set_seed`:\\n\\n  &gt;&gt;&gt; tf.random.set_seed(5)\\n  &gt;&gt;&gt; tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\\n  &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\\n  &gt;&gt;&gt; tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\\n  &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;\\n  &gt;&gt;&gt; tf.random.set_seed(5)\\n  &gt;&gt;&gt; tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\\n  &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\\n  &gt;&gt;&gt; tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\\n  &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;\\n\\n  Without `tf.random.set_seed` but with a `seed` argument is specified, small\\n  changes to function graphs or previously executed operations will change the\\n  returned value. See `tf.random.set_seed` for details.\\n\\n  Args:\\n    shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\\n    minval: A Tensor or Python value of type `dtype`, broadcastable with\\n      `shape` (for integer types, broadcasting is not supported, so it needs to\\n      be a scalar). The lower bound on the range of random values to generate\\n      (inclusive).  Defaults to 0.\\n    maxval: A Tensor or Python value of type `dtype`, broadcastable with\\n      `shape` (for integer types, broadcasting is not supported, so it needs to\\n      be a scalar). The upper bound on the range of random values to generate\\n      (exclusive). Defaults to 1 if `dtype` is floating point.\\n    dtype: The type of the output: `float16`, `bfloat16`, `float32`, `float64`,\\n      `int32`, or `int64`. Defaults to `float32`.\\n    seed: A Python integer. Used in combination with `tf.random.set_seed` to\\n      create a reproducible sequence of tensors across multiple calls.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A tensor of the specified shape filled with random uniform values.\\n\\n  Raises:\\n    ValueError: If `dtype` is integral and `maxval` is not specified.\\n  '\n\n\nA Tensor is an Array/ Matrix.\n\ntf.random.uniform([1])\n\n&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.29859698], dtype=float32)&gt;\n\n\nTransform a Tensor representation into a numpy representation\n\nvar = tf.random.uniform([1])\nprint(var.numpy())\n\n[0.09848642]\n\n\nTensorflow is trying to optimise the utilisation of the hardware. Tensorflow keeps track of operations and how they’re running on the GPU. Helping the most compute intesive parts - using pre built libraries for these operations instead.\nLet’s create some noisy data\n\ndef make_noisy_data(w=0.1, b=0.3, n=100):\n    x = tf.random.uniform(shape=(n, ))\n    noise = tf.random.normal(shape=(len(x), ), stddev=0.01)\n    y = w * x + b + noise\n    return x, y\n\n\nX, Y = make_noisy_data()\n\n\nX\n\n&lt;tf.Tensor: shape=(100,), dtype=float32, numpy=\narray([0.9027567 , 0.62425065, 0.8201971 , 0.527208  , 0.41396534,\n       0.82758033, 0.5288365 , 0.38327825, 0.09709716, 0.23363328,\n       0.50896263, 0.7590194 , 0.17011964, 0.8988272 , 0.3283522 ,\n       0.88666594, 0.86446285, 0.01178074, 0.8989016 , 0.93807924,\n       0.6083591 , 0.36750627, 0.79622936, 0.55519485, 0.11650658,\n       0.65157723, 0.28890097, 0.9825233 , 0.88861144, 0.68831944,\n       0.5839045 , 0.6038773 , 0.46507502, 0.9178356 , 0.3605219 ,\n       0.9921415 , 0.23289573, 0.84461486, 0.7918731 , 0.41578424,\n       0.57682455, 0.13426352, 0.4847386 , 0.68105924, 0.7355908 ,\n       0.8982395 , 0.11177492, 0.5470071 , 0.12767065, 0.636551  ,\n       0.85398936, 0.5885891 , 0.9522811 , 0.55156076, 0.5483254 ,\n       0.47479093, 0.7585926 , 0.7068069 , 0.13496864, 0.9430704 ,\n       0.16023445, 0.00137722, 0.85881364, 0.74494696, 0.21477783,\n       0.9410325 , 0.03414536, 0.11404002, 0.69787264, 0.83131003,\n       0.8802042 , 0.8053782 , 0.45944023, 0.35543776, 0.72830594,\n       0.35394847, 0.6955136 , 0.64491606, 0.72169936, 0.6726141 ,\n       0.6235976 , 0.31632876, 0.06785274, 0.9738995 , 0.61349654,\n       0.8516408 , 0.94514275, 0.19843233, 0.5123416 , 0.70605457,\n       0.64199543, 0.82579875, 0.47860146, 0.37357378, 0.5690578 ,\n       0.5520706 , 0.4493246 , 0.10918021, 0.44638765, 0.7698463 ],\n      dtype=float32)&gt;\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(X)\nplt.show()\n\n\n\n\n\n\n\n\nThe distribution is more or less equal (uniform)\n\nplt.plot(X, Y, 'go')\nplt.plot(X, 0.1*X+0.3)\n\n\n\n\n\n\n\n\n\nplt.plot(X.numpy(), Y.numpy(), \"bo\")\n\n\n\n\n\n\n\n\n\nw = 0.1\nb = 0.3\nplt.plot(X.numpy(), Y.numpy(), \"bo\")\nplt.plot([0, 1], [w*0+b, w*1+b], \"g:\")  # Show linear regression line according to true w=0.1, b=0.3\n\n\n\n\n\n\n\n\n\nw_guess = 0.5\nb_guess = 0.0\nplt.plot(X.numpy(), Y.numpy(), \"bo\")\nplt.plot([0, 1], [0.1*0+0.3, 0.1*1+0.3], \"g:\") \nplt.plot([0, 1], [0*w_guess+b_guess, 1*w_guess+b_guess], \"r:\")\nplt.show()\n\n\n\n\n\n\n\n\nThe problem we are trying to solve is funding the correlation, fitting the red line to our green line. Getting the red line to fit the optimal solution (the green line point)\nWe need to find the distance between our red line and the green line. We need to find our error - how far are we from correct answer.\n\ndef predict(x, w=w, b=b):\n    y = w * x + b\n    return y\n\nFinding the loss (distance between the line and a point).\nFind distance between prediction of Y and true value of Y\n\ndef mean_squared_error(y_pred, Y):\n    return tf.reduce_mean(tf.square(y_pred - Y))\n\n\nprint(mean_squared_error(predict(X), Y))\n\ntf.Tensor(8.431363e-05, shape=(), dtype=float32)\n\n\nThe loss is significant 0.12194605\n\nw_guess = 0.1\nb_guess = 0.3\nprint(mean_squared_error(predict(X), Y))    \n\ntf.Tensor(8.431363e-05, shape=(), dtype=float32)\n\n\nMatching our original values makes our loss far far lower 0.00011116295. We are closer to our ideal solution."
  },
  {
    "objectID": "roadmap/machine-learning.html#minimising-the-loss-function",
    "href": "roadmap/machine-learning.html#minimising-the-loss-function",
    "title": "Machine Learning",
    "section": "Minimising the loss function",
    "text": "Minimising the loss function\nWe could do this by hand, changing the weights and biases, twiddling them ourselves…however that would take forever! So we need an automatic solution with differentiation\nWe define the learning_rate. We want our steps to be quite small to prevent overshooting the Gradient Descent (we want to avoid jumping out of our local minima).\nThis will be an iterative method to get us close as we can. - Calculate current loss function - Decide what direction to move - Move in that direction\nGradientTape() is a Tensorflow operation where TF keeps track of mathematical operations\n\nlearning_rate = 0.1\nsteps = 200\n\nw_true = 0.1\nb_true = 0.3\n\nw = tf.Variable(0.0)\nb = tf.Variable(-1.0)\n\nfor step in range(steps):\n    with tf.GradientTape() as tape:\n        predictions = predict(X, w=w, b=b)\n        # comparing our predictions to the true Y values\n        # Calculate current loss function\n        loss = mean_squared_error(predictions, Y)\n    gradients = tape.gradient(loss, [w, b]) # list of variables to modify\n    # modify the weights\n    # print(gradients[0])\n    # print(gradients[0] * learning_rate)\n    w.assign_sub(gradients[0] * learning_rate)\n    b.assign_sub(gradients[1] * learning_rate)\n    # Decide what direction to move\n    # Move in that direction\n    if step % 20 == 0:\n        print(f\"step {step} loss {loss.numpy()}\") \n   \n        plt.plot(X, Y, \"b.\")\n        plt.plot([0, 1], [0*w+b, 1*w+b], \"r:\") \n        plt.plot([0, 1], [0*w_true+b_true, 1*w_true+b_true], \"g:\")\n        plt.show()  \n\ntf.Tensor(-1.5663323, shape=(), dtype=float32)\ntf.Tensor(-0.15663324, shape=(), dtype=float32)\nstep 0 loss 1.841741919517517\n\n\n\n\n\n\n\n\n\ntf.Tensor(-1.1303513, shape=(), dtype=float32)\ntf.Tensor(-0.113035135, shape=(), dtype=float32)\ntf.Tensor(-0.81189096, shape=(), dtype=float32)\ntf.Tensor(-0.081189096, shape=(), dtype=float32)\ntf.Tensor(-0.5793143, shape=(), dtype=float32)\ntf.Tensor(-0.05793143, shape=(), dtype=float32)\ntf.Tensor(-0.40950036, shape=(), dtype=float32)\ntf.Tensor(-0.040950038, shape=(), dtype=float32)\ntf.Tensor(-0.2855524, shape=(), dtype=float32)\ntf.Tensor(-0.028555242, shape=(), dtype=float32)\ntf.Tensor(-0.1951221, shape=(), dtype=float32)\ntf.Tensor(-0.01951221, shape=(), dtype=float32)\ntf.Tensor(-0.12918511, shape=(), dtype=float32)\ntf.Tensor(-0.012918511, shape=(), dtype=float32)\ntf.Tensor(-0.0811464, shape=(), dtype=float32)\ntf.Tensor(-0.00811464, shape=(), dtype=float32)\ntf.Tensor(-0.046186343, shape=(), dtype=float32)\ntf.Tensor(-0.0046186345, shape=(), dtype=float32)\ntf.Tensor(-0.020782612, shape=(), dtype=float32)\ntf.Tensor(-0.0020782612, shape=(), dtype=float32)\ntf.Tensor(-0.0023611865, shape=(), dtype=float32)\ntf.Tensor(-0.00023611866, shape=(), dtype=float32)\ntf.Tensor(0.010959111, shape=(), dtype=float32)\ntf.Tensor(0.0010959111, shape=(), dtype=float32)\ntf.Tensor(0.020553108, shape=(), dtype=float32)\ntf.Tensor(0.0020553109, shape=(), dtype=float32)\ntf.Tensor(0.027425393, shape=(), dtype=float32)\ntf.Tensor(0.0027425394, shape=(), dtype=float32)\ntf.Tensor(0.03231015, shape=(), dtype=float32)\ntf.Tensor(0.003231015, shape=(), dtype=float32)\ntf.Tensor(0.03574395, shape=(), dtype=float32)\ntf.Tensor(0.0035743953, shape=(), dtype=float32)\ntf.Tensor(0.03811887, shape=(), dtype=float32)\ntf.Tensor(0.003811887, shape=(), dtype=float32)\ntf.Tensor(0.039721422, shape=(), dtype=float32)\ntf.Tensor(0.0039721425, shape=(), dtype=float32)\ntf.Tensor(0.040761046, shape=(), dtype=float32)\ntf.Tensor(0.0040761046, shape=(), dtype=float32)\ntf.Tensor(0.041390672, shape=(), dtype=float32)\ntf.Tensor(0.004139067, shape=(), dtype=float32)\nstep 20 loss 0.012263456359505653\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.041722164, shape=(), dtype=float32)\ntf.Tensor(0.0041722166, shape=(), dtype=float32)\ntf.Tensor(0.041837193, shape=(), dtype=float32)\ntf.Tensor(0.0041837194, shape=(), dtype=float32)\ntf.Tensor(0.04179543, shape=(), dtype=float32)\ntf.Tensor(0.004179543, shape=(), dtype=float32)\ntf.Tensor(0.04164048, shape=(), dtype=float32)\ntf.Tensor(0.004164048, shape=(), dtype=float32)\ntf.Tensor(0.041404177, shape=(), dtype=float32)\ntf.Tensor(0.0041404176, shape=(), dtype=float32)\ntf.Tensor(0.041109815, shape=(), dtype=float32)\ntf.Tensor(0.0041109817, shape=(), dtype=float32)\ntf.Tensor(0.040774334, shape=(), dtype=float32)\ntf.Tensor(0.0040774336, shape=(), dtype=float32)\ntf.Tensor(0.040410172, shape=(), dtype=float32)\ntf.Tensor(0.0040410175, shape=(), dtype=float32)\ntf.Tensor(0.04002635, shape=(), dtype=float32)\ntf.Tensor(0.0040026354, shape=(), dtype=float32)\ntf.Tensor(0.039629493, shape=(), dtype=float32)\ntf.Tensor(0.0039629494, shape=(), dtype=float32)\ntf.Tensor(0.03922435, shape=(), dtype=float32)\ntf.Tensor(0.003922435, shape=(), dtype=float32)\ntf.Tensor(0.038814474, shape=(), dtype=float32)\ntf.Tensor(0.0038814475, shape=(), dtype=float32)\ntf.Tensor(0.038402375, shape=(), dtype=float32)\ntf.Tensor(0.0038402376, shape=(), dtype=float32)\ntf.Tensor(0.037989922, shape=(), dtype=float32)\ntf.Tensor(0.0037989921, shape=(), dtype=float32)\ntf.Tensor(0.037578415, shape=(), dtype=float32)\ntf.Tensor(0.0037578417, shape=(), dtype=float32)\ntf.Tensor(0.03716885, shape=(), dtype=float32)\ntf.Tensor(0.0037168849, shape=(), dtype=float32)\ntf.Tensor(0.036761895, shape=(), dtype=float32)\ntf.Tensor(0.0036761896, shape=(), dtype=float32)\ntf.Tensor(0.03635806, shape=(), dtype=float32)\ntf.Tensor(0.0036358058, shape=(), dtype=float32)\ntf.Tensor(0.035957668, shape=(), dtype=float32)\ntf.Tensor(0.0035957668, shape=(), dtype=float32)\ntf.Tensor(0.03556094, shape=(), dtype=float32)\ntf.Tensor(0.003556094, shape=(), dtype=float32)\nstep 40 loss 0.0078764408826828\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.03516807, shape=(), dtype=float32)\ntf.Tensor(0.003516807, shape=(), dtype=float32)\ntf.Tensor(0.034779176, shape=(), dtype=float32)\ntf.Tensor(0.0034779178, shape=(), dtype=float32)\ntf.Tensor(0.03439427, shape=(), dtype=float32)\ntf.Tensor(0.0034394271, shape=(), dtype=float32)\ntf.Tensor(0.034013428, shape=(), dtype=float32)\ntf.Tensor(0.0034013428, shape=(), dtype=float32)\ntf.Tensor(0.033636667, shape=(), dtype=float32)\ntf.Tensor(0.0033636668, shape=(), dtype=float32)\ntf.Tensor(0.033263955, shape=(), dtype=float32)\ntf.Tensor(0.0033263955, shape=(), dtype=float32)\ntf.Tensor(0.032895304, shape=(), dtype=float32)\ntf.Tensor(0.0032895305, shape=(), dtype=float32)\ntf.Tensor(0.03253068, shape=(), dtype=float32)\ntf.Tensor(0.003253068, shape=(), dtype=float32)\ntf.Tensor(0.032170042, shape=(), dtype=float32)\ntf.Tensor(0.0032170042, shape=(), dtype=float32)\ntf.Tensor(0.031813372, shape=(), dtype=float32)\ntf.Tensor(0.0031813371, shape=(), dtype=float32)\ntf.Tensor(0.031460635, shape=(), dtype=float32)\ntf.Tensor(0.0031460635, shape=(), dtype=float32)\ntf.Tensor(0.031111801, shape=(), dtype=float32)\ntf.Tensor(0.0031111802, shape=(), dtype=float32)\ntf.Tensor(0.03076682, shape=(), dtype=float32)\ntf.Tensor(0.003076682, shape=(), dtype=float32)\ntf.Tensor(0.030425655, shape=(), dtype=float32)\ntf.Tensor(0.0030425654, shape=(), dtype=float32)\ntf.Tensor(0.030088257, shape=(), dtype=float32)\ntf.Tensor(0.0030088257, shape=(), dtype=float32)\ntf.Tensor(0.029754613, shape=(), dtype=float32)\ntf.Tensor(0.0029754613, shape=(), dtype=float32)\ntf.Tensor(0.029424649, shape=(), dtype=float32)\ntf.Tensor(0.002942465, shape=(), dtype=float32)\ntf.Tensor(0.029098341, shape=(), dtype=float32)\ntf.Tensor(0.0029098343, shape=(), dtype=float32)\ntf.Tensor(0.028775657, shape=(), dtype=float32)\ntf.Tensor(0.0028775658, shape=(), dtype=float32)\ntf.Tensor(0.028456554, shape=(), dtype=float32)\ntf.Tensor(0.0028456554, shape=(), dtype=float32)\nstep 60 loss 0.00507228123024106\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.028140992, shape=(), dtype=float32)\ntf.Tensor(0.0028140992, shape=(), dtype=float32)\ntf.Tensor(0.02782893, shape=(), dtype=float32)\ntf.Tensor(0.0027828931, shape=(), dtype=float32)\ntf.Tensor(0.027520325, shape=(), dtype=float32)\ntf.Tensor(0.0027520326, shape=(), dtype=float32)\ntf.Tensor(0.027215129, shape=(), dtype=float32)\ntf.Tensor(0.002721513, shape=(), dtype=float32)\ntf.Tensor(0.026913319, shape=(), dtype=float32)\ntf.Tensor(0.002691332, shape=(), dtype=float32)\ntf.Tensor(0.02661486, shape=(), dtype=float32)\ntf.Tensor(0.002661486, shape=(), dtype=float32)\ntf.Tensor(0.026319701, shape=(), dtype=float32)\ntf.Tensor(0.0026319702, shape=(), dtype=float32)\ntf.Tensor(0.026027828, shape=(), dtype=float32)\ntf.Tensor(0.0026027828, shape=(), dtype=float32)\ntf.Tensor(0.025739193, shape=(), dtype=float32)\ntf.Tensor(0.0025739192, shape=(), dtype=float32)\ntf.Tensor(0.025453754, shape=(), dtype=float32)\ntf.Tensor(0.0025453754, shape=(), dtype=float32)\ntf.Tensor(0.025171466, shape=(), dtype=float32)\ntf.Tensor(0.0025171468, shape=(), dtype=float32)\ntf.Tensor(0.024892338, shape=(), dtype=float32)\ntf.Tensor(0.0024892339, shape=(), dtype=float32)\ntf.Tensor(0.024616286, shape=(), dtype=float32)\ntf.Tensor(0.0024616288, shape=(), dtype=float32)\ntf.Tensor(0.024343288, shape=(), dtype=float32)\ntf.Tensor(0.002434329, shape=(), dtype=float32)\ntf.Tensor(0.02407331, shape=(), dtype=float32)\ntf.Tensor(0.002407331, shape=(), dtype=float32)\ntf.Tensor(0.023806341, shape=(), dtype=float32)\ntf.Tensor(0.0023806342, shape=(), dtype=float32)\ntf.Tensor(0.02354234, shape=(), dtype=float32)\ntf.Tensor(0.0023542342, shape=(), dtype=float32)\ntf.Tensor(0.023281276, shape=(), dtype=float32)\ntf.Tensor(0.0023281276, shape=(), dtype=float32)\ntf.Tensor(0.023023095, shape=(), dtype=float32)\ntf.Tensor(0.0023023095, shape=(), dtype=float32)\ntf.Tensor(0.022767778, shape=(), dtype=float32)\ntf.Tensor(0.002276778, shape=(), dtype=float32)\nstep 80 loss 0.003277217037975788\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.022515297, shape=(), dtype=float32)\ntf.Tensor(0.0022515298, shape=(), dtype=float32)\ntf.Tensor(0.022265606, shape=(), dtype=float32)\ntf.Tensor(0.0022265606, shape=(), dtype=float32)\ntf.Tensor(0.022018688, shape=(), dtype=float32)\ntf.Tensor(0.002201869, shape=(), dtype=float32)\ntf.Tensor(0.021774516, shape=(), dtype=float32)\ntf.Tensor(0.0021774515, shape=(), dtype=float32)\ntf.Tensor(0.021533059, shape=(), dtype=float32)\ntf.Tensor(0.002153306, shape=(), dtype=float32)\ntf.Tensor(0.021294266, shape=(), dtype=float32)\ntf.Tensor(0.0021294267, shape=(), dtype=float32)\ntf.Tensor(0.021058112, shape=(), dtype=float32)\ntf.Tensor(0.0021058114, shape=(), dtype=float32)\ntf.Tensor(0.020824596, shape=(), dtype=float32)\ntf.Tensor(0.0020824596, shape=(), dtype=float32)\ntf.Tensor(0.020593641, shape=(), dtype=float32)\ntf.Tensor(0.0020593642, shape=(), dtype=float32)\ntf.Tensor(0.020365264, shape=(), dtype=float32)\ntf.Tensor(0.0020365266, shape=(), dtype=float32)\ntf.Tensor(0.020139437, shape=(), dtype=float32)\ntf.Tensor(0.0020139439, shape=(), dtype=float32)\ntf.Tensor(0.019916093, shape=(), dtype=float32)\ntf.Tensor(0.0019916093, shape=(), dtype=float32)\ntf.Tensor(0.019695243, shape=(), dtype=float32)\ntf.Tensor(0.0019695244, shape=(), dtype=float32)\ntf.Tensor(0.019476837, shape=(), dtype=float32)\ntf.Tensor(0.0019476836, shape=(), dtype=float32)\ntf.Tensor(0.019260846, shape=(), dtype=float32)\ntf.Tensor(0.0019260846, shape=(), dtype=float32)\ntf.Tensor(0.019047234, shape=(), dtype=float32)\ntf.Tensor(0.0019047235, shape=(), dtype=float32)\ntf.Tensor(0.018836, shape=(), dtype=float32)\ntf.Tensor(0.0018835999, shape=(), dtype=float32)\ntf.Tensor(0.01862712, shape=(), dtype=float32)\ntf.Tensor(0.0018627121, shape=(), dtype=float32)\ntf.Tensor(0.018420562, shape=(), dtype=float32)\ntf.Tensor(0.0018420563, shape=(), dtype=float32)\ntf.Tensor(0.01821629, shape=(), dtype=float32)\ntf.Tensor(0.001821629, shape=(), dtype=float32)\nstep 100 loss 0.0021281202789396048\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.018014267, shape=(), dtype=float32)\ntf.Tensor(0.0018014268, shape=(), dtype=float32)\ntf.Tensor(0.017814495, shape=(), dtype=float32)\ntf.Tensor(0.0017814495, shape=(), dtype=float32)\ntf.Tensor(0.017616922, shape=(), dtype=float32)\ntf.Tensor(0.0017616922, shape=(), dtype=float32)\ntf.Tensor(0.017421547, shape=(), dtype=float32)\ntf.Tensor(0.0017421547, shape=(), dtype=float32)\ntf.Tensor(0.01722836, shape=(), dtype=float32)\ntf.Tensor(0.0017228359, shape=(), dtype=float32)\ntf.Tensor(0.0170373, shape=(), dtype=float32)\ntf.Tensor(0.0017037301, shape=(), dtype=float32)\ntf.Tensor(0.016848356, shape=(), dtype=float32)\ntf.Tensor(0.0016848355, shape=(), dtype=float32)\ntf.Tensor(0.016661515, shape=(), dtype=float32)\ntf.Tensor(0.0016661516, shape=(), dtype=float32)\ntf.Tensor(0.016476743, shape=(), dtype=float32)\ntf.Tensor(0.0016476744, shape=(), dtype=float32)\ntf.Tensor(0.016294012, shape=(), dtype=float32)\ntf.Tensor(0.0016294012, shape=(), dtype=float32)\ntf.Tensor(0.016113317, shape=(), dtype=float32)\ntf.Tensor(0.0016113317, shape=(), dtype=float32)\ntf.Tensor(0.015934637, shape=(), dtype=float32)\ntf.Tensor(0.0015934637, shape=(), dtype=float32)\ntf.Tensor(0.01575793, shape=(), dtype=float32)\ntf.Tensor(0.001575793, shape=(), dtype=float32)\ntf.Tensor(0.015583178, shape=(), dtype=float32)\ntf.Tensor(0.0015583178, shape=(), dtype=float32)\ntf.Tensor(0.015410361, shape=(), dtype=float32)\ntf.Tensor(0.0015410361, shape=(), dtype=float32)\ntf.Tensor(0.015239462, shape=(), dtype=float32)\ntf.Tensor(0.0015239463, shape=(), dtype=float32)\ntf.Tensor(0.0150704635, shape=(), dtype=float32)\ntf.Tensor(0.0015070464, shape=(), dtype=float32)\ntf.Tensor(0.014903344, shape=(), dtype=float32)\ntf.Tensor(0.0014903344, shape=(), dtype=float32)\ntf.Tensor(0.014738066, shape=(), dtype=float32)\ntf.Tensor(0.0014738067, shape=(), dtype=float32)\ntf.Tensor(0.014574635, shape=(), dtype=float32)\ntf.Tensor(0.0014574635, shape=(), dtype=float32)\nstep 120 loss 0.001392532605677843\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.014413001, shape=(), dtype=float32)\ntf.Tensor(0.0014413001, shape=(), dtype=float32)\ntf.Tensor(0.014253177, shape=(), dtype=float32)\ntf.Tensor(0.0014253177, shape=(), dtype=float32)\ntf.Tensor(0.014095127, shape=(), dtype=float32)\ntf.Tensor(0.0014095127, shape=(), dtype=float32)\ntf.Tensor(0.0139387995, shape=(), dtype=float32)\ntf.Tensor(0.00139388, shape=(), dtype=float32)\ntf.Tensor(0.013784226, shape=(), dtype=float32)\ntf.Tensor(0.0013784226, shape=(), dtype=float32)\ntf.Tensor(0.01363137, shape=(), dtype=float32)\ntf.Tensor(0.001363137, shape=(), dtype=float32)\ntf.Tensor(0.013480192, shape=(), dtype=float32)\ntf.Tensor(0.0013480192, shape=(), dtype=float32)\ntf.Tensor(0.013330704, shape=(), dtype=float32)\ntf.Tensor(0.0013330703, shape=(), dtype=float32)\ntf.Tensor(0.0131828645, shape=(), dtype=float32)\ntf.Tensor(0.0013182865, shape=(), dtype=float32)\ntf.Tensor(0.013036677, shape=(), dtype=float32)\ntf.Tensor(0.0013036677, shape=(), dtype=float32)\ntf.Tensor(0.012892103, shape=(), dtype=float32)\ntf.Tensor(0.0012892103, shape=(), dtype=float32)\ntf.Tensor(0.012749145, shape=(), dtype=float32)\ntf.Tensor(0.0012749145, shape=(), dtype=float32)\ntf.Tensor(0.012607761, shape=(), dtype=float32)\ntf.Tensor(0.0012607761, shape=(), dtype=float32)\ntf.Tensor(0.01246795, shape=(), dtype=float32)\ntf.Tensor(0.001246795, shape=(), dtype=float32)\ntf.Tensor(0.0123296855, shape=(), dtype=float32)\ntf.Tensor(0.0012329685, shape=(), dtype=float32)\ntf.Tensor(0.012192948, shape=(), dtype=float32)\ntf.Tensor(0.0012192948, shape=(), dtype=float32)\ntf.Tensor(0.012057733, shape=(), dtype=float32)\ntf.Tensor(0.0012057733, shape=(), dtype=float32)\ntf.Tensor(0.011924018, shape=(), dtype=float32)\ntf.Tensor(0.0011924019, shape=(), dtype=float32)\ntf.Tensor(0.011791784, shape=(), dtype=float32)\ntf.Tensor(0.0011791785, shape=(), dtype=float32)\ntf.Tensor(0.011661023, shape=(), dtype=float32)\ntf.Tensor(0.0011661024, shape=(), dtype=float32)\nstep 140 loss 0.0009216518956236541\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.011531712, shape=(), dtype=float32)\ntf.Tensor(0.0011531712, shape=(), dtype=float32)\ntf.Tensor(0.011403834, shape=(), dtype=float32)\ntf.Tensor(0.0011403834, shape=(), dtype=float32)\ntf.Tensor(0.01127736, shape=(), dtype=float32)\ntf.Tensor(0.0011277361, shape=(), dtype=float32)\ntf.Tensor(0.011152307, shape=(), dtype=float32)\ntf.Tensor(0.0011152307, shape=(), dtype=float32)\ntf.Tensor(0.011028634, shape=(), dtype=float32)\ntf.Tensor(0.0011028635, shape=(), dtype=float32)\ntf.Tensor(0.010906326, shape=(), dtype=float32)\ntf.Tensor(0.0010906326, shape=(), dtype=float32)\ntf.Tensor(0.010785382, shape=(), dtype=float32)\ntf.Tensor(0.0010785382, shape=(), dtype=float32)\ntf.Tensor(0.0106657725, shape=(), dtype=float32)\ntf.Tensor(0.0010665773, shape=(), dtype=float32)\ntf.Tensor(0.0105474945, shape=(), dtype=float32)\ntf.Tensor(0.0010547495, shape=(), dtype=float32)\ntf.Tensor(0.010430526, shape=(), dtype=float32)\ntf.Tensor(0.0010430526, shape=(), dtype=float32)\ntf.Tensor(0.010314845, shape=(), dtype=float32)\ntf.Tensor(0.0010314845, shape=(), dtype=float32)\ntf.Tensor(0.010200448, shape=(), dtype=float32)\ntf.Tensor(0.0010200449, shape=(), dtype=float32)\ntf.Tensor(0.010087331, shape=(), dtype=float32)\ntf.Tensor(0.0010087331, shape=(), dtype=float32)\ntf.Tensor(0.009975474, shape=(), dtype=float32)\ntf.Tensor(0.0009975474, shape=(), dtype=float32)\ntf.Tensor(0.00986485, shape=(), dtype=float32)\ntf.Tensor(0.000986485, shape=(), dtype=float32)\ntf.Tensor(0.009755454, shape=(), dtype=float32)\ntf.Tensor(0.00097554544, shape=(), dtype=float32)\ntf.Tensor(0.009647273, shape=(), dtype=float32)\ntf.Tensor(0.00096472737, shape=(), dtype=float32)\ntf.Tensor(0.00954028, shape=(), dtype=float32)\ntf.Tensor(0.00095402804, shape=(), dtype=float32)\ntf.Tensor(0.009434488, shape=(), dtype=float32)\ntf.Tensor(0.0009434488, shape=(), dtype=float32)\ntf.Tensor(0.009329857, shape=(), dtype=float32)\ntf.Tensor(0.00093298574, shape=(), dtype=float32)\nstep 160 loss 0.0006202204385772347\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.009226389, shape=(), dtype=float32)\ntf.Tensor(0.0009226389, shape=(), dtype=float32)\ntf.Tensor(0.009124077, shape=(), dtype=float32)\ntf.Tensor(0.0009124077, shape=(), dtype=float32)\ntf.Tensor(0.009022885, shape=(), dtype=float32)\ntf.Tensor(0.00090228854, shape=(), dtype=float32)\ntf.Tensor(0.008922811, shape=(), dtype=float32)\ntf.Tensor(0.00089228107, shape=(), dtype=float32)\ntf.Tensor(0.008823881, shape=(), dtype=float32)\ntf.Tensor(0.0008823881, shape=(), dtype=float32)\ntf.Tensor(0.008726042, shape=(), dtype=float32)\ntf.Tensor(0.0008726042, shape=(), dtype=float32)\ntf.Tensor(0.008629285, shape=(), dtype=float32)\ntf.Tensor(0.0008629285, shape=(), dtype=float32)\ntf.Tensor(0.008533576, shape=(), dtype=float32)\ntf.Tensor(0.00085335755, shape=(), dtype=float32)\ntf.Tensor(0.00843894, shape=(), dtype=float32)\ntf.Tensor(0.000843894, shape=(), dtype=float32)\ntf.Tensor(0.008345357, shape=(), dtype=float32)\ntf.Tensor(0.00083453575, shape=(), dtype=float32)\ntf.Tensor(0.008252811, shape=(), dtype=float32)\ntf.Tensor(0.0008252811, shape=(), dtype=float32)\ntf.Tensor(0.0081612915, shape=(), dtype=float32)\ntf.Tensor(0.00081612915, shape=(), dtype=float32)\ntf.Tensor(0.00807078, shape=(), dtype=float32)\ntf.Tensor(0.00080707803, shape=(), dtype=float32)\ntf.Tensor(0.007981269, shape=(), dtype=float32)\ntf.Tensor(0.00079812686, shape=(), dtype=float32)\ntf.Tensor(0.0078927465, shape=(), dtype=float32)\ntf.Tensor(0.00078927464, shape=(), dtype=float32)\ntf.Tensor(0.007805233, shape=(), dtype=float32)\ntf.Tensor(0.0007805233, shape=(), dtype=float32)\ntf.Tensor(0.0077186795, shape=(), dtype=float32)\ntf.Tensor(0.000771868, shape=(), dtype=float32)\ntf.Tensor(0.007633075, shape=(), dtype=float32)\ntf.Tensor(0.0007633075, shape=(), dtype=float32)\ntf.Tensor(0.0075484095, shape=(), dtype=float32)\ntf.Tensor(0.00075484096, shape=(), dtype=float32)\ntf.Tensor(0.0074647157, shape=(), dtype=float32)\ntf.Tensor(0.0007464716, shape=(), dtype=float32)\nstep 180 loss 0.00042726114043034613\n\n\n\n\n\n\n\n\n\ntf.Tensor(0.007381947, shape=(), dtype=float32)\ntf.Tensor(0.0007381947, shape=(), dtype=float32)\ntf.Tensor(0.0073000733, shape=(), dtype=float32)\ntf.Tensor(0.00073000736, shape=(), dtype=float32)\ntf.Tensor(0.007219113, shape=(), dtype=float32)\ntf.Tensor(0.0007219113, shape=(), dtype=float32)\ntf.Tensor(0.0071390555, shape=(), dtype=float32)\ntf.Tensor(0.00071390555, shape=(), dtype=float32)\ntf.Tensor(0.007059889, shape=(), dtype=float32)\ntf.Tensor(0.0007059889, shape=(), dtype=float32)\ntf.Tensor(0.0069815833, shape=(), dtype=float32)\ntf.Tensor(0.00069815834, shape=(), dtype=float32)\ntf.Tensor(0.0069041736, shape=(), dtype=float32)\ntf.Tensor(0.00069041736, shape=(), dtype=float32)\ntf.Tensor(0.006827604, shape=(), dtype=float32)\ntf.Tensor(0.00068276044, shape=(), dtype=float32)\ntf.Tensor(0.006751899, shape=(), dtype=float32)\ntf.Tensor(0.00067518995, shape=(), dtype=float32)\ntf.Tensor(0.006677026, shape=(), dtype=float32)\ntf.Tensor(0.0006677026, shape=(), dtype=float32)\ntf.Tensor(0.0066029783, shape=(), dtype=float32)\ntf.Tensor(0.00066029787, shape=(), dtype=float32)\ntf.Tensor(0.0065297675, shape=(), dtype=float32)\ntf.Tensor(0.00065297674, shape=(), dtype=float32)\ntf.Tensor(0.0064573474, shape=(), dtype=float32)\ntf.Tensor(0.0006457348, shape=(), dtype=float32)\ntf.Tensor(0.00638574, shape=(), dtype=float32)\ntf.Tensor(0.000638574, shape=(), dtype=float32)\ntf.Tensor(0.006314908, shape=(), dtype=float32)\ntf.Tensor(0.00063149084, shape=(), dtype=float32)\ntf.Tensor(0.0062448834, shape=(), dtype=float32)\ntf.Tensor(0.00062448834, shape=(), dtype=float32)\ntf.Tensor(0.0061756144, shape=(), dtype=float32)\ntf.Tensor(0.00061756145, shape=(), dtype=float32)\ntf.Tensor(0.0061071375, shape=(), dtype=float32)\ntf.Tensor(0.0006107138, shape=(), dtype=float32)\ntf.Tensor(0.0060394253, shape=(), dtype=float32)\ntf.Tensor(0.00060394255, shape=(), dtype=float32)\n\n\n\ngradients\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.006039425265043974&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.00365498848259449&gt;]\n\n\n\nw\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.15535157918930054&gt;\n\n\n\nb\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.266182005405426&gt;\n\n\n\nw_true = 0.1\nb_true = 0.3\nplt.plot(X, Y, \"b.\")\nplt.plot([0, 1], [0*w+b, 1*w+b], \"r:\") \nplt.plot([0, 1], [0*w_true+b_true, 1*w_true+b_true], \"g:\")\nplt.show()"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "Not started  In progress  Completed\n\n\n\n\n\n\n\n\nNo items found.\n\n\n\n\nBlog posts\n\n\n\n\n\n\n\n\nNo items found."
  },
  {
    "objectID": "learning-journey/2025-10-26.html",
    "href": "learning-journey/2025-10-26.html",
    "title": "Deep Learning Lectures - History of Neural Networks",
    "section": "",
    "text": "Went with Spring 2025 for the best sound quality\n\n\n\nWarren McCulloch and Walter Pitts\n\n\n\n\nWhat interests me is that there’s an element of more complex rule than our on/off neurons. There’s ability for disagreement between neurons. With the Inhibitory neurons, even if the threshold has been met, the neuron will be prevented from firing.\n\n\nRosenblatt’s perceptron\n\n\na simplified model\nnumber of inputs combine linearly\nThreshold logic: Fire is combined input exceeds threshold\n\nRosenblatt also introduced a learning model.\n\n\n\nExcitatory - a positive input - more likely the associated neuron will fire Inhibitory - a negative input - less likely the associated neuron will fire\nLearning Rate - How much the value per weight is changed when we are adjusting the weights.\nLoss Function - The squared sum of the residuals (distance) between the predicted and actual values. The Loss Function measures how far off our output was from the predicted value\n\n\nLearning Rate - How much the network changes its parameters in response to the cost function."
  },
  {
    "objectID": "learning-journey/2025-10-22.html",
    "href": "learning-journey/2025-10-22.html",
    "title": "What we mean by a network ‘learning’ is the minimisation of the cost function",
    "section": "",
    "text": "Neurons , Activation, Layer, Hidden Layers, Parameters\na Weight is assigned to each connection between neurons. These weights are floating point numbers. Then take all the activations from the previous layer and compute their weighted sum according to the weights.\nWe want the Activations to be a value between 0 and 1. This is called a sigmoid function.\n\nThe Activation of the Neuron is a measure of how positive the weighted sum is.\n\nWe also want a bias for inactivity. (bias - how high the weighted sum needs to be before the neuron is meaningfully active)\n\nLearning: Getting the computer to find a valid settings for all the weights and biases so that it will actually solve the problem at hand\n\nVector, Matrix, Transistion of Activations\nTaking the Weighted sum of the Activations in the first layer according to these weights corresponds to one of the turns in a Matrix Vector Product\nThink of Neurons as Functions that take in the outputs of all the neurons of the previous layer and spits out a number between 0 and 1. The entire network is a complex function that is a composition of many simple functions.\nThe Network learns the appropriate weights and biases to solve the problem just by looking at data.\nReLU is used now moreso than Sigmoid because its much easier to train. Rather than a wavy line its a straight line which reflects the idea ‘is it active or not’ rather than warmth or coldness. If the value passes a certain threshold it is active, otherwise it is not. - ReLU is Rectified Linear Unit, a simplification\n\n\nGradient Descent\nA Cost Function is how far off (in values) the prediction is from the actual value. Average cost - a measure for how bad the network performs.\nThe Cost Function takes the weights and biases as its input, it outputs a single number (the cost), and the way its defined depends on the networks behaviour over thousands of pieces of training data.\n\nFinding an input that minimises the cost function is called Gradient Descent. Remember it could be 13,394 inputs… how do you find the input value that minimises the output value of the cost function.\nLocal minima = doable. Global minima = crazy hard.\n\nGradient - the direction of steepest increase\nGradient Descent - Robert Constable\n\nGradient descent is an iterative optimisation algorithm, which is used in machine learning to train models, by finding the parameters which minimise a cost function. Gradient descent is essential where the exhaustive calculation of best parameters would be unfeasible, for example in the training of neural networks.\n\n\n\nWhat we mean by a network ‘learning’ is the minimisation of the cost function\nArtificial neurons have continously ranging neuron activations (0.45….0.93). They are not binary (0, 1).\nGradient Descent - A process of repeatedly nudging an input of a function by some multiple of the negative gradient is called Gradient Descent. It is a way to converge towards some local minimum of a cost function.\nThe sin tells us if the number should be nudged up or down the gradient.\nThe network itself is a function with inputs and outputs, defined in terms of weighted sums:\nFor example:\n\nNeural Network Function\n\ninput: 784 numbers (pixels)\noutput: 10 numbers (digits)\nparameters: 13,002 weights/biases\n\n\n\nCost Function\n\ninputs: 13,002 weights/biases\noutput 1 number (the cost)\nparameters: many, many training examples\n\n\nThe Gradient of the Cost Function tells us what nudges to all the weights/biases cause the fastest change to the value of the cost function. Which changes to which weights matter the most."
  },
  {
    "objectID": "learning-journey/index.html",
    "href": "learning-journey/index.html",
    "title": "AI Engineer Learning Journey",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSELF-TUNING: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching\n\n\n\nself-tuning\n\nllm\n\npaper\n\n\n\n\n\n\n\n\n\nOct 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 2 - Neural Nets As Universal Approximators\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Layer Perceptrons\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Lectures - History of Neural Networks\n\n\n\nneural-networks\n\n\n\nand other cool videos I found as I wanted to hear other perspectives\n\n\n\n\n\nOct 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Python\n\n\n\npython\n\n\n\nfrontendmaster course\n\n\n\n\n\nOct 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\nconvolutional-neural-networks\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we mean by a network ‘learning’ is the minimisation of the cost function\n\n\n\nneural-networks\n\nmachine-learning\n\ndeep-learning\n\n\n\nGetting through my tabs\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers and Attention\n\n\n\ntransformers\n\nattention\n\n\n\n\n\n\n\n\n\nOct 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into LLMs like ChatGPT\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Python: Beginner’s Guide\n\n\n\npython\n\ndata-structures\n\ntuples\n\nsets\n\ndictionaries\n\nfundamentals\n\n\n\nsets, tuples, dictionaries, mutability & hashing.\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learning-journey/2025-10-17.html",
    "href": "learning-journey/2025-10-17.html",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "",
    "text": "🍷 FineWeb: decanting the web for the finest text data at scale"
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-1.-pretraining",
    "href": "learning-journey/2025-10-17.html#step-1.-pretraining",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 1. Pretraining",
    "text": "Step 1. Pretraining\nDownload and preprocess the internet. We want a huge quantity of high quality documents, with large diversity. Huggingface made FineWeb which is a new, large-scale (15-trillion tokens, 44TB disk space) dataset for LLM pretraining. They used Common Crawl as their source of data. OpenAI and Anthrophic crawl themselves OpenAI Crawlers, heres Claudebot\n\n\nURL filtering - Firstly data needs filtering. Loads of websites are not included from categories like adult stuff. BLocklists are lists of urls to block.\ntext extraction - Raw HTML is what the crawlers save. We only want the text content.\nlanguage filtering - There’s a guess (using a classifier) which rules out non english pages, keeping pages that score above 65% confidence it is English.\ngopher filtering Gopher is an LLM Transformer model. The architecture is same as GPT2. It uses a huge dataset. Templates were used to prompt the model to try to stop biases in the data, sentiment analysers were used to stop biased content.\n\n\n\n\nMinhash deduplication - This is a technique to remove duplicate documents from the dataset. It’s a hashing technique that is used to identify duplicate documents.\nC4 filters\nCustom fitlers\nPII Removal - Personal Identifiable Information is detected and removed like addresses, phone numbers, emails, etc."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-2.-tokenization",
    "href": "learning-journey/2025-10-17.html#step-2.-tokenization",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 2. Tokenization",
    "text": "Step 2. Tokenization\nThe way the technology works for these neural nets is that they expect a one dimesional sequence of symbols. They want a finite set of symbols that are possible. We have to decide what the symbols are, then have to represent our data as a one dimensional sequence of symbols.\n\nText → bytes → bits (roundtrip, compact and readable)\n\ntext = \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n\n# Text → bytes (UTF-8)\nutf8_bytes = text.encode(\"utf-8\")\nprint(\"chars:\", len(text))\nprint(\"bytes:\", len(utf8_bytes))\nprint(\"first 16 bytes (int):\", list(utf8_bytes[:16]))\nprint(\"first 16 bytes (hex):\", utf8_bytes[:16].hex())\n\n# Bytes → bits (grouped in 8)\ndef bytes_to_bits(data: bytes) -&gt; str:\n    return \" \".join(f\"{b:08b}\" for b in data)\n\nbits = bytes_to_bits(utf8_bytes)\npreview_bits = \" \".join(bits.split(\" \")[:12])  # first 12 bytes as bits\nprint(\"bits (first 12 bytes):\", preview_bits, \"...\")\nprint(\"total bits:\", len(utf8_bytes) * 8)\n\n# Bits → bytes → text (roundtrip)\ndef bits_to_bytes(bits_str: str) -&gt; bytes:\n    cleaned = bits_str.replace(\" \", \"\")\n    assert len(cleaned) % 8 == 0\n    return bytes(int(cleaned[i:i+8], 2) for i in range(0, len(cleaned), 8))\n\nroundtrip_bytes = bits_to_bytes(bits)\nprint(\"roundtrip matches bytes:\", roundtrip_bytes == utf8_bytes)\nprint(\"decoded:\", roundtrip_bytes.decode(\"utf-8\"))\n\nchars: 167\nbytes: 167\nfirst 16 bytes (int): [73, 39, 109, 32, 101, 110, 106, 111, 121, 105, 110, 103, 32, 108, 101, 97]\nfirst 16 bytes (hex): 49276d20656e6a6f79696e67206c6561\nbits (first 12 bytes): 01001001 00100111 01101101 00100000 01100101 01101110 01101010 01101111 01111001 01101001 01101110 01100111 ...\ntotal bits: 1336\nroundtrip matches bytes: True\ndecoded: I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\n\n\n\n\nPer-character view (code point → UTF‑8 bytes)\nWe want more symbols and shorter sequences. Let’s compress the binary sequence. A group of 8 bits are a byte.\n\nsample = text[:8]\nprint(f\"{'char':&lt;6}{'codepoint':&lt;12}{'hex':&lt;20}{'bin'}\")\nfor ch in sample:\n    b = ch.encode('utf-8')\n    hx = \" \".join(f\"{x:02x}\" for x in b)\n    bn = \" \".join(f\"{x:08b}\" for x in b)\n    print(f\"{repr(ch):&lt;6}{ord(ch):&lt;12}{hx:&lt;20}{bn}\")\n\nchar  codepoint   hex                 bin\n'I'   73          49                  01001001\n\"'\"   39          27                  00100111\n'm'   109         6d                  01101101\n' '   32          20                  00100000\n'e'   101         65                  01100101\n'n'   110         6e                  01101110\n'j'   106         6a                  01101010\n'o'   111         6f                  01101111\n\n\n\n\nByte Pair Encoding (BPE) — learn merges and tokenize\n\n\nWe’ll train a tiny BPE on a short corpus, learn a few merges, then tokenize a sentence.\n\nfrom collections import Counter\n\ncorpus = (\n    \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n).lower()\n\ndef build_vocab(text: str):\n    vocab = Counter()\n    for word in text.split():\n        symbols = tuple(list(word) + ['&lt;/w&gt;'])\n        vocab[symbols] += 1\n    return vocab\n\ndef get_stats(vocab: Counter):\n    pairs = Counter()\n    for symbols, freq in vocab.items():\n        for a, b in zip(symbols, symbols[1:]):\n            pairs[(a, b)] += freq\n    return pairs\n\ndef merge_vocab(pair, vocab: Counter):\n    a, b = pair\n    merged = Counter()\n    for symbols, freq in vocab.items():\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == (a, b):\n                new.append(a + b)\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        merged[tuple(new)] += freq\n    return merged\n\nvocab = build_vocab(corpus)\nmerges = []\nfor _ in range(20):  # learn up to 20 merges\n    stats = get_stats(vocab)\n    if not stats:\n        break\n    best = max(stats, key=stats.get)\n    merges.append(best)\n    vocab = merge_vocab(best, vocab)\n\nprint(\"top merges:\", merges[:10])\n\nrank = {pair: i for i, pair in enumerate(merges)}\n\ndef bpe_tokenize(word: str):\n    symbols = list(word) + ['&lt;/w&gt;']\n    while True:\n        pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols)-1)]\n        ranked = [(rank.get(p, 1e9), p) for p in pairs]\n        best_rank, best_pair = min(ranked, default=(1e9, None))\n        if best_pair is None or best_rank == 1e9:\n            break\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == best_pair:\n                new.append(symbols[i] + symbols[i+1])\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        symbols = new\n    return [s for s in symbols if s != '&lt;/w&gt;']\n\nsentence = \"Viewing single post from Spoilers of the week Lil\".lower()\nchar_tokens = sum((list(w) for w in sentence.split()), [])\nbpe_tokens = []\nfor w in sentence.split():\n    bpe_tokens.extend(bpe_tokenize(w))\n\nprint(\"char-level token count:\", len(char_tokens))\nprint(\"bpe token count:\", len(bpe_tokens))\nprint(\"bpe tokens (first 30):\", bpe_tokens[:30])\n\ntop merges: [('t', '&lt;/w&gt;'), ('n', 'd'), ('i', \"'\"), ('m', '&lt;/w&gt;'), ('i', 'n'), ('g', '&lt;/w&gt;'), ('a', 'r'), ('e', '&lt;/w&gt;'), (\"i'\", 'm&lt;/w&gt;'), ('in', 'g&lt;/w&gt;')]\nchar-level token count: 41\nbpe token count: 36\nbpe tokens (first 30): ['v', 'i', 'e', 'w', 'ing&lt;/w&gt;', 's', 'in', 'g', 'l', 'e&lt;/w&gt;', 'p', 'o', 's', 't&lt;/w&gt;', 'f', 'r', 'o', 'm&lt;/w&gt;', 's', 'p', 'o', 'i', 'l', 'e', 'r', 's&lt;/w&gt;', 'o', 'f', 'the&lt;/w&gt;', 'w']\n\n\nWe mint a symbol for each unique byte pair in the corpus. There are 100,277 symbols in GPT4\n\nGPT2 Tokenizer\n\n\n\n\nTikTokenizer\n\n\nThis token sequence is what GPT4 will ‘see’ the text as."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-3-neural-network-training",
    "href": "learning-journey/2025-10-17.html#step-3-neural-network-training",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 3: Neural Network Training",
    "text": "Step 3: Neural Network Training\nNow we are trying to predict the next token in the sequence. Currently there are 100,277 probabilities for the next token. The neural network is going to output exactly 100,277 numbers, and of those numbers, correspond to the probablility of that token as coming next in the sequence\nIn the beginning the Nural Network is randomly initialised, random probabilities. We’ve sampled this window from our dataset.\nWe know the correct next token for this sentence, so we need a mathematical process to update the weights on the network - tuning it. (Making the probability of the correct next token as high as possible, and making the other potential answers lower.)\nWe mathematically adjust the neural network so that the correct answer has a slightly higher probability.\ninput sequence tokens\niteratively updating the neural network = training the neural network."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#visualization-of-llm-in-3d",
    "href": "learning-journey/2025-10-17.html#visualization-of-llm-in-3d",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Visualization of LLM in 3D",
    "text": "Visualization of LLM in 3D\nTry not to think of these LLM neurons like the ones in our brain, our biological ones have complex dynamical processes that have memory. There’sno memory in LLM neurons, it’s stateless input and output.\nThe LLM in basic terms is a mathematical function. It it parameterised by some fixed set of parameters (85,584) it is a way of transforming inputs to outputs as we twiddle the parameters we are getting different kinds of predictions, and then we need to find a good setting of these parameters so they match up with the patterns seen in the training set"
  },
  {
    "objectID": "learning-journey/2025-10-14.html",
    "href": "learning-journey/2025-10-14.html",
    "title": "Intro to Large Language Models",
    "section": "",
    "text": "Mathematically there is a very close relationship between prediction and compression\nparameters bytes artifact Neural Network compressed into the weights sample model inference feeding back in perform inference\nRun the neural network - or as we say perform inference\ndreaming/mimicking/hallucinating\nIt’s parroting the training set distribution\nlossy compression of the internet\n\nTransformer Neural Net Architecture\n\n\n\nTransformer Neural Net Architecture - https://deeprevision.github.io/posts/001-transformer/\n\n\n100 billion parameters are dispersed throughout the entire Neural Net and all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task - BUT we don’t actually know what these 100 billion parameters are doing.\nWe can measure that it’s getting better to next word prediction, but we don’t know how these parameters collaborate to actually perform that\nknowledge isnt stored in traditional sense\nAll we can really measure is whether it works or not and the probability that it works.\ncome from a long process of optimization interpretability or mechanistic interpretability empirical artifacts\nWe can give them some inputs and can measure the outputs. We can measure their behaviour - this requires corresponsively sophisticated evaluations to work with these models because they’re mostly empirical (evidence based).\n\n\nTraining the assistant - Finetuning\nQuality is prefered over quantity in this stage.\nFine tuning creates an Assistant Model\nThis assisstant model now suscribes to the form of its new training documents.\nRetrieval Augmented Generation - ChatGPT can browse the files that you upload and can use them as reference inforamtion for creating its answers\nThink of LLMs as the kernel process of an emerging operating system - this process is coordiatng many different processes be they memory or computational tools for problemsolving.\nContext window suffix jailbreaking prompt injection Poisioned model/Corrupted model"
  },
  {
    "objectID": "learning-journey/perceptrons.html",
    "href": "learning-journey/perceptrons.html",
    "title": "Multi-Layer Perceptrons",
    "section": "",
    "text": "Perceptrons\n\n\n\nperceptrons do have logic gates for AND, OR, and NOT.\nHowever…there is no solution for XOR gate.\n\nUntil we thought about Networks of Perceptrons. Networked elements are required.\nThere is a need for three perceptrons to solve the XOR gate.\n\nWhen the inidividual outputs of a layer of perceptrons is not needed to be visuallised, we call it a Hidden Layer.\nOnce you begin networking the perceptions, you can perform any boolean function.\nThis is deemed a multi-layer perceptron. Perceptrons are arranged in layers.\n\nLinear Classifier\nA perceptron operates on real-valued vectors.\nThere is a boundary where all inputs are classified as 0 or 1.\n\nA perceptron defines a boundary (the line and the area) where on the graph its a 0 or 1 on each side of that linear classifier line.\nYou can create a shape with many perceptrons with their own linear classifiers. You create a boundary.\nSo you create a region where all perceptrons must output a 1.\n\nDecision Boundaries\n\n\n\nFinding and fitting a decision boundary to the data is one of the main objectives of Machine Learning\n\n\n\nConverges, Coeficiants\nFor each misclassifcation, we adjust the coeficiants to move the boundary in the direction of the misclassification. If still not classified correctly, we adjust the coeficiants again. This process is called Gradient Descent.\n\n\nTensorFlow Playground\n\nBuild a network that isolates the region within the region we wish to classify.\n\n\nIndividual perceptrons capture linear boundaries"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "prophecy.institute",
    "section": "",
    "text": "Not started  In progress  Completed\n\n\n\n\n\n\n\n\nNo items found."
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Not started  In progress  Completed\n\n\n\n\n\n\n\n\nNo items found."
  },
  {
    "objectID": "learning-journey/2025-10-28.html#linear-vs-affine-3d-surfaces",
    "href": "learning-journey/2025-10-28.html#linear-vs-affine-3d-surfaces",
    "title": "Lecture 2 - Neural Nets As Universal Approximators",
    "section": "Linear vs Affine: 3D Surfaces",
    "text": "Linear vs Affine: 3D Surfaces\nThe difference is a bias value\nThis section uses Plotly 3D charts to visualize simple functions of two variables and clarify what’s linear, what’s affine, and what’s not.\n\nLinear function: z = 3x + 7y\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Grid\nx = np.linspace(-5, 5, 60)\ny = np.linspace(-5, 5, 60)\nX, Y = np.meshgrid(x, y)\nZ_linear = 3*X + 7*Y\n\nfig_linear = go.Figure(data=[go.Surface(x=X, y=Y, z=Z_linear, colorscale='Viridis', showscale=True)])\nfig_linear.update_layout(\n    title='Linear plane: z = 3x + 7y',\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n        camera=dict(eye=dict(x=1.6, y=-1.6, z=1.0))\n    ),\n    height=500,\n)\nfig_linear\n\n                            \n                                            \n\n\nKey property: for a linear function f, f(αx + βy) = α f(x) + β f(y) when viewed as maps from a vector space to reals with zero intercept.\n\n\nAffine function: z = 3x + 7y + 4\n\nZ_affine = Z_linear + 4\n\nfig_affine = go.Figure(data=[go.Surface(x=X, y=Y, z=Z_affine, colorscale='Plasma', showscale=True)])\nfig_affine.update_layout(\n    title='Affine plane: z = 3x + 7y + 4',\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n        camera=dict(eye=dict(x=1.6, y=-1.6, z=1.0))\n    ),\n    height=500,\n)\nfig_affine\n\n                            \n                                            \n\n\nAn affine function is a linear function plus a constant offset. It does not preserve the linear-combination property due to the intercept.\n\n\nNon-linear (not linear, not affine): z = 3x^2 + 7y\n\nZ_nonlinear = 3*(X**2) + 7*Y\n\nfig_nl = go.Figure(data=[go.Surface(x=X, y=Y, z=Z_nonlinear, colorscale='Cividis', showscale=True)])\nfig_nl.update_layout(\n    title='Non-linear surface: z = 3x^2 + 7y',\n    scene=dict(\n        xaxis_title='x',\n        yaxis_title='y',\n        zaxis_title='z',\n        camera=dict(eye=dict(x=1.6, y=-1.6, z=1.0))\n    ),\n    height=500,\n)\nfig_nl\n\n                            \n                                            \n\n\n\n\nLinear-combination check (scatter demo)\n\n# Demonstrate linear vs affine on sample points\ndef f_linear(x, y):\n    return 3*x + 7*y\n\ndef f_affine(x, y):\n    return 3*x + 7*y + 4\n\n# Pick two points in R^2\np = np.array([2.0, -1.0])\nq = np.array([-3.0, 2.0])\nalpha, beta = 0.4, 0.6\ncomb = alpha*p + beta*q\n\nfp = f_linear(*p)\nfq = f_linear(*q)\nfcomb = f_linear(*comb)\nlhs_linear = fcomb\nrhs_linear = alpha*fp + beta*fq\n\ngp = f_affine(*p)\ngq = f_affine(*q)\ngcomb = f_affine(*comb)\nlhs_affine = gcomb\nrhs_affine = alpha*gp + beta*gq\n\nprint('Linear f: f(alpha p + beta q) =', lhs_linear, ' ; alpha f(p) + beta f(q) =', rhs_linear)\nprint('Affine g: g(alpha p + beta q) =', lhs_affine, ' ; alpha g(p) + beta g(q) =', rhs_affine)\n\n# 3D scatter of the three points under f and g for visualization\npts = np.array([p, q, comb])\nz_f = np.array([f_linear(*pt) for pt in pts])\nz_g = np.array([f_affine(*pt) for pt in pts])\n\nfig_pts = go.Figure()\nfig_pts.add_trace(go.Scatter3d(x=pts[:,0], y=pts[:,1], z=z_f, mode='markers+lines',\n                               name='Linear f', marker=dict(size=5, color='blue')))\nfig_pts.add_trace(go.Scatter3d(x=pts[:,0], y=pts[:,1], z=z_g, mode='markers+lines',\n                               name='Affine g', marker=dict(size=5, color='red')))\nfig_pts.update_layout(title='Linear-combination check on sample points',\n                      scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z',\n                                 camera=dict(eye=dict(x=1.6, y=-1.6, z=1.0))))\nfig_pts\n\nLinear f: f(alpha p + beta q) = 2.6000000000000005  ; alpha f(p) + beta f(q) = 2.6\nAffine g: g(alpha p + beta q) = 6.6000000000000005  ; alpha g(p) + beta g(q) = 6.6\n\n\n                            \n                                            \n\n\nReferences: Plotly 3D charts documentation at plotly.com/python/3d-charts."
  },
  {
    "objectID": "learning-journey/2025-10-29.html",
    "href": "learning-journey/2025-10-29.html",
    "title": "SELF-TUNING: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching",
    "section": "",
    "text": "SELF-TUNING: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching\nXiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, Helen Meng\nInspired by the success of the Feynman Technique - how children are taught to learn.\nSELFTUNING - , a learning framework aimed at improving an LLM’s ability to effectively acquire new knowledge from unseen raw documents through self-teaching.\na SELF-TEACHING strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection\nOutcome - significantly enhanced factual accuracy compared to the standard approaches. SELFTUNING significantly outperforms all other compared methods on knowledge memorization and extraction tasks.\nIn addition, SELF-TUNING consistently yields high accuracy on reasoning tasks, while the performance of the compared methods largely fluctuates in different scenarios.\nInspiringly, SELF-TUNING exhibits exceptional performance in retaining previously acquired knowledge (i.e., knowledge retention) concerning extraction and reasoning on two well-established benchmarks\n\nStage 1: Learn How to Effectively Absorb Knowledge from Raw Documents.\nequip an LLM M, parameterized by θ,with the ability to learn how to derive knowledge\n\\[\nL_\\text{Stage 1}(\\theta) = L_\\theta(D_\\text{Doc}^{\\text{train}}) + L_\\theta(D_\\text{Self}^{\\text{train}}) + L_\\theta(D_\\text{QA}^{\\text{train}})\n\\\n\\]\n\n\nStage 2: Learn New Knowledge while Reviewing QA Skills\ntrain the model to apply the learned strategy for spontaneously extracting new knowledge from unseen documents\nDocTrain is training documents, QATrain is training QA Dataset. Self Train is the knowledge-intensive tasks created in a self-supervised manner.\n\\[\nL_\\text{Stage 2}(\\theta) = L_\\theta(D_\\text{Doc}^{\\text{test}}) + L_\\theta(D_\\text{QA}^{\\text{train}})\n\\]\nDocTest is raw test corpora, in addition to training on this, the model is also trained on QATrain -allowing the model M to review and refine its question-answering ability\n\nStage 3: Continually Learn.\nTheir goal is to ensure that the model M thoroughly absorbs the new knowledge by conducting follow-up training on DDocTest (raw corpora). The objective is as follows:\n\\[\nL_\\text{Stage 3}(\\theta) = L_\\theta(D_\\text{Doc}^{\\text{test}})\n\\]\nMemorisation is tested via nexttoken prediction task on plain document texts.\nComprehension is tested through the following tasks:\n\nSummarization allows the model to learn to grasp the topic by using the prompt Write a title: to encourage the model to summarize the raw text\nGist identification improves the model’s ability to pinpoint the key elements within the atomic facts. prompt the model with Highlight the key information within the article:, and use the entities within the document as gold answers, identified using Spacy\nNatural language inference provides the model with the capability to determine whether a statement can be inferred from specific document contents (i.e., “Yes,” “No,” or “It’s impossible to say”), thus avoiding misconceptions that may arise during knowledge acquisition."
  },
  {
    "objectID": "learning-journey/2025-10-13.html",
    "href": "learning-journey/2025-10-13.html",
    "title": "Practical Python: Beginner’s Guide",
    "section": "",
    "text": "Frontend Masters — Practical Guide to Python (full course) by Nina Zakharenko: https://frontendmasters.com/courses/practical-python/\n\n\nNumbers and booleans\n\na = 12\nb = 3.5\ntotal = a + b\nis_equal = (a == 2)\ntruthy = True and (1 &lt; 2)\nfalsy = False or (2 &gt; 5)\nnot_val = not False\n\nprint(a, type(a))\nprint(b, type(b))\nprint(total)\nprint(is_equal, truthy, falsy, not_val)\n\n12 &lt;class 'int'&gt;\n3.5 &lt;class 'float'&gt;\n15.5\nFalse True False True\n\n\n\n\nStrings and f-strings\n\nname = \"Charlotte\"\nlang = \"Python\"\nmsg = f\"Hi {name}, welcome to {lang}!\"\n\nprint(name.upper())\nprint(len(name))\nprint(\"thon\" in lang)\nprint(msg)\n\nCHARLOTTE\n9\nTrue\nHi Charlotte, welcome to Python!\n\n\n\n\nLists\n\nnums = [1, 2, 3]\nnums.append(4)\nsliced = nums[1:3]\ndoubled = [x * 2 for x in nums]\n\nprint(nums)\nprint(sliced)\nprint(doubled)\n\n[1, 2, 3, 4]\n[2, 3]\n[2, 4, 6, 8]\n\n\n\n\nTuples\n\nt = (1, \"a\", True)\nsingle = (42,)\npacked = 1, 2  # tuple without parentheses\nx, y = (10, 20)  # unpacking\n\nprint(t)\nprint(single)\nprint(packed)\nprint(x, y)\n\n# t[0] = 99  # TypeError: 'tuple' object does not support item assignment\n\n(1, 'a', True)\n(42,)\n(1, 2)\n10 20\n\n\n\n\nDictionaries\n\nuser = {\"name\": \"Charlotte\", \"role\": \"AI Engineer\"}\nuser[\"city\"] = \"Ipswich\"\n\nprint(user[\"name\"])       # indexing by key\nprint(\"role\" in user)      # membership check on keys\nprint(user)\n\nCharlotte\nTrue\n{'name': 'Charlotte', 'role': 'AI Engineer', 'city': 'Ipswich'}\n\n\n\n\nSets\n\nnames = [\"alice\", \"bob\", \"alice\"]\ns = set(names)\ns.add(\"carol\")\n\nprint(s)\nprint(\"alice\" in s)\n\n{'carol', 'alice', 'bob'}\nTrue\n\n\n\n\nHash function and hashability\n\nprint(hash((\"a\", 1)))  # tuples are hashable if their items are hashable\nprint(hash(\"abc\"))\n\n# hash([1, 2])   # TypeError: unhashable type: 'list'\n# hash({\"k\": 1}) # TypeError: unhashable type: 'dict'\n\n3679728146113103727\n6486578851436102898\n\n\n\n\nLogic: and / or / in\n\nprint(True and False)\nprint(True or False)\nprint(False or 0)\nprint(0 or \"fallback\")\nprint(\"x\" and \"y\")      # returns last truthy operand\nprint(\"py\" in \"python\")  # substring membership\n\nFalse\nTrue\n0\nfallback\ny\nTrue\n\n\n\n\nFunctions\n\ndef my_function(x=4):\n   return x + 2\n\nmy_function()\n\n6\n\n\n\ndef my_other_function(x):\n   return x * 2\n\nmy_other_function(21312)\n\n42624\n\n\n\ndef another_cat(x, y, z=12):\n   return z + (x + y)\n\nanother_cat(1, 2)\n\n15\n\n\n\n\nConditionals\n\ndef my_dogs(x, one, two):\n    if x == 2:\n        return f\"my doggos are {one} and {two}\"\n    elif x &gt; 2:\n        return \"Soon! soon we will have them all!\"\n    else: \n        return \"Got no doggos\"\n\nmy_dogs(1, \"Annie\", \"Anubis\")\n\n'Got no doggos'\n\n\n\ndef fizzbuzz(number):\n    if (number % 3 == 0) and (number % 5 == 0):\n        print(\"fizz\")\n    else: print(\"buzz\")\n\nfizzbuzz(15)\nfizzbuzz(5)\n\nfizz\nbuzz\n\n\n\n\nLoops\n\nfamily = [\"Annie\", \"Anubis\", \"Alex\", \"Charlotte\"]\n\nfor family_member in family:\n    print(f\"My name is {family_member}!\")\n\nprint(f\"outside of the loop {family_member}\")\n\nlist(enumerate(family))\n\nMy name is Annie!\nMy name is Anubis!\nMy name is Alex!\nMy name is Charlotte!\noutside of the loop Charlotte\n\n\n[(0, 'Annie'), (1, 'Anubis'), (2, 'Alex'), (3, 'Charlotte')]\n\n\nenumerate returns list of tuples , first item index, second item the value\n\ncolours = [\"Red\", \"Yellow\", \"Pink\", \"Green\", \"Orange\", \"Purple\", \"Blue\"]\n\nfor index, colour in enumerate(colours):\n    print(f\"this is the greatest colour {colour} number {index}\")\n\nthis is the greatest colour Red number 0\nthis is the greatest colour Yellow number 1\nthis is the greatest colour Pink number 2\nthis is the greatest colour Green number 3\nthis is the greatest colour Orange number 4\nthis is the greatest colour Purple number 5\nthis is the greatest colour Blue number 6\n\n\n\nconcepts_to_learn = {\n    \"Python\": \"Language\",\n    \"TensorFlow\": \"Execution Env\",\n    \"Pytorch\": \"differnt exectuion place\",\n    \"Deep Learning\": \"Neural networks and that\"\n}\n\nfor foo in concepts_to_learn:\n    print(foo)\n\nPython\nTensorFlow\nPytorch\nDeep Learning\n\n\n\nconcepts_to_learn.items()\n\ndict_items([('Python', 'Language'), ('TensorFlow', 'Execution Env'), ('Pytorch', 'differnt exectuion place'), ('Deep Learning', 'Neural networks and that')])\n\n\n\nfor key, value in concepts_to_learn.items():\n    print(key) \n    print(\"----\")\n    print(value)\n\nPython\n----\nLanguage\nTensorFlow\n----\nExecution Env\nPytorch\n----\ndiffernt exectuion place\nDeep Learning\n----\nNeural networks and that\n\n\n\nx = 0\nwhile x &lt; 5:\n    print(x)\n    x += 1\n\n0\n1\n2\n3\n4\n\n\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\n\ndef return_target(target=\"Jeremy\"):\n    for name in names:\n        print(name)\n        if name == target:\n            print(f\"we found {target}!\")\n            return name\n\n\n\nList comprehensions\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\nmy_list = []\n\nfor name in names:\n    my_list.append(len(name))\n\nprint(\"First way: \", my_list)\n\nprint(\"Shorter way:\", [len(name) for name in names])\n\nFirst way:  [4, 3, 6, 6, 5]\nShorter way: [4, 3, 6, 6, 5]\n\n\n\nnums = [0, 1, 2, 3, 4]\n\n[num * 2 for num in nums] \n\n[0, 2, 4, 6, 8]\n\n\n\n\nSlicing\n\nmy_cake = \"Hey this is a big cake!\"\nmy_cake[14:22]\n\nmy_cake[:18]\nmy_cake[19:]\nmy_cake[-1]\n\n'!'\n\n\n\n\nfiles - reading, writing, appending, and JSON\nBelow are executable examples that will run in this page’s kernel. They demonstrate different open() modes and working with a small JSON file stored alongside this page at learning-journey/data/example.json.\n\n# Write: creates or truncates file\nwith open(\"my_file.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"first line\\n\")\n    f.write(\"second line\\n\")\n\n# Append: adds to end of file\nwith open(\"my_file.txt\", \"a\", encoding=\"utf-8\") as f:\n    f.write(\"appended line\\n\")\n\n# Read entire file\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    contents = f.read()\ncontents\n\n'first line\\nsecond line\\nappended line\\n'\n\n\n\n# Read line-by-line\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        print(line.rstrip())\n\nfirst line\nsecond line\nappended line\n\n\n\n# Explicit open/close (less preferred vs. context manager)\nf = open(\"my_file.txt\", \"r\", encoding=\"utf-8\")\ntry:\n    print(f.readline().rstrip())\nfinally:\n    f.close()\n\nfirst line\n\n\n\n# pathlib usage\nfrom pathlib import Path\n\npath = Path(\"my_file.txt\")\npath.write_text(\"overwritten via pathlib\\n\", encoding=\"utf-8\")\nprint(path.read_text(encoding=\"utf-8\"))\n\noverwritten via pathlib\n\n\n\n\n\nClasses\n\nclass Car:\n    runs = True\n\n    def start(self):\n        if self.runs:\n            print(\"The car starts.\")\n        else:\n            print(\"The car is broken.\")\n\nmy_car = Car()\nmy_car.start()\nmy_car.runs = False\nmy_car.start()\n\nmy_other_car = Car()\nmy_other_car.start()\n\nThe car starts.\nThe car is broken.\nThe car starts.\n\n\n\n\nisinstance\n\nprint(isinstance(my_car, Car))\nprint(isinstance(my_car, str))\nprint(isinstance(\"Hallo there\", str))\nprint(isinstance(12, int))\n\nTrue\nFalse\nTrue\nTrue\n\n\n\n\ninitializing classes\n\nclass Supersupercar:\n    runs = True\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def start(self):\n        if self.runs:\n            print(f\"The {self.make} {self.model} {self.year} starts.\")\n        else:\n            print(f\"The {self.make} {self.model} {self.year} is broken.\")\n\nmy_car = Supersupercar(\"Toyota\", \"Corolla\", 2020)\nmy_car.start()\n\nmy_better_car = Supersupercar(\"Mustang\", \"GT\", 2025)\nmy_better_car.start()\n\nThe Toyota Corolla 2020 starts.\nThe Mustang GT 2025 starts.\n\n\n\n\n\nMustang GT\n\n\n\n\ninheritance\n\nclass Mustang(Supersupercar):\n    def __init__(self, make, model, year, color):\n        super().__init__(make, model, year)\n        self.color = color\n\nmy_jaguar = Mustang(\"Jaguar\", \"2027\", 2027, \"Green\")\nmy_jaguar.start()\n\nThe Jaguar 2027 2027 starts.\n\n\n\n\n\nJaguar 2027\n\n\n\n\nExceptions\n\ntry:\n    print(10 / 0)\nexcept ZeroDivisionError:\n    print(\"You can't divide by zero!\")\n\ntry:\n    print(10 / 2)\nexcept ValueError:\n    print(\"You can divide by two!\")\n\nYou can't divide by zero!\n5.0\n\n\n\n\nrequests\n\n# import requests\n\n# response = requests.get(\"https://ur-api.....\")\n\n# print(response.status_code)\n# print(response.json())"
  },
  {
    "objectID": "learning-journey/2025-10-20.html",
    "href": "learning-journey/2025-10-20.html",
    "title": "Transformers and Attention",
    "section": "",
    "text": "This video is very information dense and heavy and far beyond currrent full understanding. Just watch to familiarise yourself with the concepts loosely.\n\n\nAttention pattern, Weighted Sum, Dot Product, Sequence of Vectors, Multi-Headed Attention, Multilayer Perceptron\nContext is revelant to updating meanings.\nWhy is this technique as effective as it is? One lesson is that scale alone matters. Simply making things bigger and simply giving them more training data can sometimes give qualititivie improvements to the model performance.\nFor a given size of the model, for a given amount of training that you do — what’s the cost function going to look like? If that cost function is going down, that typically responds to improvements in the model performance that are qualitativley visable - a chatbot that behaves better.\nThe attention mechanism allows things to talk to eachother without sequential processing - it means they can take in the whole text passage at once, letting all the different embeddings talk to eachother as it does. This way it can effectively do more floating point operations in a given amount of time.\nYou can alos train on a huge amount of data that doesnt require human labelling. You can just give it massive amounts of data wihtout being restrained by human feedback.\nYou can Tokenise essentially anything and then embed those as vectors. This means you can have lots of distinct data types working in conjunction with each other.\n\n\n\nUnder the hood of ChatGP(Transformer)\nEach Transformer performs a set of fixed matrix operations on an input matrix of data and typically returns an output matrix of the same size.\nTo figure out what it’s going to say next, ChatGPT breaks apart what you ask it into words and word fragments, maps each of these to a vector and stacks all these vectors together into a matrix.\nThis matrix is then passed into the first transformer block which returns a new matrix of the same size. This operation is repeated again and again.\nThe next word is the final column of its final output matrix mapped from a vector back to text.\nThen this final word is appended to the end of the input sequence and the process is repeated again and again. With one new column appended to the end of the input matrix each time.\nChatGPT slowly morphs the input you give it into the output it returns.\nConvolutional Blocks, Kernel, Activation Maps. Activations maps are stacked together to form a Tensor that become the input to the Convolutional Compute Block.\nDot Product can be thought of as a similarity score.\n\n\n\nConvolutional Block Architecture\n\n\nLatent/Embedding Space is the space of all the possible vectors that the model can output. Distance but Directionality in these latent spaces is meaningful.\nTo de-age an image for example, you can use the latent space to find the closest image to the original image and then apply a transformation (literally moving the point in the age direction) to the latent space to make the image younger/older. Then mapping the modified vector back into an image."
  },
  {
    "objectID": "learning-journey/2025-10-23.html",
    "href": "learning-journey/2025-10-23.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Deep Learning CNN: Convolutional Neural Networks with Python AI Sciences - Published by Packt Publishing\nA CNN is for any structural data. Images, Video, Audio, Text, etc.\nColours in digital screens are represented as a combination of Red, Green, and Blue in different intensities. There are three image ‘Planes’ stacked ontop of eachother.\nHigh spectral images have more stacks ontop of the basic 3. (Geospacial images from satellites for example get height maps too)\nimport numpy as np\nimport matplotlib.pyplot as plt\nim = plt.imread('../images/owl.jpg')\nplt.imshow(im)\nim.shape # (height, width, channels)\n\n(4800, 7200, 3)\nR = im[:,:,0]\nG = im[:,:,1]\nB = im[:,:,2]\nplt.imshow(R)\nplt.imshow(G)\nplt.imshow(B)\nplt.axis('off')\nplt.imshow(R, cmap='Reds')\nplt.axis('off')\nplt.imshow(R, cmap='gray')\nplt.axis('off')\nfig,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 15))\nax1.imshow(R, cmap='Reds')\nax1.axis('off')\nax2.imshow(G, cmap='Greens')\nax2.axis('off')\nax3.imshow(B, cmap='Blues')\nax3.axis('off')\nplt.show()\nmodified_im = im.copy()\nmodified_im[:,:,2] = 255 # Blue channel\nplt.imshow(modified_im)\nplt.axis('off')\nmodified_im2 = im.copy()\nmodified_im2[1000:3600,1400:3600,0] = 0 # A position in the image\nmodified_im2[1000:3600,1400:3600,1] = 255\nmodified_im2[1000:3600,1400:3600,0] = 0\nplt.imshow(modified_im2)\ngrayscale_im = 0.299 * R + 0.587 * G + 0.114 * B # not equal (0.333) values because a more realistic grayscale image would have these values\nplt.imshow(grayscale_im, cmap='gray')\nplt.axis('off')\nA filter is a function grid. To blur the image, each pixel is put through an averaging function. The filter function moves along each pixel, outputting into another image. it takes the average value of the pixels in the filter and applies it to the pixel in the image.\nIt takes all the input pixels and multiplies them by the filter values and sums them up to get the output pixel value. This is an image filtering function.\nThe output image has had convolution applied to it."
  },
  {
    "objectID": "learning-journey/2025-10-23.html#testing-knowledge",
    "href": "learning-journey/2025-10-23.html#testing-knowledge",
    "title": "Convolutional Neural Networks",
    "section": "Testing Knowledge",
    "text": "Testing Knowledge\n\nyou must have a function f_conv2d which accepts an image, and a mask.\nthis function returns the convolved image.\nno inbuilt functions for convolution.\nshould auto check if image is greyscale or rgb\nshould handle both cases\nreturn image2\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# image = plt.imread('../images/anubis.JPG')\n\n# 1. Check if image is greyscale or rgb\n\n# image.shape # this last value tells me 3 layers, so it's rgb\n\n\n# if image.shape[2] == 3: # check the channels value\n#     print(\"Image is rgb\")\n# else:\n#     print(\"Image is greyscale\")\n\n\n# mask = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\ndef f_conv2d(image, mask):\n    assert(image.ndim == 2) # Image must be 2D\n    assert(mask.ndim == 2) # Mask must be 2D\n    # if image.shape[2] == 3:\n    #     print(\"Image is rgb\")\n    # else:\n    #     print(\"Image is greyscale\")\n    image_columns = image.shape[1] # number of columns in image\n    image_rows = image.shape[0] # number of rows in image\n    mask_columns = mask.shape[1] # number of columns in mask saved in this variable\n    mask_rows = mask.shape[0] # number of rows in mask\n\n    rows_convolution_will_be_performed_on = image_rows + mask_rows - 1 # subtract 1 because new result after convolution will have one less row and column\n    columns_convolution_will_be_performed_on = image_columns + mask_columns - 1 # subtract 1 because new result after convolution will have one less row and column\n    Y = np.zeros((rows_convolution_will_be_performed_on, columns_convolution_will_be_performed_on))\n    # empty array to store the result of the convolution\n    # loop over every row and column in the image\n    for m in range(rows_convolution_will_be_performed_on):\n        for n in range(columns_convolution_will_be_performed_on):\n            for i in range(mask_rows):\n                for j in range(mask_columns):\n                    #if negative index that cannot exist... checking most things calculatable in the loop\n                    if(m-i&gt;0) and (m-i &lt; image_rows) and (n-j&gt;0) and (n-j &lt; image_columns):\n                        Y[m,n] = Y[m,n] + mask[i,j] * image[m-i, n-j] # how dot product will be completed\n    return Y                    \n\n\nmask = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\nimage = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n\nf_conv2d(image, mask) # convolution result\n\narray([[  0.,   0.,   0.,   0.,   0.],\n       [  0.,  -5., -16., -17.,  -6.],\n       [  0.,  -8., -25., -26.,  -9.],\n       [  0.,   5.,  16.,  17.,   6.],\n       [  0.,   8.,  25.,  26.,   9.]])"
  },
  {
    "objectID": "learning-journey/2025-10-24.html",
    "href": "learning-journey/2025-10-24.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "Intermediate Python - Frontend Masters - Nina Zakharenko\n\nmy_data = \"hello, I am Charlotte, I am learning Python...\"\nmy_data.split(\",\") # creates a list split by ,\n\n['hello', ' I am Charlotte', ' I am learning Python...']\n\n\n\n\",\".join(my_data) # creates a string joined by ,\n\n'h,e,l,l,o,,, ,I, ,a,m, ,C,h,a,r,l,o,t,t,e,,, ,I, ,a,m, ,l,e,a,r,n,i,n,g, ,P,y,t,h,o,n,.,.,.'\n\n\n\n\"💻\".join(my_data) # creates a string joined by 💻\n\n'h💻e💻l💻l💻o💻,💻 💻I💻 💻a💻m💻 💻C💻h💻a💻r💻l💻o💻t💻t💻e💻,💻 💻I💻 💻a💻m💻 💻l💻e💻a💻r💻n💻i💻n💻g💻 💻P💻y💻t💻h💻o💻n💻.💻.💻.'\n\n\n\nmy_data.split(\" \")\n\" \".join(my_data) # careful, this will add a space between each character\n\n'h e l l o ,   I   a m   C h a r l o t t e ,   I   a m   l e a r n i n g   P y t h o n . . .'\n\n\n\n\"\".join(my_data) # join on empty string\n\n'hello, I am Charlotte, I am learning Python...'\n\n\n\ngym_set = \"Deadlifts,Hip Thrusts,Squats\"\ngym_set.split(\",\") # creates a list split by ,\nfirst_excercise, second_excercise, third_excercise = gym_set.split(\",\")\nfirst_excercise\n\n'Deadlifts'\n\n\n\nType Conversion\n\nint(\"123\")\n\n123\n\n\n\nfloat(\"123\")\n\n123.0\n\n\n\nstr(123)\n\n'123'\n\n\n\nname = [\"Charlotte\", \"Alex\", \"Anubis\", \"Annie\", \"Anubis\", \"Charlotte\", \"Alex\", \"Annie\", \"Anubis\"]\nset(name) # creates a set of unique names. Sets CANNOT have duplicate values.\n\n{'Alex', 'Annie', 'Anubis', 'Charlotte'}\n\n\n\nsorted(set(name)) # creates a list of unique names sorted alphabetically, lists have ORDER. sets do not.\n\n['Alex', 'Annie', 'Anubis', 'Charlotte']\n\n\n\n\nList Comprehensions\n\nfor n in name: print(n.upper())\n\nCHARLOTTE\nALEX\nANUBIS\nANNIE\nANUBIS\nCHARLOTTE\nALEX\nANNIE\nANUBIS\n\n\n\nlowercase_names = []\nfor n in name: \n    lowercase_names.append(n.lower())\nprint(lowercase_names)\n\n['charlotte', 'alex', 'anubis', 'annie', 'anubis', 'charlotte', 'alex', 'annie', 'anubis']\n\n\n\n[n.upper() for n in name] # list comprehension\n# action you want to perform on each item in the list is first arg  \n\n['CHARLOTTE',\n 'ALEX',\n 'ANUBIS',\n 'ANNIE',\n 'ANUBIS',\n 'CHARLOTTE',\n 'ALEX',\n 'ANNIE',\n 'ANUBIS']\n\n\n\nlist(range(10)) # peak inside what range is doing\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nlist(range(1,4))\n\n[1, 2, 3]\n\n\n\nfor num in range(6): print(num)\n\n0\n1\n2\n3\n4\n5\n\n\n\n6 % 2 # modulo operator\n\n0\n\n\n\nfizz_buzz = []\nfor num in range(55, 74):\n    if num % 3 == 0 and num % 5 == 0:\n        fizz_buzz.append(f\"{num} FizzBuzz\")\n    elif num % 3 == 0:\n        fizz_buzz.append(f\"{num} Fizz\")\n    elif num % 5 == 0:\n        fizz_buzz.append(f\"{num} Buzz\")\n    else:\n        fizz_buzz.append(num)\nprint(fizz_buzz)\n\n['55 Buzz', 56, '57 Fizz', 58, 59, '60 FizzBuzz', 61, 62, '63 Fizz', 64, '65 Buzz', '66 Fizz', 67, 68, '69 Fizz', '70 Buzz', 71, '72 Fizz', 73]\n\n\n\n\nList Operations\n\nsquares = [num * num for num in range(10)]\nsquares\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\nsum(squares)\n\n285\n\n\n\nmax(squares)\n\n81\n\n\n\nmin(squares)\n\n0\n\n\n\nlen(squares)\n\n10\n\n\n\nsorted(squares, reverse=True)\n\n[81, 64, 49, 36, 25, 16, 9, 4, 1, 0]\n\n\n\n# get largest number in list\nlottery_numbers = \"12, 23, 34, 45, 56, 2\" \n\n\nlottery_numbers.split(\", \")\n\n['12', '23', '34', '45', '56', '2']\n\n\n\n[int(num) for num in lottery_numbers.split(\", \")]\n\n[12, 23, 34, 45, 56, 2]\n\n\n\nmax([int(num) for num in lottery_numbers.split(\", \")])\n\n56\n\n\n\n[num * num for num in range(10)] # ordered list\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n\nSet Operations\n\n{num * num for num in range(10)} # unordered set\n\n{0, 1, 4, 9, 16, 25, 36, 49, 64, 81}\n\n\n\n\nDictionaries\n\n{f\"square of {num}\": num * num for num in range(10)} # unordered set\n\n{'square of 0': 0,\n 'square of 1': 1,\n 'square of 2': 4,\n 'square of 3': 9,\n 'square of 4': 16,\n 'square of 5': 25,\n 'square of 6': 36,\n 'square of 7': 49,\n 'square of 8': 64,\n 'square of 9': 81}\n\n\n\n\nGenerator Expression\nGenerators are cruical in AI/ML, its a lazy evalution. They produce a sequence lazily, yielding one item at a time instead of building the whole list in memory.\n\nMemory efficient (stream large data)\nFast start-up (compute-on-demand)\nComposable (pipe transformations)\n\nThere’s an issue that you’re constantly pushing values to a potentially infinite queue. This takes up a load of memory.\nGenerators don’t return; they yield.\nThere’s no index positions.\nDefine what’s going to happen, only run on yield. Yielding is like running once and returning a value.\nIts faking something that is multi-threaded. “hey, let’s come back to this later”.\nAlso something in the generator could fail, and the rest of your application is unaffected - as long as the application is expecting the error.\nin range(0,5) each number is generated one by one as its looped through.\n\nfamily = [\"Annie\", \"Anubis\", \"Alex\", \"Charlotte\"]\n(len(name) for name in family)\n\n&lt;generator object &lt;genexpr&gt; at 0x105853eb0&gt;\n\n\nyou can pass a generator comprehension into a function that expects an iterable.\n\nset((len(name) for name in family))\n\n{4, 5, 6, 9}\n\n\n\nsum((len(name) for name in family))\n\n24\n\n\nTo get items out of a generator you need to iterate over it.\n\nmy_generator = (num * num for num in range(10))\n\nfor num in my_generator:\n    print(num)\n\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n\n\n\n\nzip function\n\nsquares = {f\"square of {num}\": num * num for num in range(10)}\nsquares.keys() # get the keys \n\ndict_keys(['square of 0', 'square of 1', 'square of 2', 'square of 3', 'square of 4', 'square of 5', 'square of 6', 'square of 7', 'square of 8', 'square of 9'])\n\n\n\nsquares.values() # get the values \n\ndict_values([0, 1, 4, 9, 16, 25, 36, 49, 64, 81])\n\n\n\nsquares.items() # get the key values \n\ndict_items([('square of 0', 0), ('square of 1', 1), ('square of 2', 4), ('square of 3', 9), ('square of 4', 16), ('square of 5', 25), ('square of 6', 36), ('square of 7', 49), ('square of 8', 64), ('square of 9', 81)])\n\n\n\n\nzip function\nCombine values of lists\n\ndogs = [\"Annie\", \"Anubis\"]\nages = [4, 6]\nzip(dogs, ages)\n\n&lt;zip at 0x105873740&gt;\n\n\nCannot do anything with this data structure until we loop over it\n\nfor dog in zip(dogs, ages):\n    print(dog)\n\n('Annie', 4)\n('Anubis', 6)\n\n\n\nfor name, age in zip(dogs, ages):\n    print(f\"{name} is {age} years old\")\n\nAnnie is 4 years old\nAnubis is 6 years old\n\n\n\nlist(zip(dogs, ages)) # returns a list of tuples\n\n[('Annie', 4), ('Anubis', 6)]\n\n\n\ndict(zip(dogs, ages))\n\n{'Annie': 4, 'Anubis': 6}"
  },
  {
    "objectID": "roadmap/perceptrons.html",
    "href": "roadmap/perceptrons.html",
    "title": "Perceptrons",
    "section": "",
    "text": "perceptrons do have logic gates for AND, OR, and NOT.\nHowever…there is no solution for XOR gate.\n\nUntil we thought about Networks of Perceptrons. Networked elements are required.\nThere is a need for three perceptrons to solve the XOR gate.\n\nWhen the inidividual outputs of a layer of perceptrons is not needed to be visuallised, we call it a Hidden Layer.\nOnce you begin networking the perceptions, you can perform any boolean function.\nThis is deemed a multi-layer perceptron. Perceptrons are arranged in layers.\n\nLinear Classifier\nA perceptron operates on real-valued vectors.\nThere is a boundary where all inputs are classified as 0 or 1.\n\nA perceptron defines a boundary (the line and the area) where on the graph its a 0 or 1 on each side of that linear classifier line.\nYou can create a shape with many perceptrons with their own linear classifiers. You create a boundary.\nSo you create a region where all perceptrons must output a 1.\n\nDecision Boundaries\n\n\n\nFinding and fitting a decision boundary to the data is one of the main objectives of Machine Learning\n\n\n\nConverges, Coeficiants\nFor each misclassifcation, we adjust the coeficiants to move the boundary in the direction of the misclassification. If still not classified correctly, we adjust the coeficiants again. This process is called Gradient Descent.\n\n\nTensorFlow Playground\n\nBuild a network that isolates the region within the region we wish to classify.\n\n\nIndividual perceptrons capture linear boundaries\n\n\n\n\nx_input = [0.1, 0.5, 0.2]\nw_weights = [0.4, 0.3, 0.6]\nthreshold = 0.5\n\ndef step_function(weighted_sum):\n    if weighted_sum &gt; threshold:\n        return 1\n    else:\n        return 0\n\ndef perceptron():\n    weighted_sum = 0 \n    for x,w in zip(x_input, w_weights):\n        weighted_sum += x*w\n        print(weighted_sum)\n    return step_function(weighted_sum)\n\noutput = perceptron()\nprint(\"Output:\", str(output))\n\n\n0.04000000000000001\n0.19\n0.31\nOutput: 0\n\n\nEach iteration is increasing our weighted sum, at the end we reached 0.31 which was smaller than the threshold of 0.5, therefore the output is 0\n\n\nThe perceptron is behaving like a Linear Classifier"
  },
  {
    "objectID": "roadmap/index.html",
    "href": "roadmap/index.html",
    "title": "Roadmap",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptrons\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTokenisation\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2020-07-social-prototype.html",
    "href": "projects/2020-07-social-prototype.html",
    "title": "Social media prototype",
    "section": "",
    "text": "GetStream‑based social feeds; Amplify, Serverless, Heroku, Mantine, Next.js.\nLinks:\n\nPlaceholder link"
  },
  {
    "objectID": "projects/2021-09-learning-roadmap.html",
    "href": "projects/2021-09-learning-roadmap.html",
    "title": "Engineer Learning Roadmap",
    "section": "",
    "text": "Prior: Clerk auth, Next.js + TypeScript, Payload CMS (MongoDB, PlanetScale, Prisma). Current: Docusaurus; planned progression tracking and starter code.\nLinks:\n\nPlaceholder link"
  },
  {
    "objectID": "projects/2023-01-marketplace.html",
    "href": "projects/2023-01-marketplace.html",
    "title": "Marketplace buildout",
    "section": "",
    "text": "Stack: Medusa.js · Next.js · Vendure · Qwik · Docker · Railway · PostgreSQL\nHighlights:\n\nRebuilt multiple times for DX/perf improvements\nPitched to VCs and interviewed early users\nComponentized creator tooling for speed\n\nLinks:\n\nPlaceholder link"
  },
  {
    "objectID": "blog/posts/2025-10-23-inpso.html",
    "href": "blog/posts/2025-10-23-inpso.html",
    "title": "This weeks Inspirations & Interesting Finds",
    "section": "",
    "text": "Manya’s Journal\n\nRead on Substack\n\n\n\n\n\n\n\n\nLearn to create value for other people, become deeply motivated in creating value in others.\n\n\nYou can’t solve a problem unless you can visualise it through others eyes\n\n\nIt’s addictive to make others happy. You can only go so far on motivation to ‘be better than other people’ - you’ll never be satisfied\n\n\nConstantly try to think “is there another way to do this?” - Destroy your ideas, shoot it down. 99% of your ideas are fundamentally flawed. - Is there another way to do this?\n\n\n\n\n\nVibe Coding Menu Generator - Karpathy\n\n\nAfter seeing this video, feeling thankful right now for the life I have, my worries seem insignificant."
  },
  {
    "objectID": "blog/posts/2025-10-21-brain.html",
    "href": "blog/posts/2025-10-21-brain.html",
    "title": "An llm that tweaks itself - Evolutionary based LLM",
    "section": "",
    "text": "Thinking about the complexity and scale of the problem further, a seemingly inescapable conclusion for me is that we may also need embodiment, and that the only way to build computers that can interpret scenes like we do is to allow them to get exposed to all the years of (structured, temporally coherent) experience we have, ability to interact with the world, and some magical active learning/inference architecture that I can barely even imagine when I think backwards about what it should be capable of.\n\n\nIn any case, we are very, very far and this depresses me. What is the way forward? :( Maybe I should just do a startup. I have a really cool idea for a mobile local social iPhone app.\n\n\nAndrej Kaparthy | The state of Computer Vision and AI: we are really, really far away. 2012\n\nIn the journey of building AGI, we hope to mimic the human brain.\nThe core idea is an LLM that can tweak its own parameters. An LLM that has core drivers, motivations, and goals.\nBut that’s just the basics, that’s a very simplified explainaton of how we humans grow and change.\nHow we currently interpret our environment is ridiculously complex, beautifully demonstrated in Kaparthy’s blog post. We take in so much from our environment. We almost simulate in miniature the thoughts of others,\nbut then there’s the question of how much does it matter? To accomplish goals and tasks does it need all that.\nBirds and planes ….to create an ai bird we’d consider it’s circulatory system, lungs, feathers, wings, etc. It’s eyes and the complexity of them… and then we’d consider the complexity of the brain making that in basic terms motivated to survive and knowing how to fly and where in the world its nest resides.\nHowever if our objective is to make something that flies…a paper plane does the job.\nIf our objective is to make people carriers that fly… a plane is still simpler than a bird AND there isn’t a single bird on earth that can even carry a person.\nSo point being… do we really need everything that the human brain has to offer? Or is this just a jumbled mess from natural selection to keep us alive…not the best thinking machines.\nI love this particular thing which as an atheist I like to say “The giraffe is proof that God does not exist”, I do find these creatures breathtaking to look at, however this laryngeal nerve is a large imperfection.\n\n\n\nNo engineer would ever make a mistake like that\n\n\nRemember that a designer, an engineer can go back to the drawing board; throw away the old design, start afresh with what looks more sensible. A designer has foresight. Evolution cannot go back to the drawing board, evolution has no foresight.\n\n\nRichard Dawkins\n\nThis nerve problem happened due to evolution as the giraffe was once but a small deer that would reach up to eat the leaves of the trees. Overtime the longest necks won out (they could reach the leaves other’s could not), and generations later you’ve got a deer with a ridiculously long neck.\n\n\n\nThe goal of AGI developers is to create an AI model that has the cognitive abilities of a human, including the human brain’s ability to reason, learn, and solve a wide range of tasks.\n\nSo perhaps we are always speaing about only the learning part of the brain…not the thinking part.\n\n\nPersonal Superintelligence | Mark Zuckerberg\nPlanning for AGI and beyond | OpenAI\nTaking a responsible path to AGI | DeepMind\nAi as it currently stands can be very easily manipulated"
  },
  {
    "objectID": "blog/posts/2025-11-01-dopamine.html",
    "href": "blog/posts/2025-11-01-dopamine.html",
    "title": "Dopamine Addiction - the true killer of progression",
    "section": "",
    "text": "I’ve got a theory on Dopamine and it goes like this: people nowadays are addicted to social media, tiktok, and video games (yep I’m coming for those too). I can say this because there was a time in my life I too spent from moment of waking to moment of sleeping on mobile games and social media. My life at the time was pretty shit (thanks to my own doing for most part), so the online world was far better.\nTurns out it is not only me… it is everyone I see. Most people are depressed nowadays, they don’t like socialising (try getting mates to hang out with you on short notice…its not happening). Why? they’ve used up all their dopamine for the day - flat out depleted.\nPeople used to have boring lives. Boring as shit. Your best moments were diving into books and imagining fantasy lands, and hanging out and meeting human beings, and perhaps going on walks and exploring the world around you.\nPeople ‘back in the day’ ENJOYED SCHOOL. Their lives were not constant streams of perfected, animated, hollywood esque random information tailored specific to them. People had to put in more effort to learn things and listen.\nI have on my wall a whiteboard of my principals. One of them is “I DO NOT USE TIKTOK”. Hwoever I literally downloaded it this mroning and watched for about an hour as “as reward” before deleting it for about the 100th time.\nI don’t blame myself thought or get angry at myself - this is one of the most exciting things ever created by man.\nWe are looking at generational ADDICTION. Addiction to the online.\nA portal opened in Ipswich my town center. I got this funny realisation that I was wanting to stand and see other people in different lands…hwoever those standing right next to me I couldn’t give a damn about.\nA load of strangers were more intrigued - and far more likely - to interact with people where there was a literal barrier in between them.\n\n\n\nPortal in Ipwich\n\n\nNobody waved at eachother down the street before the portal opened, and still they don’t. Hwoever they’re more than happy to wave at people they’ll never atually bump into in real life.\nWe are far more interested in meaningless online interactions than legitimate ones.\n\nWe are about to see a splitting of society\n\n\n\nWe are about to see a splitting of society, the splitting is going to be the people who live their life with completely unfiltered internet access from the second they were born, and the people that were raised by parents that kept them in moderation of screens and technology… we are already seeing the enormouse negative effects of kids who are being raised on ipads. - @betterwithhb\n\nYou don’t see that many children playing outside anymore."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Selected projects",
    "section": "",
    "text": "Hey there — I’m Charlotte, a Frontend‑focused Product Developer who loves building fast, elegant, and accessible web experiences. I dive deep, move quickly, and use strong workflows to ship pixel‑perfect UI.\nI started by asking “can I build this?” and dove head‑first into full‑stack projects. Today I lean frontend, collaborate closely with design, and sweat the details that make products feel alive.\nFocus areas - Pixel‑perfect UI and component systems - Rapid, feedback‑driven iteration - Performance, accessibility, and DX\n\n\nIpswich, UK\nLinkedIn · GitHub\n\n\n\n\nSkills\nCore: TypeScript, React, Next.js, JavaScript, UI/UX, Testing (Playwright)\nUI Systems: shadcn/ui, TailwindCSS, design systems, atomic design\nBackend/Infra: Node.js, GraphQL/REST, Serverless, AWS (S3, Lambda, DynamoDB, Amplify), Docker, Netlify\nCMS: Contentful, Payload CMS\nData/DB: MongoDB, SQL, PostgreSQL\nTooling: Git, Figma, Linear, Adobe Analytics, Prisma, Heroku, Railway\n\n\nExperience\n\n\n\n\n\n\nFrontend Engineer · Hargreaves Lansdown (03/2023 — current)\n- Led rebuild of main and dropdown menus on hl.co.uk with Next.js and Contentful\n- Built new Help Docs and News & Insights from scratch; promoted to Pensions team\n- Owned calculators and high‑traffic apps; integrated Adobe Analytics\n- Migrated UI to an internal library; TDD; sole manager of three repos\n- Mentored two apprentices; taught Playwright/Next.js; wrote technical docs\n\n\n\n\n\n\n\n\nFrontend Engineer · Munch (01/2021 — 01/2023)\n- Mobile‑first website builder with complex multi‑touch interactions\n- Rebuilt the entire menu system for responsiveness and configurability\n- Improved productivity via modular architecture and clear routing/naming\n- Migrated frontend from Craft.js to Google Web Stories in one weekend; desktop/mobile editor up within a week\n\n\nCPO · Kynk (04/2019 — 08/2020)\n- Product discovery and UX; user research into user stories and flows\n- Pitched scope and product vision to engineering team\n- Explored payments for creator‑safe platform\nWordPress Developer · Lella.co (01/2019 — 06/2019)\n- Built and maintained the site; advised on product positioning\n\n\nCurrently exploring\n\nAI companion to support autistic people (React/Next.js)\n\nIn‑browser OS (Tauri/React Native)\n\nAliveUI — rapidly composable UI library\n\nProduct planning with Figma and Linear\n\n\n\nGet in touch\nIf you’re building ambitious, design‑led products and want to ship fast with quality, let’s talk.\nsldsmls asdmlsadml\n\n\nSelected projects\nBelow is an inline listing of recent projects. For the full archive, see the Projects page.\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\nMarketplace buildout\n\n\nUltra‑fast marketplaces with non‑technical creator tools.\n\n\n\n\n\n\nEngineer Learning Roadmap\n\n\nCurated learning hub with progression tracking.\n\n\n\n\n\n\nSocial media prototype\n\n\nDeep‑dive first project; IPFS media, auth/feeds prototype.\n\n\n\n\n\n\nRealtime character rig\n\n\nMotion‑tracked 3D avatar with custom retargeting and live web integration.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tools/youtube_transcripts.html",
    "href": "tools/youtube_transcripts.html",
    "title": "Minimal transcript → .txt (no timestamps)",
    "section": "",
    "text": "Edit the VIDEO_INPUT and LANGUAGES below.\nRun the cell to write transcript.txt in the current working directory.\n\nReference: https://pypi.org/project/youtube-transcript-api/\n\n# If not installed, uncomment the next line to install\n# %pip install youtube-transcript-api\n\nimport re\nfrom youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled, CouldNotRetrieveTranscript\n\n# --- Inputs ---\nVIDEO_INPUT = \"https://www.youtube.com/watch?v=NXYrIEP1LRs\"  # YouTube URL or raw video ID\nLANGUAGES = [\"en\"]  # Try these in order\nOUTPUT_PATH = \"transcript.txt\"\n\n\ndef extract_video_id(url_or_id: str) -&gt; str:\n    s = url_or_id.strip()\n    m = (\n        re.search(r\"[?&]v=([A-Za-z0-9_-]{6,})\", s)\n        or re.search(r\"youtu\\.be/([A-Za-z0-9_-]{6,})\", s)\n        or re.search(r\"youtube\\.com/(?:embed|shorts)/([A-Za-z0-9_-]{6,})\", s)\n    )\n    return m.group(1) if m else s\n\n\ntry:\n    vid = extract_video_id(VIDEO_INPUT)\n    fetched = YouTubeTranscriptApi().fetch(vid, languages=LANGUAGES)\n    # Join snippet texts (no timestamps)\n    text = \"\\n\".join(snippet.text for snippet in fetched if snippet.text.strip())\n    if not text:\n        raise RuntimeError(\"Transcript fetched but empty.\")\n    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n        f.write(text)\n    print(f\"Saved: {OUTPUT_PATH}\")\nexcept NoTranscriptFound:\n    print(\"No transcript found for requested languages:\", LANGUAGES)\nexcept TranscriptsDisabled:\n    print(\"Transcripts are disabled for this video.\")\nexcept CouldNotRetrieveTranscript:\n    print(\"Could not retrieve transcript due to a YouTube/network error. Try again later.\")\nexcept Exception as e:\n    print(\"Error:\", e)\n\nSaved: transcript.txt"
  },
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "No items found."
  }
]