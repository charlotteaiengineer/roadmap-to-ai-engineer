[
  {
    "objectID": "bookshelf.html",
    "href": "bookshelf.html",
    "title": "Bookshelf",
    "section": "",
    "text": "Welcome to the bookshelf — a curated grid of books and courses I recommend.\n\n  AI Engineering Chip Huyen \n  Practical Guide to Python Nina Zakharenko · Frontend Masters \n    Generative AI with Python & PyTorch (2e) Babcock & Bali · O’Reilly/Packt \n    But how do AI images and videos actually work? Welch Labs"
  },
  {
    "objectID": "learning-journey/week-2025-10-14.html",
    "href": "learning-journey/week-2025-10-14.html",
    "title": "Intro to Large Language Models",
    "section": "",
    "text": "parameters bytes mathematically there is a very close relationship between prediction and compression artifact Neural Network compressed into the weights sample model inference feeding back in run the neural network - or as we say  perform inference dreaming, mimicking, hallucinating"
  },
  {
    "objectID": "learning-journey/week-2025-10-14.html#learning-outcomes",
    "href": "learning-journey/week-2025-10-14.html#learning-outcomes",
    "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/week-2025-10-14.html#todays-syllabus",
    "href": "learning-journey/week-2025-10-14.html#todays-syllabus",
    "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/week-2025-10-14.html#resources",
    "href": "learning-journey/week-2025-10-14.html#resources",
    "title": "Intro to Large Language Models",
    "section": "",
    "text": "parameters bytes mathematically there is a very close relationship between prediction and compression artifact Neural Network compressed into the weights sample model inference feeding back in run the neural network - or as we say  perform inference dreaming, mimicking, hallucinating"
  },
  {
    "objectID": "learning-journey/week-2025-10-14.html#exercises",
    "href": "learning-journey/week-2025-10-14.html#exercises",
    "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/week-2025-10-14.html#readiness-checklist",
    "href": "learning-journey/week-2025-10-14.html#readiness-checklist",
    "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/week-2025-10-14.html#examples",
    "href": "learning-journey/week-2025-10-14.html#examples",
    "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['the', 'the', 'the', 'the', 'the']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "learning-journey/index.html",
    "href": "learning-journey/index.html",
    "title": "AI Engineer Learning Journey",
    "section": "",
    "text": "This is a living, outcome-driven curriculum for becoming an AI Engineer. The dates are simply the day I began that course. Some parts can take a long time to complete, often I went back and forth into different topics to get a better understanding.\nCheck out what I’ve been learning this week, or change the ordering to ‘Oldest’ to start from scratch too.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAi Engineering\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI with Python and PyTorch — Second Edition\n\n\n\npython\n\npytorch\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\ntransformers\n\n\n\nStudy outline, exercises, and examples based on Babcock & Bali (O’Reilly, 2025).\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers and Attention\n\n\n\ntransformers\n\nattention\n\n\n\n\n\n\n\n\n\nOct 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into LLMs like ChatGPT\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Python: Beginner’s Guide\n\n\n\npython\n\ndata-structures\n\ntuples\n\nsets\n\ndictionaries\n\nfundamentals\n\n\n\nsets, tuples, dictionaries, mutability & hashing.\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learning-journey/week-2025-10-13.html",
    "href": "learning-journey/week-2025-10-13.html",
    "title": "Practical Python - Nina Zakharenko",
    "section": "",
    "text": "Understand how and when to use sets, tuples, and dictionaries\nExplain mutability vs. immutability and implications for hashing\nApply tuple unpacking safely and predictably"
  },
  {
    "objectID": "learning-journey/week-2025-10-13.html#learning-outcomes",
    "href": "learning-journey/week-2025-10-13.html#learning-outcomes",
    "title": "Practical Python - Nina Zakharenko",
    "section": "",
    "text": "Understand how and when to use sets, tuples, and dictionaries\nExplain mutability vs. immutability and implications for hashing\nApply tuple unpacking safely and predictably"
  },
  {
    "objectID": "learning-journey/week-2025-10-13.html#key-notes",
    "href": "learning-journey/week-2025-10-13.html#key-notes",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Key notes",
    "text": "Key notes\n\nEmpty set literal is set(), not {} (that’s an empty dict).\nSets are unordered, unique collections; you cannot index into a set.\nTuples are immutable; beware stray trailing commas creating tuples unexpectedly.\nTuple unpacking requires counts to match; use _ for values you do not need.\nDictionaries are keyed mappings; use in to check key existence.\nHashable typically implies immutable; lists/dicts/sets are unhashable, tuples/ints/str are hashable."
  },
  {
    "objectID": "learning-journey/week-2025-10-13.html#resources",
    "href": "learning-journey/week-2025-10-13.html#resources",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Resources",
    "text": "Resources\n\nFrontend Masters — Practical Guide to Python (full course) by Nina Zakharenko: https://frontendmasters.com/courses/practical-python/"
  },
  {
    "objectID": "learning-journey/week-2025-10-13.html#exercises",
    "href": "learning-journey/week-2025-10-13.html#exercises",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Exercises",
    "text": "Exercises\n\nPractice in REPL: construct sets from lists, perform tuple unpacking, and update dicts."
  },
  {
    "objectID": "learning-journey/week-2025-10-13.html#readiness-checklist",
    "href": "learning-journey/week-2025-10-13.html#readiness-checklist",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nCan create and manipulate sets, and explain why they are unordered\nCan define tuples and avoid accidental tuple creation via trailing commas\nCan unpack tuples defensively and handle mismatched lengths\nCan use dictionaries idiomatically (membership checks, updates)"
  },
  {
    "objectID": "learning-journey/week-2025-10-13.html#examples",
    "href": "learning-journey/week-2025-10-13.html#examples",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Examples",
    "text": "Examples\n\nNumbers and booleans\n\na = 12\nb = 3.5\ntotal = a + b\nis_equal = (a == 2)\ntruthy = True and (1 &lt; 2)\nfalsy = False or (2 &gt; 5)\nnot_val = not False\n\nprint(a, type(a))\nprint(b, type(b))\nprint(total)\nprint(is_equal, truthy, falsy, not_val)\n\n12 &lt;class 'int'&gt;\n3.5 &lt;class 'float'&gt;\n15.5\nFalse True False True\n\n\n\n\nStrings and f-strings\n\nname = \"Charlotte\"\nlang = \"Python\"\nmsg = f\"Hi {name}, welcome to {lang}!\"\n\nprint(name.upper())\nprint(len(name))\nprint(\"thon\" in lang)\nprint(msg)\n\nCHARLOTTE\n9\nTrue\nHi Charlotte, welcome to Python!\n\n\n\n\nLists\n\nnums = [1, 2, 3]\nnums.append(4)\nsliced = nums[1:3]\ndoubled = [x * 2 for x in nums]\n\nprint(nums)\nprint(sliced)\nprint(doubled)\n\n[1, 2, 3, 4]\n[2, 3]\n[2, 4, 6, 8]\n\n\n\n\nTuples\n\nt = (1, \"a\", True)\nsingle = (42,)\npacked = 1, 2  # tuple without parentheses\nx, y = (10, 20)  # unpacking\n\nprint(t)\nprint(single)\nprint(packed)\nprint(x, y)\n\n# t[0] = 99  # TypeError: 'tuple' object does not support item assignment\n\n(1, 'a', True)\n(42,)\n(1, 2)\n10 20\n\n\n\n\nDictionaries\n\nuser = {\"name\": \"Charlotte\", \"role\": \"AI Engineer\"}\nuser[\"city\"] = \"Ipswich\"\n\nprint(user[\"name\"])       # indexing by key\nprint(\"role\" in user)      # membership check on keys\nprint(user)\n\nCharlotte\nTrue\n{'name': 'Charlotte', 'role': 'AI Engineer', 'city': 'Ipswich'}\n\n\n\n\nSets\n\nnames = [\"alice\", \"bob\", \"alice\"]\ns = set(names)\ns.add(\"carol\")\n\nprint(s)\nprint(\"alice\" in s)\n\n{'alice', 'carol', 'bob'}\nTrue\n\n\n\n\nHash function and hashability\n\nprint(hash((\"a\", 1)))  # tuples are hashable if their items are hashable\nprint(hash(\"abc\"))\n\n# hash([1, 2])   # TypeError: unhashable type: 'list'\n# hash({\"k\": 1}) # TypeError: unhashable type: 'dict'\n\n-9209922719070372553\n506194112504147846\n\n\n\n\nLogic: and / or / in\n\nprint(True and False)\nprint(True or False)\nprint(False or 0)\nprint(0 or \"fallback\")\nprint(\"x\" and \"y\")      # returns last truthy operand\nprint(\"py\" in \"python\")  # substring membership\n\nFalse\nTrue\n0\nfallback\ny\nTrue\n\n\n\n\nFunctions\n\ndef my_function(x=4):\n   return x + 2\n\nmy_function()\n\n6\n\n\n\ndef my_other_function(x):\n   return x * 2\n\nmy_other_function(21312)\n\n42624\n\n\n\ndef another_cat(x, y, z=12):\n   return z + (x + y)\n\nanother_cat(1, 2)\n\n15\n\n\n\n\nConditionals\n\ndef my_dogs(x, one, two):\n    if x == 2:\n        return f\"my doggos are {one} and {two}\"\n    elif x &gt; 2:\n        return \"Soon! soon we will have them all!\"\n    else: \n        return \"Got no doggos\"\n\nmy_dogs(1, \"Annie\", \"Anubis\")\n\n'Got no doggos'\n\n\n\ndef fizzbuzz(number):\n    if (number % 3 == 0) and (number % 5 == 0):\n        print(\"fizz\")\n    else: print(\"buzz\")\n\nfizzbuzz(15)\nfizzbuzz(5)\n\nfizz\nbuzz\n\n\n\n\nLoops\n\nfamily = [\"Annie\", \"Anubis\", \"Alex\", \"Charlotte\"]\n\nfor family_member in family:\n    print(f\"My name is {family_member}!\")\n\nprint(f\"outside of the loop {family_member}\")\n\nlist(enumerate(family))\n\nMy name is Annie!\nMy name is Anubis!\nMy name is Alex!\nMy name is Charlotte!\noutside of the loop Charlotte\n\n\n[(0, 'Annie'), (1, 'Anubis'), (2, 'Alex'), (3, 'Charlotte')]\n\n\nenumerate returns list of tuples , first item index, second item the value\n\ncolours = [\"Red\", \"Yellow\", \"Pink\", \"Green\", \"Orange\", \"Purple\", \"Blue\"]\n\nfor index, colour in enumerate(colours):\n    print(f\"this is the greatest colour {colour} number {index}\")\n\nthis is the greatest colour Red number 0\nthis is the greatest colour Yellow number 1\nthis is the greatest colour Pink number 2\nthis is the greatest colour Green number 3\nthis is the greatest colour Orange number 4\nthis is the greatest colour Purple number 5\nthis is the greatest colour Blue number 6\n\n\n\nconcepts_to_learn = {\n    \"Python\": \"Language\",\n    \"TensorFlow\": \"Execution Env\",\n    \"Pytorch\": \"differnt exectuion place\",\n    \"Deep Learning\": \"Neural networks and that\"\n}\n\nfor foo in concepts_to_learn:\n    print(foo)\n\nPython\nTensorFlow\nPytorch\nDeep Learning\n\n\n\nconcepts_to_learn.items()\n\ndict_items([('Python', 'Language'), ('TensorFlow', 'Execution Env'), ('Pytorch', 'differnt exectuion place'), ('Deep Learning', 'Neural networks and that')])\n\n\n\nfor key, value in concepts_to_learn.items():\n    print(key) \n    print(\"----\")\n    print(value)\n\nPython\n----\nLanguage\nTensorFlow\n----\nExecution Env\nPytorch\n----\ndiffernt exectuion place\nDeep Learning\n----\nNeural networks and that\n\n\n\nx = 0\nwhile x &lt; 5:\n    print(x)\n    x += 1\n\n0\n1\n2\n3\n4\n\n\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\n\ndef return_target(target=\"Jeremy\"):\n    for name in names:\n        print(name)\n        if name == target:\n            print(f\"we found {target}!\")\n            return name\n\n\n\nList comprehensions\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\nmy_list = []\n\nfor name in names:\n    my_list.append(len(name))\n\nprint(\"First way: \", my_list)\n\nprint(\"Shorter way:\", [len(name) for name in names])\n\nFirst way:  [4, 3, 6, 6, 5]\nShorter way: [4, 3, 6, 6, 5]\n\n\n\nnums = [0, 1, 2, 3, 4]\n\n[num * 2 for num in nums] \n\n[0, 2, 4, 6, 8]\n\n\n\n\nSlicing\n\nmy_cake = \"Hey this is a big cake!\"\nmy_cake[14:22]\n\nmy_cake[:18]\nmy_cake[19:]\nmy_cake[-1]\n\n'!'\n\n\n\n\nfiles - reading, writing, appending, and JSON\nBelow are executable examples that will run in this page’s kernel. They demonstrate different open() modes and working with a small JSON file stored alongside this page at learning-journey/data/example.json.\n\n# Write: creates or truncates file\nwith open(\"my_file.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"first line\\n\")\n    f.write(\"second line\\n\")\n\n# Append: adds to end of file\nwith open(\"my_file.txt\", \"a\", encoding=\"utf-8\") as f:\n    f.write(\"appended line\\n\")\n\n# Read entire file\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    contents = f.read()\ncontents\n\n'first line\\nsecond line\\nappended line\\n'\n\n\n\n# Read line-by-line\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        print(line.rstrip())\n\nfirst line\nsecond line\nappended line\n\n\n\n# Explicit open/close (less preferred vs. context manager)\nf = open(\"my_file.txt\", \"r\", encoding=\"utf-8\")\ntry:\n    print(f.readline().rstrip())\nfinally:\n    f.close()\n\nfirst line\n\n\n\n# pathlib usage\nfrom pathlib import Path\n\npath = Path(\"my_file.txt\")\npath.write_text(\"overwritten via pathlib\\n\", encoding=\"utf-8\")\nprint(path.read_text(encoding=\"utf-8\"))\n\noverwritten via pathlib\n\n\n\n\n\nClasses\n\nclass Car:\n    runs = True\n\n    def start(self):\n        if self.runs:\n            print(\"The car starts.\")\n        else:\n            print(\"The car is broken.\")\n\nmy_car = Car()\nmy_car.start()\nmy_car.runs = False\nmy_car.start()\n\nmy_other_car = Car()\nmy_other_car.start()\n\nThe car starts.\nThe car is broken.\nThe car starts.\n\n\n\n\nisinstance\n\nprint(isinstance(my_car, Car))\nprint(isinstance(my_car, str))\nprint(isinstance(\"Hallo there\", str))\nprint(isinstance(12, int))\n\nTrue\nFalse\nTrue\nTrue\n\n\n\n\ninitializing classes\n\nclass Supersupercar:\n    runs = True\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def start(self):\n        if self.runs:\n            print(f\"The {self.make} {self.model} {self.year} starts.\")\n        else:\n            print(f\"The {self.make} {self.model} {self.year} is broken.\")\n\nmy_car = Supersupercar(\"Toyota\", \"Corolla\", 2020)\nmy_car.start()\n\nmy_better_car = Supersupercar(\"Mustang\", \"GT\", 2025)\nmy_better_car.start()\n\nThe Toyota Corolla 2020 starts.\nThe Mustang GT 2025 starts.\n\n\n\n\n\nMustang GT\n\n\n\n\ninheritance\n\nclass Mustang(Supersupercar):\n    def __init__(self, make, model, year, color):\n        super().__init__(make, model, year)\n        self.color = color\n\nmy_jaguar = Mustang(\"Jaguar\", \"2027\", 2027, \"Green\")\nmy_jaguar.start()\n\nThe Jaguar 2027 2027 starts.\n\n\n\n\n\nJaguar 2027\n\n\n\n\nExceptions\n\ntry:\n    print(10 / 0)\nexcept ZeroDivisionError:\n    print(\"You can't divide by zero!\")\n\ntry:\n    print(10 / 2)\nexcept ValueError:\n    print(\"You can divide by two!\")\n\nYou can't divide by zero!\n5.0\n\n\n\n\nrequests\n\n# import requests\n\n# response = requests.get(\"https://ur-api.....\")\n\n# print(response.status_code)\n# print(response.json())"
  },
  {
    "objectID": "learning-journey/week-2025-10-14-3.html",
    "href": "learning-journey/week-2025-10-14-3.html",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/week-2025-10-14-3.html#learning-outcomes",
    "href": "learning-journey/week-2025-10-14-3.html#learning-outcomes",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/week-2025-10-14-3.html#todays-syllabus",
    "href": "learning-journey/week-2025-10-14-3.html#todays-syllabus",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/week-2025-10-14-3.html#resources",
    "href": "learning-journey/week-2025-10-14-3.html#resources",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Resources",
    "text": "Resources\n\nGenerative AI with Python and PyTorch — Second Edition, Joseph Babcock & Raghav Bali, 2025: https://learning.oreilly.com/library/view/generative-ai-with/9781835884447/"
  },
  {
    "objectID": "learning-journey/week-2025-10-14-3.html#exercises",
    "href": "learning-journey/week-2025-10-14-3.html#exercises",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/week-2025-10-14-3.html#readiness-checklist",
    "href": "learning-journey/week-2025-10-14-3.html#readiness-checklist",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/week-2025-10-14-3.html#examples",
    "href": "learning-journey/week-2025-10-14-3.html#examples",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['cat', 'cat', 'the', 'the', 'the']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "learning-journey/week-2025-10-16.html",
    "href": "learning-journey/week-2025-10-16.html",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/week-2025-10-16.html#learning-outcomes",
    "href": "learning-journey/week-2025-10-16.html#learning-outcomes",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/week-2025-10-16.html#todays-syllabus",
    "href": "learning-journey/week-2025-10-16.html#todays-syllabus",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/week-2025-10-16.html#resources",
    "href": "learning-journey/week-2025-10-16.html#resources",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Resources",
    "text": "Resources\n\nGenerative AI with Python and PyTorch — Second Edition, Joseph Babcock & Raghav Bali, 2025: https://learning.oreilly.com/library/view/generative-ai-with/9781835884447/"
  },
  {
    "objectID": "learning-journey/week-2025-10-16.html#exercises",
    "href": "learning-journey/week-2025-10-16.html#exercises",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/week-2025-10-16.html#readiness-checklist",
    "href": "learning-journey/week-2025-10-16.html#readiness-checklist",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/week-2025-10-16.html#examples",
    "href": "learning-journey/week-2025-10-16.html#examples",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['cat', 'the', 'the', 'cat', 'the']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "learning-journey/2025-10-13.html",
    "href": "learning-journey/2025-10-13.html",
    "title": "Practical Python: Beginner’s Guide",
    "section": "",
    "text": "Frontend Masters — Practical Guide to Python (full course) by Nina Zakharenko: https://frontendmasters.com/courses/practical-python/\n\n\nNumbers and booleans\n\na = 12\nb = 3.5\ntotal = a + b\nis_equal = (a == 2)\ntruthy = True and (1 &lt; 2)\nfalsy = False or (2 &gt; 5)\nnot_val = not False\n\nprint(a, type(a))\nprint(b, type(b))\nprint(total)\nprint(is_equal, truthy, falsy, not_val)\n\n12 &lt;class 'int'&gt;\n3.5 &lt;class 'float'&gt;\n15.5\nFalse True False True\n\n\n\n\nStrings and f-strings\n\nname = \"Charlotte\"\nlang = \"Python\"\nmsg = f\"Hi {name}, welcome to {lang}!\"\n\nprint(name.upper())\nprint(len(name))\nprint(\"thon\" in lang)\nprint(msg)\n\nCHARLOTTE\n9\nTrue\nHi Charlotte, welcome to Python!\n\n\n\n\nLists\n\nnums = [1, 2, 3]\nnums.append(4)\nsliced = nums[1:3]\ndoubled = [x * 2 for x in nums]\n\nprint(nums)\nprint(sliced)\nprint(doubled)\n\n[1, 2, 3, 4]\n[2, 3]\n[2, 4, 6, 8]\n\n\n\n\nTuples\n\nt = (1, \"a\", True)\nsingle = (42,)\npacked = 1, 2  # tuple without parentheses\nx, y = (10, 20)  # unpacking\n\nprint(t)\nprint(single)\nprint(packed)\nprint(x, y)\n\n# t[0] = 99  # TypeError: 'tuple' object does not support item assignment\n\n(1, 'a', True)\n(42,)\n(1, 2)\n10 20\n\n\n\n\nDictionaries\n\nuser = {\"name\": \"Charlotte\", \"role\": \"AI Engineer\"}\nuser[\"city\"] = \"Ipswich\"\n\nprint(user[\"name\"])       # indexing by key\nprint(\"role\" in user)      # membership check on keys\nprint(user)\n\nCharlotte\nTrue\n{'name': 'Charlotte', 'role': 'AI Engineer', 'city': 'Ipswich'}\n\n\n\n\nSets\n\nnames = [\"alice\", \"bob\", \"alice\"]\ns = set(names)\ns.add(\"carol\")\n\nprint(s)\nprint(\"alice\" in s)\n\n{'carol', 'alice', 'bob'}\nTrue\n\n\n\n\nHash function and hashability\n\nprint(hash((\"a\", 1)))  # tuples are hashable if their items are hashable\nprint(hash(\"abc\"))\n\n# hash([1, 2])   # TypeError: unhashable type: 'list'\n# hash({\"k\": 1}) # TypeError: unhashable type: 'dict'\n\n5499565780759542809\n-1804567949909786335\n\n\n\n\nLogic: and / or / in\n\nprint(True and False)\nprint(True or False)\nprint(False or 0)\nprint(0 or \"fallback\")\nprint(\"x\" and \"y\")      # returns last truthy operand\nprint(\"py\" in \"python\")  # substring membership\n\nFalse\nTrue\n0\nfallback\ny\nTrue\n\n\n\n\nFunctions\n\ndef my_function(x=4):\n   return x + 2\n\nmy_function()\n\n6\n\n\n\ndef my_other_function(x):\n   return x * 2\n\nmy_other_function(21312)\n\n42624\n\n\n\ndef another_cat(x, y, z=12):\n   return z + (x + y)\n\nanother_cat(1, 2)\n\n15\n\n\n\n\nConditionals\n\ndef my_dogs(x, one, two):\n    if x == 2:\n        return f\"my doggos are {one} and {two}\"\n    elif x &gt; 2:\n        return \"Soon! soon we will have them all!\"\n    else: \n        return \"Got no doggos\"\n\nmy_dogs(1, \"Annie\", \"Anubis\")\n\n'Got no doggos'\n\n\n\ndef fizzbuzz(number):\n    if (number % 3 == 0) and (number % 5 == 0):\n        print(\"fizz\")\n    else: print(\"buzz\")\n\nfizzbuzz(15)\nfizzbuzz(5)\n\nfizz\nbuzz\n\n\n\n\nLoops\n\nfamily = [\"Annie\", \"Anubis\", \"Alex\", \"Charlotte\"]\n\nfor family_member in family:\n    print(f\"My name is {family_member}!\")\n\nprint(f\"outside of the loop {family_member}\")\n\nlist(enumerate(family))\n\nMy name is Annie!\nMy name is Anubis!\nMy name is Alex!\nMy name is Charlotte!\noutside of the loop Charlotte\n\n\n[(0, 'Annie'), (1, 'Anubis'), (2, 'Alex'), (3, 'Charlotte')]\n\n\nenumerate returns list of tuples , first item index, second item the value\n\ncolours = [\"Red\", \"Yellow\", \"Pink\", \"Green\", \"Orange\", \"Purple\", \"Blue\"]\n\nfor index, colour in enumerate(colours):\n    print(f\"this is the greatest colour {colour} number {index}\")\n\nthis is the greatest colour Red number 0\nthis is the greatest colour Yellow number 1\nthis is the greatest colour Pink number 2\nthis is the greatest colour Green number 3\nthis is the greatest colour Orange number 4\nthis is the greatest colour Purple number 5\nthis is the greatest colour Blue number 6\n\n\n\nconcepts_to_learn = {\n    \"Python\": \"Language\",\n    \"TensorFlow\": \"Execution Env\",\n    \"Pytorch\": \"differnt exectuion place\",\n    \"Deep Learning\": \"Neural networks and that\"\n}\n\nfor foo in concepts_to_learn:\n    print(foo)\n\nPython\nTensorFlow\nPytorch\nDeep Learning\n\n\n\nconcepts_to_learn.items()\n\ndict_items([('Python', 'Language'), ('TensorFlow', 'Execution Env'), ('Pytorch', 'differnt exectuion place'), ('Deep Learning', 'Neural networks and that')])\n\n\n\nfor key, value in concepts_to_learn.items():\n    print(key) \n    print(\"----\")\n    print(value)\n\nPython\n----\nLanguage\nTensorFlow\n----\nExecution Env\nPytorch\n----\ndiffernt exectuion place\nDeep Learning\n----\nNeural networks and that\n\n\n\nx = 0\nwhile x &lt; 5:\n    print(x)\n    x += 1\n\n0\n1\n2\n3\n4\n\n\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\n\ndef return_target(target=\"Jeremy\"):\n    for name in names:\n        print(name)\n        if name == target:\n            print(f\"we found {target}!\")\n            return name\n\n\n\nList comprehensions\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\nmy_list = []\n\nfor name in names:\n    my_list.append(len(name))\n\nprint(\"First way: \", my_list)\n\nprint(\"Shorter way:\", [len(name) for name in names])\n\nFirst way:  [4, 3, 6, 6, 5]\nShorter way: [4, 3, 6, 6, 5]\n\n\n\nnums = [0, 1, 2, 3, 4]\n\n[num * 2 for num in nums] \n\n[0, 2, 4, 6, 8]\n\n\n\n\nSlicing\n\nmy_cake = \"Hey this is a big cake!\"\nmy_cake[14:22]\n\nmy_cake[:18]\nmy_cake[19:]\nmy_cake[-1]\n\n'!'\n\n\n\n\nfiles - reading, writing, appending, and JSON\nBelow are executable examples that will run in this page’s kernel. They demonstrate different open() modes and working with a small JSON file stored alongside this page at learning-journey/data/example.json.\n\n# Write: creates or truncates file\nwith open(\"my_file.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"first line\\n\")\n    f.write(\"second line\\n\")\n\n# Append: adds to end of file\nwith open(\"my_file.txt\", \"a\", encoding=\"utf-8\") as f:\n    f.write(\"appended line\\n\")\n\n# Read entire file\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    contents = f.read()\ncontents\n\n'first line\\nsecond line\\nappended line\\n'\n\n\n\n# Read line-by-line\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        print(line.rstrip())\n\nfirst line\nsecond line\nappended line\n\n\n\n# Explicit open/close (less preferred vs. context manager)\nf = open(\"my_file.txt\", \"r\", encoding=\"utf-8\")\ntry:\n    print(f.readline().rstrip())\nfinally:\n    f.close()\n\nfirst line\n\n\n\n# pathlib usage\nfrom pathlib import Path\n\npath = Path(\"my_file.txt\")\npath.write_text(\"overwritten via pathlib\\n\", encoding=\"utf-8\")\nprint(path.read_text(encoding=\"utf-8\"))\n\noverwritten via pathlib\n\n\n\n\n\nClasses\n\nclass Car:\n    runs = True\n\n    def start(self):\n        if self.runs:\n            print(\"The car starts.\")\n        else:\n            print(\"The car is broken.\")\n\nmy_car = Car()\nmy_car.start()\nmy_car.runs = False\nmy_car.start()\n\nmy_other_car = Car()\nmy_other_car.start()\n\nThe car starts.\nThe car is broken.\nThe car starts.\n\n\n\n\nisinstance\n\nprint(isinstance(my_car, Car))\nprint(isinstance(my_car, str))\nprint(isinstance(\"Hallo there\", str))\nprint(isinstance(12, int))\n\nTrue\nFalse\nTrue\nTrue\n\n\n\n\ninitializing classes\n\nclass Supersupercar:\n    runs = True\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def start(self):\n        if self.runs:\n            print(f\"The {self.make} {self.model} {self.year} starts.\")\n        else:\n            print(f\"The {self.make} {self.model} {self.year} is broken.\")\n\nmy_car = Supersupercar(\"Toyota\", \"Corolla\", 2020)\nmy_car.start()\n\nmy_better_car = Supersupercar(\"Mustang\", \"GT\", 2025)\nmy_better_car.start()\n\nThe Toyota Corolla 2020 starts.\nThe Mustang GT 2025 starts.\n\n\n\n\n\nMustang GT\n\n\n\n\ninheritance\n\nclass Mustang(Supersupercar):\n    def __init__(self, make, model, year, color):\n        super().__init__(make, model, year)\n        self.color = color\n\nmy_jaguar = Mustang(\"Jaguar\", \"2027\", 2027, \"Green\")\nmy_jaguar.start()\n\nThe Jaguar 2027 2027 starts.\n\n\n\n\n\nJaguar 2027\n\n\n\n\nExceptions\n\ntry:\n    print(10 / 0)\nexcept ZeroDivisionError:\n    print(\"You can't divide by zero!\")\n\ntry:\n    print(10 / 2)\nexcept ValueError:\n    print(\"You can divide by two!\")\n\nYou can't divide by zero!\n5.0\n\n\n\n\nrequests\n\n# import requests\n\n# response = requests.get(\"https://ur-api.....\")\n\n# print(response.status_code)\n# print(response.json())"
  },
  {
    "objectID": "learning-journey/2025-10-13.html#learning-outcomes",
    "href": "learning-journey/2025-10-13.html#learning-outcomes",
    "title": "Practical Python - Nina Zakharenko",
    "section": "",
    "text": "Understand how and when to use sets, tuples, and dictionaries\nExplain mutability vs. immutability and implications for hashing\nApply tuple unpacking safely and predictably"
  },
  {
    "objectID": "learning-journey/2025-10-13.html#key-notes",
    "href": "learning-journey/2025-10-13.html#key-notes",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Key notes",
    "text": "Key notes\n\nEmpty set literal is set(), not {} (that’s an empty dict).\nSets are unordered, unique collections; you cannot index into a set.\nTuples are immutable; beware stray trailing commas creating tuples unexpectedly.\nTuple unpacking requires counts to match; use _ for values you do not need.\nDictionaries are keyed mappings; use in to check key existence.\nHashable typically implies immutable; lists/dicts/sets are unhashable, tuples/ints/str are hashable."
  },
  {
    "objectID": "learning-journey/2025-10-13.html#resources",
    "href": "learning-journey/2025-10-13.html#resources",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Resources",
    "text": "Resources\n\nFrontend Masters — Practical Guide to Python (full course) by Nina Zakharenko: https://frontendmasters.com/courses/practical-python/"
  },
  {
    "objectID": "learning-journey/2025-10-13.html#exercises",
    "href": "learning-journey/2025-10-13.html#exercises",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Exercises",
    "text": "Exercises\n\nPractice in REPL: construct sets from lists, perform tuple unpacking, and update dicts."
  },
  {
    "objectID": "learning-journey/2025-10-13.html#readiness-checklist",
    "href": "learning-journey/2025-10-13.html#readiness-checklist",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nCan create and manipulate sets, and explain why they are unordered\nCan define tuples and avoid accidental tuple creation via trailing commas\nCan unpack tuples defensively and handle mismatched lengths\nCan use dictionaries idiomatically (membership checks, updates)"
  },
  {
    "objectID": "learning-journey/2025-10-13.html#examples",
    "href": "learning-journey/2025-10-13.html#examples",
    "title": "Practical Python - Nina Zakharenko",
    "section": "Examples",
    "text": "Examples\n\nNumbers and booleans\n\na = 12\nb = 3.5\ntotal = a + b\nis_equal = (a == 2)\ntruthy = True and (1 &lt; 2)\nfalsy = False or (2 &gt; 5)\nnot_val = not False\n\nprint(a, type(a))\nprint(b, type(b))\nprint(total)\nprint(is_equal, truthy, falsy, not_val)\n\n12 &lt;class 'int'&gt;\n3.5 &lt;class 'float'&gt;\n15.5\nFalse True False True\n\n\n\n\nStrings and f-strings\n\nname = \"Charlotte\"\nlang = \"Python\"\nmsg = f\"Hi {name}, welcome to {lang}!\"\n\nprint(name.upper())\nprint(len(name))\nprint(\"thon\" in lang)\nprint(msg)\n\nCHARLOTTE\n9\nTrue\nHi Charlotte, welcome to Python!\n\n\n\n\nLists\n\nnums = [1, 2, 3]\nnums.append(4)\nsliced = nums[1:3]\ndoubled = [x * 2 for x in nums]\n\nprint(nums)\nprint(sliced)\nprint(doubled)\n\n[1, 2, 3, 4]\n[2, 3]\n[2, 4, 6, 8]\n\n\n\n\nTuples\n\nt = (1, \"a\", True)\nsingle = (42,)\npacked = 1, 2  # tuple without parentheses\nx, y = (10, 20)  # unpacking\n\nprint(t)\nprint(single)\nprint(packed)\nprint(x, y)\n\n# t[0] = 99  # TypeError: 'tuple' object does not support item assignment\n\n(1, 'a', True)\n(42,)\n(1, 2)\n10 20\n\n\n\n\nDictionaries\n\nuser = {\"name\": \"Charlotte\", \"role\": \"AI Engineer\"}\nuser[\"city\"] = \"Ipswich\"\n\nprint(user[\"name\"])       # indexing by key\nprint(\"role\" in user)      # membership check on keys\nprint(user)\n\nCharlotte\nTrue\n{'name': 'Charlotte', 'role': 'AI Engineer', 'city': 'Ipswich'}\n\n\n\n\nSets\n\nnames = [\"alice\", \"bob\", \"alice\"]\ns = set(names)\ns.add(\"carol\")\n\nprint(s)\nprint(\"alice\" in s)\n\n{'carol', 'bob', 'alice'}\nTrue\n\n\n\n\nHash function and hashability\n\nprint(hash((\"a\", 1)))  # tuples are hashable if their items are hashable\nprint(hash(\"abc\"))\n\n# hash([1, 2])   # TypeError: unhashable type: 'list'\n# hash({\"k\": 1}) # TypeError: unhashable type: 'dict'\n\n-1658022632611108254\n201837563434132227\n\n\n\n\nLogic: and / or / in\n\nprint(True and False)\nprint(True or False)\nprint(False or 0)\nprint(0 or \"fallback\")\nprint(\"x\" and \"y\")      # returns last truthy operand\nprint(\"py\" in \"python\")  # substring membership\n\nFalse\nTrue\n0\nfallback\ny\nTrue\n\n\n\n\nFunctions\n\ndef my_function(x=4):\n   return x + 2\n\nmy_function()\n\n6\n\n\n\ndef my_other_function(x):\n   return x * 2\n\nmy_other_function(21312)\n\n42624\n\n\n\ndef another_cat(x, y, z=12):\n   return z + (x + y)\n\nanother_cat(1, 2)\n\n15\n\n\n\n\nConditionals\n\ndef my_dogs(x, one, two):\n    if x == 2:\n        return f\"my doggos are {one} and {two}\"\n    elif x &gt; 2:\n        return \"Soon! soon we will have them all!\"\n    else: \n        return \"Got no doggos\"\n\nmy_dogs(1, \"Annie\", \"Anubis\")\n\n'Got no doggos'\n\n\n\ndef fizzbuzz(number):\n    if (number % 3 == 0) and (number % 5 == 0):\n        print(\"fizz\")\n    else: print(\"buzz\")\n\nfizzbuzz(15)\nfizzbuzz(5)\n\nfizz\nbuzz\n\n\n\n\nLoops\n\nfamily = [\"Annie\", \"Anubis\", \"Alex\", \"Charlotte\"]\n\nfor family_member in family:\n    print(f\"My name is {family_member}!\")\n\nprint(f\"outside of the loop {family_member}\")\n\nlist(enumerate(family))\n\nMy name is Annie!\nMy name is Anubis!\nMy name is Alex!\nMy name is Charlotte!\noutside of the loop Charlotte\n\n\n[(0, 'Annie'), (1, 'Anubis'), (2, 'Alex'), (3, 'Charlotte')]\n\n\nenumerate returns list of tuples , first item index, second item the value\n\ncolours = [\"Red\", \"Yellow\", \"Pink\", \"Green\", \"Orange\", \"Purple\", \"Blue\"]\n\nfor index, colour in enumerate(colours):\n    print(f\"this is the greatest colour {colour} number {index}\")\n\nthis is the greatest colour Red number 0\nthis is the greatest colour Yellow number 1\nthis is the greatest colour Pink number 2\nthis is the greatest colour Green number 3\nthis is the greatest colour Orange number 4\nthis is the greatest colour Purple number 5\nthis is the greatest colour Blue number 6\n\n\n\nconcepts_to_learn = {\n    \"Python\": \"Language\",\n    \"TensorFlow\": \"Execution Env\",\n    \"Pytorch\": \"differnt exectuion place\",\n    \"Deep Learning\": \"Neural networks and that\"\n}\n\nfor foo in concepts_to_learn:\n    print(foo)\n\nPython\nTensorFlow\nPytorch\nDeep Learning\n\n\n\nconcepts_to_learn.items()\n\ndict_items([('Python', 'Language'), ('TensorFlow', 'Execution Env'), ('Pytorch', 'differnt exectuion place'), ('Deep Learning', 'Neural networks and that')])\n\n\n\nfor key, value in concepts_to_learn.items():\n    print(key) \n    print(\"----\")\n    print(value)\n\nPython\n----\nLanguage\nTensorFlow\n----\nExecution Env\nPytorch\n----\ndiffernt exectuion place\nDeep Learning\n----\nNeural networks and that\n\n\n\nx = 0\nwhile x &lt; 5:\n    print(x)\n    x += 1\n\n0\n1\n2\n3\n4\n\n\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\n\ndef return_target(target=\"Jeremy\"):\n    for name in names:\n        print(name)\n        if name == target:\n            print(f\"we found {target}!\")\n            return name\n\n\n\nList comprehensions\n\nnames = [\"Lisa\", \"Bob\", \"Jeremy\", \"Django\", \"Mario\"]\nmy_list = []\n\nfor name in names:\n    my_list.append(len(name))\n\nprint(\"First way: \", my_list)\n\nprint(\"Shorter way:\", [len(name) for name in names])\n\nFirst way:  [4, 3, 6, 6, 5]\nShorter way: [4, 3, 6, 6, 5]\n\n\n\nnums = [0, 1, 2, 3, 4]\n\n[num * 2 for num in nums] \n\n[0, 2, 4, 6, 8]\n\n\n\n\nSlicing\n\nmy_cake = \"Hey this is a big cake!\"\nmy_cake[14:22]\n\nmy_cake[:18]\nmy_cake[19:]\nmy_cake[-1]\n\n'!'\n\n\n\n\nfiles - reading, writing, appending, and JSON\nBelow are executable examples that will run in this page’s kernel. They demonstrate different open() modes and working with a small JSON file stored alongside this page at learning-journey/data/example.json.\n\n# Write: creates or truncates file\nwith open(\"my_file.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"first line\\n\")\n    f.write(\"second line\\n\")\n\n# Append: adds to end of file\nwith open(\"my_file.txt\", \"a\", encoding=\"utf-8\") as f:\n    f.write(\"appended line\\n\")\n\n# Read entire file\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    contents = f.read()\ncontents\n\n'first line\\nsecond line\\nappended line\\n'\n\n\n\n# Read line-by-line\nwith open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        print(line.rstrip())\n\nfirst line\nsecond line\nappended line\n\n\n\n# Explicit open/close (less preferred vs. context manager)\nf = open(\"my_file.txt\", \"r\", encoding=\"utf-8\")\ntry:\n    print(f.readline().rstrip())\nfinally:\n    f.close()\n\nfirst line\n\n\n\n# pathlib usage\nfrom pathlib import Path\n\npath = Path(\"my_file.txt\")\npath.write_text(\"overwritten via pathlib\\n\", encoding=\"utf-8\")\nprint(path.read_text(encoding=\"utf-8\"))\n\noverwritten via pathlib\n\n\n\n\n\nClasses\n\nclass Car:\n    runs = True\n\n    def start(self):\n        if self.runs:\n            print(\"The car starts.\")\n        else:\n            print(\"The car is broken.\")\n\nmy_car = Car()\nmy_car.start()\nmy_car.runs = False\nmy_car.start()\n\nmy_other_car = Car()\nmy_other_car.start()\n\nThe car starts.\nThe car is broken.\nThe car starts.\n\n\n\n\nisinstance\n\nprint(isinstance(my_car, Car))\nprint(isinstance(my_car, str))\nprint(isinstance(\"Hallo there\", str))\nprint(isinstance(12, int))\n\nTrue\nFalse\nTrue\nTrue\n\n\n\n\ninitializing classes\n\nclass Supersupercar:\n    runs = True\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def start(self):\n        if self.runs:\n            print(f\"The {self.make} {self.model} {self.year} starts.\")\n        else:\n            print(f\"The {self.make} {self.model} {self.year} is broken.\")\n\nmy_car = Supersupercar(\"Toyota\", \"Corolla\", 2020)\nmy_car.start()\n\nmy_better_car = Supersupercar(\"Mustang\", \"GT\", 2025)\nmy_better_car.start()\n\nThe Toyota Corolla 2020 starts.\nThe Mustang GT 2025 starts.\n\n\n\n\n\nMustang GT\n\n\n\n\ninheritance\n\nclass Mustang(Supersupercar):\n    def __init__(self, make, model, year, color):\n        super().__init__(make, model, year)\n        self.color = color\n\nmy_jaguar = Mustang(\"Jaguar\", \"2027\", 2027, \"Green\")\nmy_jaguar.start()\n\nThe Jaguar 2027 2027 starts.\n\n\n\n\n\nJaguar 2027\n\n\n\n\nExceptions\n\ntry:\n    print(10 / 0)\nexcept ZeroDivisionError:\n    print(\"You can't divide by zero!\")\n\ntry:\n    print(10 / 2)\nexcept ValueError:\n    print(\"You can divide by two!\")\n\nYou can't divide by zero!\n5.0\n\n\n\n\nrequests\n\n# import requests\n\n# response = requests.get(\"https://ur-api.....\")\n\n# print(response.status_code)\n# print(response.json())"
  },
  {
    "objectID": "learning-journey/2025-10-14.html",
    "href": "learning-journey/2025-10-14.html",
    "title": "Intro to Large Language Models",
    "section": "",
    "text": "Mathematically there is a very close relationship between prediction and compression\nparameters bytes artifact Neural Network compressed into the weights sample model inference feeding back in perform inference\nRun the neural network - or as we say perform inference\ndreaming/mimicking/hallucinating\nIt’s parroting the training set distribution\nlossy compression of the internet\n\nTransformer Neural Net Architecture\n\n\n\nTransformer Neural Net Architecture - https://deeprevision.github.io/posts/001-transformer/\n\n\n100 billion parameters are dispersed throughout the entire Neural Net and all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task - BUT we don’t actually know what these 100 billion parameters are doing.\nWe can measure that it’s getting better to next word prediction, but we don’t know how these parameters collaborate to actually perform that\nknowledge isnt stored in traditional sense\nAll we can really measure is whether it works or not and the probability that it works.\ncome from a long process of optimization interpretability or mechanistic interpretability empirical artifacts\nWe can give them some inputs and can measure the outputs. We can measure their behaviour - this requires corresponsively sophisticated evaluations to work with these models because they’re mostly empirical (evidence based).\n\n\nTraining the assistant - Finetuning\nQuality is prefered over quantity in this stage.\nFine tuning creates an Assistant Model\nThis assisstant model now suscribes to the form of its new training documents.\nRetrieval Augmented Generation - ChatGPT can browse the files that you upload and can use them as reference inforamtion for creating its answers\nThink of LLMs as the kernel process of an emerging operating system - this process is coordiatng many different processes be they memory or computational tools for problemsolving.\nContext window suffix jailbreaking prompt injection Poisioned model/Corrupted model"
  },
  {
    "objectID": "learning-journey/2025-10-14.html#resources",
    "href": "learning-journey/2025-10-14.html#resources",
    "title": "Intro to Large Language Models",
    "section": "",
    "text": "parameters bytes\n\nmathematically there is a very close relationship between prediction and compression\n\nartifact Neural Network compressed into the weights sample model inference feeding back in perform inference\n\nrun the neural network - or as we say perform inference\n\ndreaming/mimicking/hallucinating"
  },
  {
    "objectID": "learning-journey/2025-10-16.html",
    "href": "learning-journey/2025-10-16.html",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-16.html#learning-outcomes",
    "href": "learning-journey/2025-10-16.html#learning-outcomes",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-16.html#todays-syllabus",
    "href": "learning-journey/2025-10-16.html#todays-syllabus",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/2025-10-16.html#resources",
    "href": "learning-journey/2025-10-16.html#resources",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Resources",
    "text": "Resources\n\nGenerative AI with Python and PyTorch — Second Edition, Joseph Babcock & Raghav Bali, 2025: https://learning.oreilly.com/library/view/generative-ai-with/9781835884447/"
  },
  {
    "objectID": "learning-journey/2025-10-16.html#exercises",
    "href": "learning-journey/2025-10-16.html#exercises",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/2025-10-16.html#readiness-checklist",
    "href": "learning-journey/2025-10-16.html#readiness-checklist",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/2025-10-16.html#examples",
    "href": "learning-journey/2025-10-16.html#examples",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['the', 'cat', 'the', 'cat', 'the']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "learning-journey/2025-10-14.html#data",
    "href": "learning-journey/2025-10-14.html#data",
    "title": "Intro to Large Language Models && Deep Dive into LLMs like ChatGPT",
    "section": "Data",
    "text": "Data\nFirstly data needs filtering. Loads of websites are not included from categories like adult stuff."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#data",
    "href": "learning-journey/2025-10-17.html#data",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Data",
    "text": "Data\nFirstly data needs filtering. Loads of websites are not included from categories like adult stuff."
  },
  {
    "objectID": "learning-journey/2025-10-18.html",
    "href": "learning-journey/2025-10-18.html",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-18.html#learning-outcomes",
    "href": "learning-journey/2025-10-18.html#learning-outcomes",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-18.html#todays-syllabus",
    "href": "learning-journey/2025-10-18.html#todays-syllabus",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/2025-10-18.html#resources",
    "href": "learning-journey/2025-10-18.html#resources",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Resources",
    "text": "Resources\n\nGenerative AI with Python and PyTorch — Second Edition, Joseph Babcock & Raghav Bali, 2025: https://learning.oreilly.com/library/view/generative-ai-with/9781835884447/"
  },
  {
    "objectID": "learning-journey/2025-10-18.html#exercises",
    "href": "learning-journey/2025-10-18.html#exercises",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/2025-10-18.html#readiness-checklist",
    "href": "learning-journey/2025-10-18.html#readiness-checklist",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/2025-10-18.html#examples",
    "href": "learning-journey/2025-10-18.html#examples",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['cat', 'cat', 'the', 'cat', 'cat']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#pretraining",
    "href": "learning-journey/2025-10-17.html#pretraining",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Pretraining",
    "text": "Pretraining\n\nData\nFirstly data needs filtering. Loads of websites are not included from categories like adult stuff."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-1.-pretraining",
    "href": "learning-journey/2025-10-17.html#step-1.-pretraining",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 1. Pretraining",
    "text": "Step 1. Pretraining\nDownload and preprocess the internet. We want a huge quantity of high quality documents, with large diversity. Huggingface made FineWeb which is a new, large-scale (15-trillion tokens, 44TB disk space) dataset for LLM pretraining. They used Common Crawl as their source of data. OpenAI and Anthrophic crawl themselves OpenAI Crawlers, heres Claudebot\n\n\nURL filtering - Firstly data needs filtering. Loads of websites are not included from categories like adult stuff. BLocklists are lists of urls to block.\ntext extraction - Raw HTML is what the crawlers save. We only want the text content.\nlanguage filtering - There’s a guess (using a classifier) which rules out non english pages, keeping pages that score above 65% confidence it is English.\ngopher filtering Gopher is an LLM Transformer model. The architecture is same as GPT2. It uses a huge dataset. Templates were used to prompt the model to try to stop biases in the data, sentiment analysers were used to stop biased content.\n\n\n\n\nMinhash deduplication - This is a technique to remove duplicate documents from the dataset. It’s a hashing technique that is used to identify duplicate documents.\nC4 filters\nCustom fitlers\nPII Removal - Personal Identifiable Information is detected and removed like addresses, phone numbers, emails, etc."
  },
  {
    "objectID": "learning-journey/2025-10-17.html",
    "href": "learning-journey/2025-10-17.html",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "",
    "text": "🍷 FineWeb: decanting the web for the finest text data at scale"
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-2.-tokenization",
    "href": "learning-journey/2025-10-17.html#step-2.-tokenization",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 2. Tokenization",
    "text": "Step 2. Tokenization\nThe way the technology works for these neural nets is that they expect a one dimesional sequence of symbols. They want a finite set of symbols that are possible. We have to decide what the symbols are, then have to represent our data as a one dimensional sequence of symbols.\n\nText → bytes → bits (roundtrip, compact and readable)\n\ntext = \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n\n# Text → bytes (UTF-8)\nutf8_bytes = text.encode(\"utf-8\")\nprint(\"chars:\", len(text))\nprint(\"bytes:\", len(utf8_bytes))\nprint(\"first 16 bytes (int):\", list(utf8_bytes[:16]))\nprint(\"first 16 bytes (hex):\", utf8_bytes[:16].hex())\n\n# Bytes → bits (grouped in 8)\ndef bytes_to_bits(data: bytes) -&gt; str:\n    return \" \".join(f\"{b:08b}\" for b in data)\n\nbits = bytes_to_bits(utf8_bytes)\npreview_bits = \" \".join(bits.split(\" \")[:12])  # first 12 bytes as bits\nprint(\"bits (first 12 bytes):\", preview_bits, \"...\")\nprint(\"total bits:\", len(utf8_bytes) * 8)\n\n# Bits → bytes → text (roundtrip)\ndef bits_to_bytes(bits_str: str) -&gt; bytes:\n    cleaned = bits_str.replace(\" \", \"\")\n    assert len(cleaned) % 8 == 0\n    return bytes(int(cleaned[i:i+8], 2) for i in range(0, len(cleaned), 8))\n\nroundtrip_bytes = bits_to_bytes(bits)\nprint(\"roundtrip matches bytes:\", roundtrip_bytes == utf8_bytes)\nprint(\"decoded:\", roundtrip_bytes.decode(\"utf-8\"))\n\nchars: 167\nbytes: 167\nfirst 16 bytes (int): [73, 39, 109, 32, 101, 110, 106, 111, 121, 105, 110, 103, 32, 108, 101, 97]\nfirst 16 bytes (hex): 49276d20656e6a6f79696e67206c6561\nbits (first 12 bytes): 01001001 00100111 01101101 00100000 01100101 01101110 01101010 01101111 01111001 01101001 01101110 01100111 ...\ntotal bits: 1336\nroundtrip matches bytes: True\ndecoded: I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\n\n\n\n\nPer-character view (code point → UTF‑8 bytes)\nWe want more symbols and shorter sequences. Let’s compress the binary sequence. A group of 8 bits are a byte.\n\nsample = text[:8]\nprint(f\"{'char':&lt;6}{'codepoint':&lt;12}{'hex':&lt;20}{'bin'}\")\nfor ch in sample:\n    b = ch.encode('utf-8')\n    hx = \" \".join(f\"{x:02x}\" for x in b)\n    bn = \" \".join(f\"{x:08b}\" for x in b)\n    print(f\"{repr(ch):&lt;6}{ord(ch):&lt;12}{hx:&lt;20}{bn}\")\n\nchar  codepoint   hex                 bin\n'I'   73          49                  01001001\n\"'\"   39          27                  00100111\n'm'   109         6d                  01101101\n' '   32          20                  00100000\n'e'   101         65                  01100101\n'n'   110         6e                  01101110\n'j'   106         6a                  01101010\n'o'   111         6f                  01101111\n\n\n\n\nByte Pair Encoding (BPE) — learn merges and tokenize\n\n\nWe’ll train a tiny BPE on a short corpus, learn a few merges, then tokenize a sentence.\n\nfrom collections import Counter\n\ncorpus = (\n    \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n).lower()\n\ndef build_vocab(text: str):\n    vocab = Counter()\n    for word in text.split():\n        symbols = tuple(list(word) + ['&lt;/w&gt;'])\n        vocab[symbols] += 1\n    return vocab\n\ndef get_stats(vocab: Counter):\n    pairs = Counter()\n    for symbols, freq in vocab.items():\n        for a, b in zip(symbols, symbols[1:]):\n            pairs[(a, b)] += freq\n    return pairs\n\ndef merge_vocab(pair, vocab: Counter):\n    a, b = pair\n    merged = Counter()\n    for symbols, freq in vocab.items():\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == (a, b):\n                new.append(a + b)\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        merged[tuple(new)] += freq\n    return merged\n\nvocab = build_vocab(corpus)\nmerges = []\nfor _ in range(20):  # learn up to 20 merges\n    stats = get_stats(vocab)\n    if not stats:\n        break\n    best = max(stats, key=stats.get)\n    merges.append(best)\n    vocab = merge_vocab(best, vocab)\n\nprint(\"top merges:\", merges[:10])\n\nrank = {pair: i for i, pair in enumerate(merges)}\n\ndef bpe_tokenize(word: str):\n    symbols = list(word) + ['&lt;/w&gt;']\n    while True:\n        pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols)-1)]\n        ranked = [(rank.get(p, 1e9), p) for p in pairs]\n        best_rank, best_pair = min(ranked, default=(1e9, None))\n        if best_pair is None or best_rank == 1e9:\n            break\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == best_pair:\n                new.append(symbols[i] + symbols[i+1])\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        symbols = new\n    return [s for s in symbols if s != '&lt;/w&gt;']\n\nsentence = \"Viewing single post from Spoilers of the week Lil\".lower()\nchar_tokens = sum((list(w) for w in sentence.split()), [])\nbpe_tokens = []\nfor w in sentence.split():\n    bpe_tokens.extend(bpe_tokenize(w))\n\nprint(\"char-level token count:\", len(char_tokens))\nprint(\"bpe token count:\", len(bpe_tokens))\nprint(\"bpe tokens (first 30):\", bpe_tokens[:30])\n\ntop merges: [('t', '&lt;/w&gt;'), ('n', 'd'), ('i', \"'\"), ('m', '&lt;/w&gt;'), ('i', 'n'), ('g', '&lt;/w&gt;'), ('a', 'r'), ('e', '&lt;/w&gt;'), (\"i'\", 'm&lt;/w&gt;'), ('in', 'g&lt;/w&gt;')]\nchar-level token count: 41\nbpe token count: 36\nbpe tokens (first 30): ['v', 'i', 'e', 'w', 'ing&lt;/w&gt;', 's', 'in', 'g', 'l', 'e&lt;/w&gt;', 'p', 'o', 's', 't&lt;/w&gt;', 'f', 'r', 'o', 'm&lt;/w&gt;', 's', 'p', 'o', 'i', 'l', 'e', 'r', 's&lt;/w&gt;', 'o', 'f', 'the&lt;/w&gt;', 'w']\n\n\nWe mint a symbol for each unique byte pair in the corpus. There are 100,277 symbols in GPT4\n\nGPT2 Tokenizer\n\n\n\n\nTikTokenizer\n\n\nThis token sequence is what GPT4 will ‘see’ the text as."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#text-bytes-bits-roundtrip-compact-and-readable",
    "href": "learning-journey/2025-10-17.html#text-bytes-bits-roundtrip-compact-and-readable",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Text → bytes → bits (roundtrip, compact and readable)",
    "text": "Text → bytes → bits (roundtrip, compact and readable)\n\ntext = \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n\n# Text → bytes (UTF-8)\nutf8_bytes = text.encode(\"utf-8\")\nprint(\"chars:\", len(text))\nprint(\"bytes:\", len(utf8_bytes))\nprint(\"first 16 bytes (int):\", list(utf8_bytes[:16]))\nprint(\"first 16 bytes (hex):\", utf8_bytes[:16].hex())\n\n# Bytes → bits (grouped in 8)\ndef bytes_to_bits(data: bytes) -&gt; str:\n    return \" \".join(f\"{b:08b}\" for b in data)\n\nbits = bytes_to_bits(utf8_bytes)\npreview_bits = \" \".join(bits.split(\" \")[:12])  # first 12 bytes as bits\nprint(\"bits (first 12 bytes):\", preview_bits, \"...\")\nprint(\"total bits:\", len(utf8_bytes) * 8)\n\n# Bits → bytes → text (roundtrip)\ndef bits_to_bytes(bits_str: str) -&gt; bytes:\n    cleaned = bits_str.replace(\" \", \"\")\n    assert len(cleaned) % 8 == 0\n    return bytes(int(cleaned[i:i+8], 2) for i in range(0, len(cleaned), 8))\n\nroundtrip_bytes = bits_to_bytes(bits)\nprint(\"roundtrip matches bytes:\", roundtrip_bytes == utf8_bytes)\nprint(\"decoded:\", roundtrip_bytes.decode(\"utf-8\"))\n\nchars: 167\nbytes: 167\nfirst 16 bytes (int): [73, 39, 109, 32, 101, 110, 106, 111, 121, 105, 110, 103, 32, 108, 101, 97]\nfirst 16 bytes (hex): 49276d20656e6a6f79696e67206c6561\nbits (first 12 bytes): 01001001 00100111 01101101 00100000 01100101 01101110 01101010 01101111 01111001 01101001 01101110 01100111 ...\ntotal bits: 1336\nroundtrip matches bytes: True\ndecoded: I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\n\n\n\nPer-character view (code point → UTF‑8 bytes)\n\nsample = text[:8]\nprint(f\"{'char':&lt;6}{'codepoint':&lt;12}{'hex':&lt;20}{'bin'}\")\nfor ch in sample:\n    b = ch.encode('utf-8')\n    hx = \" \".join(f\"{x:02x}\" for x in b)\n    bn = \" \".join(f\"{x:08b}\" for x in b)\n    print(f\"{repr(ch):&lt;6}{ord(ch):&lt;12}{hx:&lt;20}{bn}\")\n\nchar  codepoint   hex                 bin\n'I'   73          49                  01001001\n\"'\"   39          27                  00100111\n'm'   109         6d                  01101101\n' '   32          20                  00100000\n'e'   101         65                  01100101\n'n'   110         6e                  01101110\n'j'   106         6a                  01101010\n'o'   111         6f                  01101111\n\n\nConcatenated text:\nViewing single post from: Spoilers of the week |\\nLil| Feb 1 2023 hoping that we get some good \"SAMANTHA GEENE!!\" Marlena Death-Stares out of it. and Why not just the state. And you're still running\nTransformed into tokens of bits two symbols 0 and 1. This is two symbols and extremely long sequences:\n01010110 01101001 01100101 01110111 01101001 01101110 01100111 00100000 01110011 01101001 01101110 01100111 01101100 01100101 00100000 01110000 01101111 01110011 01110100 00100000 01100110 01110010 01101111 01101101 00111010 00100000 01010011 01110000 01101111 01101001 01101100 01100101 01110010 01110011 00100000 01101111 01100110 00100000 01110100 01101000 01100101 00100000 01110111 01100101 01100101 01101011 00100000 01111100 01011100 01101110 01001100 01101001 01101100 01111100 00100000 01000110 01100101 01100010 00100000 00110001 00100000 00110010 00110000 00110010 00110011 00100000 01101000 01101111 01110000 01101001 01101110 01100111 00100000 01110100 01101000 01100001 01110100 00100000 01110111 01100101 00100000 01100111 01100101 01110100 00100000 01110011 01101111 01101101 01100101 00100000 01100111 01101111 01101111 01100100 00100000 00100010 01010011 01000001 01001101 01000001 01001110 01010100 01001000 01000001 00100000 01000111 01000101 01000101 01001110 01000101 00100001 00100001 00100010 00100000 01001101 01100001 01110010 01101100 01100101 01101110 01100001 00100000 01000100 01100101 01100001 01110100 01101000 00101101 01010011 01110100 01100001 01110010 01100101 01110011 00100000 01101111 01110101 01110100 00100000 01101111 01100110 00100000 01101001 01110100 00101110 00100000 01100001 01101110 01100100 00100000 01010111 01101000 01111001 00100000 01101110 01101111 01110100 00100000 01101010 01110101 01110011 01110100 00100000 01110100 01101000 01100101 00100000 01110011 01110100 01100001 01110100 01100101 00101110 00100000 01000001 01101110 01100100 00100000 01111001 01101111 01110101 00100111 01110010 01100101 00100000 01110011 01110100 01101001 01101100 01101100 00100000 01110010 01110101 01101110 01101110 01101001 01101110 01100111\nWe want more symbols and shorter sequences. Let’s compress the binary sequence. A group of 8 bits are a byte."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-3.-byte-pair-encoding-bpe-learn-merges-and-tokenize",
    "href": "learning-journey/2025-10-17.html#step-3.-byte-pair-encoding-bpe-learn-merges-and-tokenize",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 3. Byte Pair Encoding (BPE) — learn merges and tokenize",
    "text": "Step 3. Byte Pair Encoding (BPE) — learn merges and tokenize\n\n\nWe’ll train a tiny BPE on a short corpus, learn a few merges, then tokenize a sentence.\n\nfrom collections import Counter\n\ncorpus = (\n    \"I'm enjoying learning about how ChatGPT works from the inside. It's pretty difficult to understand, but I'm sure I'll get the hang of it if I stick around and carry on\"\n).lower()\n\ndef build_vocab(text: str):\n    vocab = Counter()\n    for word in text.split():\n        symbols = tuple(list(word) + ['&lt;/w&gt;'])\n        vocab[symbols] += 1\n    return vocab\n\ndef get_stats(vocab: Counter):\n    pairs = Counter()\n    for symbols, freq in vocab.items():\n        for a, b in zip(symbols, symbols[1:]):\n            pairs[(a, b)] += freq\n    return pairs\n\ndef merge_vocab(pair, vocab: Counter):\n    a, b = pair\n    merged = Counter()\n    for symbols, freq in vocab.items():\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == (a, b):\n                new.append(a + b)\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        merged[tuple(new)] += freq\n    return merged\n\nvocab = build_vocab(corpus)\nmerges = []\nfor _ in range(20):  # learn up to 20 merges\n    stats = get_stats(vocab)\n    if not stats:\n        break\n    best = max(stats, key=stats.get)\n    merges.append(best)\n    vocab = merge_vocab(best, vocab)\n\nprint(\"top merges:\", merges[:10])\n\nrank = {pair: i for i, pair in enumerate(merges)}\n\ndef bpe_tokenize(word: str):\n    symbols = list(word) + ['&lt;/w&gt;']\n    while True:\n        pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols)-1)]\n        ranked = [(rank.get(p, 1e9), p) for p in pairs]\n        best_rank, best_pair = min(ranked, default=(1e9, None))\n        if best_pair is None or best_rank == 1e9:\n            break\n        i = 0\n        new = []\n        while i &lt; len(symbols):\n            if i &lt; len(symbols)-1 and (symbols[i], symbols[i+1]) == best_pair:\n                new.append(symbols[i] + symbols[i+1])\n                i += 2\n            else:\n                new.append(symbols[i])\n                i += 1\n        symbols = new\n    return [s for s in symbols if s != '&lt;/w&gt;']\n\nsentence = \"Viewing single post from Spoilers of the week Lil\".lower()\nchar_tokens = sum((list(w) for w in sentence.split()), [])\nbpe_tokens = []\nfor w in sentence.split():\n    bpe_tokens.extend(bpe_tokenize(w))\n\nprint(\"char-level token count:\", len(char_tokens))\nprint(\"bpe token count:\", len(bpe_tokens))\nprint(\"bpe tokens (first 30):\", bpe_tokens[:30])\n\ntop merges: [('t', '&lt;/w&gt;'), ('n', 'd'), ('i', \"'\"), ('m', '&lt;/w&gt;'), ('i', 'n'), ('g', '&lt;/w&gt;'), ('a', 'r'), ('e', '&lt;/w&gt;'), (\"i'\", 'm&lt;/w&gt;'), ('in', 'g&lt;/w&gt;')]\nchar-level token count: 41\nbpe token count: 36\nbpe tokens (first 30): ['v', 'i', 'e', 'w', 'ing&lt;/w&gt;', 's', 'in', 'g', 'l', 'e&lt;/w&gt;', 'p', 'o', 's', 't&lt;/w&gt;', 'f', 'r', 'o', 'm&lt;/w&gt;', 's', 'p', 'o', 'i', 'l', 'e', 'r', 's&lt;/w&gt;', 'o', 'f', 'the&lt;/w&gt;', 'w']\n\n\nWe mint a symbol for each unique byte pair in the corpus. There are 100,000 symbols in GPT4"
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-4.-llm-tokenization-in-practice-openai-tiktoken",
    "href": "learning-journey/2025-10-17.html#step-4.-llm-tokenization-in-practice-openai-tiktoken",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 4. LLM tokenization in practice (OpenAI tiktoken)",
    "text": "Step 4. LLM tokenization in practice (OpenAI tiktoken)\nThe most widely used LLM tokenizers today are byte-level BPEs with a regex pre-tokenizer and special tokens. Below we use tiktoken (OpenAI’s tokenizer) to show the actual token IDs and the byte-level pieces (including whitespace).\n\ntext = \"Viewing single post from: Spoilers of the week | Lil\\nNew line + emojis 😊🚀\"\n\ntry:\n    import tiktoken\n    enc = tiktoken.get_encoding(\"cl100k_base\")  # used by many GPT models\n\n    ids = enc.encode(text)\n    print(\"chars:\", len(text), \"tokens:\", len(ids))\n    print(\"first 20 token ids:\", ids[:20])\n\n    # Show each token as bytes → visible string; reveal whitespace/newlines\n    def viz(b: bytes) -&gt; str:\n        s = b.decode(\"utf-8\", \"replace\")\n        return s.replace(\" \", \"·\").replace(\"\\n\", \"⏎\")\n\n    pieces = [viz(enc.decode_single_token_bytes(t)) for t in ids[:40]]\n    print(\"first 40 tokens (byte-level):\")\n    print(pieces)\n\n    # Prove round-trip\n    print(\"roundtrip equal:\", enc.decode(ids) == text)\nexcept Exception as e:\n    print(\"tiktoken not installed. Install with: pip install tiktoken\")\n    print(\"Error:\", e)\n\ntiktoken not installed. Install with: pip install tiktoken\nError: No module named 'tiktoken'\n\n\nNotes:\n\nByte-level BPE means every possible byte 0–255 can be represented; no OOV tokens.\nRegex pre-tokenizer groups common patterns (e.g., whitespace, punctuation), improving merge efficiency.\nSpecial tokens (e.g., system/user separators) are reserved outside normal merges."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#step-3-neural-network-training",
    "href": "learning-journey/2025-10-17.html#step-3-neural-network-training",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Step 3: Neural Network Training",
    "text": "Step 3: Neural Network Training\nNow we are trying to predict the next token in the sequence. Currently there are 100,277 probabilities for the next token. The neural network is going to output exactly 100,277 numbers, and of those numbers, correspond to the probablility of that token as coming next in the sequence\nIn the beginning the Nural Network is randomly initialised, random probabilities. We’ve sampled this window from our dataset.\nWe know the correct next token for this sentence, so we need a mathematical process to update the weights on the network - tuning it. (Making the probability of the correct next token as high as possible, and making the other potential answers lower.)\nWe mathematically adjust the neural network so that the correct answer has a slightly higher probability.\ninput sequence tokens\niteratively updating the neural network = training the neural network."
  },
  {
    "objectID": "learning-journey/2025-10-17.html#visualization-of-the-neural-network-in-3d",
    "href": "learning-journey/2025-10-17.html#visualization-of-the-neural-network-in-3d",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Visualization of the neural network in 3D",
    "text": "Visualization of the neural network in 3D"
  },
  {
    "objectID": "learning-journey/2025-10-17.html#visualization-of-neural-networks-in-3d",
    "href": "learning-journey/2025-10-17.html#visualization-of-neural-networks-in-3d",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Visualization of neural networks in 3D",
    "text": "Visualization of neural networks in 3D"
  },
  {
    "objectID": "learning-journey/2025-10-17.html#visualization-of-llm-in-3d",
    "href": "learning-journey/2025-10-17.html#visualization-of-llm-in-3d",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Visualization of LLM in 3D",
    "text": "Visualization of LLM in 3D\nTry not to think of these LLM neurons like the ones in our brain, our biological ones have complex dynamical processes that have memory. There’sno memory in LLM neurons, it’s stateless input and output.\nThe LLM in basic terms is a mathematical function. It it parameterised by some fixed set of parameters (85,584) it is a way of transforming inputs to outputs as we twiddle the parameters we are getting different kinds of predictions, and then we need to find a good setting of these parameters so they match up with the patterns seen in the training set"
  },
  {
    "objectID": "learning-journey/2025-10-17.html#inference",
    "href": "learning-journey/2025-10-17.html#inference",
    "title": "Deep Dive into LLMs like ChatGPT",
    "section": "Inference",
    "text": "Inference\nIn inference, we are generating new data from the model. We want to see what patterns it has internalised in the paramteres of its network.\nprobability vector, probability distribution, sample a token based on the probability distribution\nWe check if we can reproduce the data in the initial training set."
  },
  {
    "objectID": "learning-journey/2025-10-20.html",
    "href": "learning-journey/2025-10-20.html",
    "title": "Transformers and Attention",
    "section": "",
    "text": "This video is very information dense and heavy and far beyond currrent full understanding. Just watch to familiarise yourself with the concepts loosely.\n\n\nAttention pattern, Weighted Sum, Dot Product, Sequence of Vectors, Multi-Headed Attention, Multilayer Perceptron\nContext is revelant to updating meanings.\nWhy is this technique as effective as it is? One lesson is that scale alone matters. Simply making things bigger and simply giving them more training data can sometimes give qualititivie improvements to the model performance.\nFor a given size of the model, for a given amount of training that you do — what’s the cost function going to look like? If that cost function is going down, that typically responds to improvements in the model performance that are qualitativley visable - a chatbot that behaves better.\nThe attention mechanism allows things to talk to eachother without sequential processing - it means they can take in the whole text passage at once, letting all the different embeddings talk to eachother as it does. This way it can effectively do more floating point operations in a given amount of time.\nYou can alos train on a huge amount of data that doesnt require human labelling. You can just give it massive amounts of data wihtout being restrained by human feedback.\nYou can Tokenise essentially anything and then embed those as vectors. This means you can have lots of distinct data types working in conjunction with each other.\n\n\n\nUnder the hood of ChatGP(Transformer)\nEach Transformer performs a set of fixed matrix operations on an input matrix of data and typically returns an output matrix of the same size.\nTo figure out what it’s going to say next, ChatGPT breaks apart what you ask it into words and word fragments, maps each of these to a vector and stacks all these vectors together into a matrix.\nThis matrix is then passed into the first transformer block which returns a new matrix of the same size. This operation is repeated again and again.\nThe next word is the final column of its final output matrix mapped from a vector back to text.\nThen this final word is appended to the end of the input sequence and the process is repeated again and again. With one new column appended to the end of the input matrix each time.\nChatGPT slowly morphs the input you give it into the output it returns.\nConvolutional Blocks, Kernel, Activation Maps. Activations maps are stacked together to form a Tensor that become the input to the Convolutional Compute Block.\nDot Product can be thought of as a similarity score.\n\n\n\nConvolutional Block Architecture\n\n\nLatent/Embedding Space is the space of all the possible vectors that the model can output. Distance but Directionality in these latent spaces is meaningful.\nTo de-age an image for example, you can use the latent space to find the closest image to the original image and then apply a transformation (literally moving the point in the age direction) to the latent space to make the image younger/older. Then mapping the modified vector back into an image."
  },
  {
    "objectID": "learning-journey/2025-10-20-1.html",
    "href": "learning-journey/2025-10-20-1.html",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-20-1.html#learning-outcomes",
    "href": "learning-journey/2025-10-20-1.html#learning-outcomes",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-20-1.html#todays-syllabus",
    "href": "learning-journey/2025-10-20-1.html#todays-syllabus",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/2025-10-20-1.html#resources",
    "href": "learning-journey/2025-10-20-1.html#resources",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Resources",
    "text": "Resources\n\nGenerative AI with Python and PyTorch — Second Edition, Joseph Babcock & Raghav Bali, 2025: https://learning.oreilly.com/library/view/generative-ai-with/9781835884447/"
  },
  {
    "objectID": "learning-journey/2025-10-20-1.html#exercises",
    "href": "learning-journey/2025-10-20-1.html#exercises",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/2025-10-20-1.html#readiness-checklist",
    "href": "learning-journey/2025-10-20-1.html#readiness-checklist",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/2025-10-20-1.html#examples",
    "href": "learning-journey/2025-10-20-1.html#examples",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['the', 'the', 'the', 'the', 'cat']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "learning-journey/2025-10-21.html",
    "href": "learning-journey/2025-10-21.html",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-21.html#learning-outcomes",
    "href": "learning-journey/2025-10-21.html#learning-outcomes",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "",
    "text": "Explain the difference between discriminative and generative models\nDescribe transformer basics (attention, positional encodings) at a high level\nCompare decoding strategies (greedy vs sampling; top-k intuition)\nApply prompt-engineering fundamentals to steer model outputs\nOutline a minimal LLM app stack (prompting, logging, tooling)"
  },
  {
    "objectID": "learning-journey/2025-10-21.html#todays-syllabus",
    "href": "learning-journey/2025-10-21.html#todays-syllabus",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Today’s syllabus",
    "text": "Today’s syllabus\n\nIntroduction to Generative AI (motivation, generative vs discriminative)\nTransformer refresher (attention and positional encoding intuition)\nText generation strategies (greedy vs sampling)\nPrompt engineering fundamentals and safety\nLightweight LLM app scaffolding and tooling overview"
  },
  {
    "objectID": "learning-journey/2025-10-21.html#resources",
    "href": "learning-journey/2025-10-21.html#resources",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Resources",
    "text": "Resources\n\nGenerative AI with Python and PyTorch — Second Edition, Joseph Babcock & Raghav Bali, 2025: https://learning.oreilly.com/library/view/generative-ai-with/9781835884447/"
  },
  {
    "objectID": "learning-journey/2025-10-21.html#exercises",
    "href": "learning-journey/2025-10-21.html#exercises",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Exercises",
    "text": "Exercises\n\nSummarize generative vs discriminative in 3 sentences with one concrete example of each.\nSketch the data flow of attention for 2 tokens (Q, K, V), labeling shapes.\nImplement a tiny decoder that picks tokens via greedy vs top-k sampling from fake logits."
  },
  {
    "objectID": "learning-journey/2025-10-21.html#readiness-checklist",
    "href": "learning-journey/2025-10-21.html#readiness-checklist",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Readiness checklist",
    "text": "Readiness checklist\n\nI can define discriminative vs generative models and give an example of each.\nI can explain attention at a high level and what positional encodings do.\nI can describe greedy vs sampling (and why top-k/top-p exist).\nI can write an effective system and user prompt for a concrete task.\nI can outline components of a minimal LLM app (prompting, logging, tooling)."
  },
  {
    "objectID": "learning-journey/2025-10-21.html#examples",
    "href": "learning-journey/2025-10-21.html#examples",
    "title": "Generative AI with Python and PyTorch — Second Edition",
    "section": "Examples",
    "text": "Examples\n\nGreedy vs sampling with softmax (no external libs)\n\nimport math, random\n\ndef softmax(logits):\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\ndef greedy_sample(logits, vocab):\n    idx = max(range(len(logits)), key=lambda i: logits[i])\n    return vocab[idx]\n\ndef top_k_sample(logits, vocab, k=2):\n    # keep top-k, renormalize, sample\n    idxs = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:k]\n    kept = [logits[i] for i in idxs]\n    probs = softmax(kept)\n    r = random.random()\n    cum = 0.0\n    for i, p in enumerate(probs):\n        cum += p\n        if r &lt;= cum:\n            return vocab[idxs[i]]\n\nvocab = [\"the\", \"a\", \"an\", \"cat\", \"dog\"]\nfake_logits = [1.0, 0.7, 0.2, 0.9, 0.6]  # pretend model scores\n\nprint(\"greedy:\", greedy_sample(fake_logits, vocab))\nprint(\"top-k (k=2):\", [top_k_sample(fake_logits, vocab, k=2) for _ in range(5)])\n\ngreedy: the\ntop-k (k=2): ['the', 'the', 'cat', 'the', 'the']\n\n\n\n\nTiny attention intuition with two tokens (toy numbers)\n\n# Two token embeddings (dim=2), toy Q,K,V projections\nimport math\n\nX = [[1.0, 0.0],  # token 1\n     [0.5, 0.5]]  # token 2\n\nW_Q = [[1.0, 0.0],[0.0, 1.0]]\nW_K = [[0.5, 0.5],[0.5, 0.5]]\nW_V = [[1.0, 0.0],[0.0, 1.0]]\n\ndef matmul(A, B):\n    return [[sum(a*b for a, b in zip(row, col)) for col in zip(*B)] for row in A]\n\nQ = matmul(X, W_Q)\nK = matmul(X, W_K)\nV = matmul(X, W_V)\n\ndef dot(a, b):\n    return sum(x*y for x, y in zip(a, b))\n\ndef attention_weights(q_i, K):\n    scores = [dot(q_i, k) for k in K]\n    m = max(scores)\n    exps = [math.exp(s - m) for s in scores]\n    Z = sum(exps)\n    return [e/Z for e in exps]\n\nweights_0 = attention_weights(Q[0], K)\nout_0 = [sum(w*v for w, v in zip(weights_0, col)) for col in zip(*V)]\n\nweights_1 = attention_weights(Q[1], K)\nout_1 = [sum(w*v for w, v in zip(weights_1, col)) for col in zip(*V)]\n\nprint(\"weights token0:\", weights_0)\nprint(\"attended token0:\", out_0)\nprint(\"weights token1:\", weights_1)\nprint(\"attended token1:\", out_1)\n\nweights token0: [0.5, 0.5]\nattended token0: [0.75, 0.25]\nweights token1: [0.5, 0.5]\nattended token1: [0.75, 0.25]\n\n\n\n\nPrompt templating (system + user)\n\ntask = \"Summarize discriminative vs generative models with examples.\"\nsystem = \"You are a precise assistant. Keep answers under 100 words.\"\nuser = f\"Task: {task}\\nConstraints: 3 sentences max.\"\n\nprint(\"SYSTEM:\\n\" + system)\nprint(\"USER:\\n\" + user)\n\nSYSTEM:\nYou are a precise assistant. Keep answers under 100 words.\nUSER:\nTask: Summarize discriminative vs generative models with examples.\nConstraints: 3 sentences max."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A roadmap to becoming an AI Engineer",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAI Engineer Learning Journey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAi Engineering\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI with Python and PyTorch — Second Edition\n\n\n\npython\n\npytorch\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\ntransformers\n\n\n\nStudy outline, exercises, and examples based on Babcock & Bali (O’Reilly, 2025).\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers and Attention\n\n\n\ntransformers\n\nattention\n\n\n\n\n\n\n\n\n\nOct 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into LLMs like ChatGPT\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Large Language Models\n\n\n\ndeep-learning\n\ngenerative-ai\n\nllm\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Python: Beginner’s Guide\n\n\n\npython\n\ndata-structures\n\ntuples\n\nsets\n\ndictionaries\n\nfundamentals\n\n\n\nsets, tuples, dictionaries, mutability & hashing.\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "prophecy.engineer",
    "section": "",
    "text": "Hey there — I’m Charlotte Bondarev, a Frontend‑focused Product Developer who loves building fast, elegant, and accessible web experiences. I dive deep, move quickly, and use strong workflows to ship pixel‑perfect UI.\nI started by asking “can I build this?” and dove head‑first into full‑stack projects. Today I lean frontend, collaborate closely with design, and sweat the details that make products feel alive.\nFocus areas - Pixel‑perfect UI and component systems - Rapid, feedback‑driven iteration - Performance, accessibility, and DX\n\n\nIpswich, UK\nLinkedIn · GitHub\n\n\n\n\nSkills\nCore: TypeScript, React, Next.js, JavaScript, UI/UX, Testing (Playwright)\nUI Systems: shadcn/ui, TailwindCSS, design systems, atomic design\nBackend/Infra: Node.js, GraphQL/REST, Serverless, AWS (S3, Lambda, DynamoDB, Amplify), Docker, Netlify\nCMS: Contentful, Payload CMS\nData/DB: MongoDB, SQL, PostgreSQL\nTooling: Git, Figma, Linear, Adobe Analytics, Prisma, Heroku, Railway\n\n\nExperience\nFrontend Engineer · Hargreaves Lansdown (03/2023 — current)\n- Led rebuild of main and dropdown menus on hl.co.uk with Next.js and Contentful\n- Built new Help Docs and News & Insights from scratch; promoted to Pensions team\n- Owned calculators and high‑traffic apps; integrated Adobe Analytics\n- Migrated UI to an internal library; TDD; sole manager of three repos\n- Mentored two apprentices; taught Playwright/Next.js; wrote technical docs\nFrontend Engineer · Munch (01/2021 — 01/2023)\n- Mobile‑first website builder with complex multi‑touch interactions\n- Rebuilt the entire menu system for responsiveness and configurability\n- Improved productivity via modular architecture and clear routing/naming\n- Migrated frontend from Craft.js to Google Web Stories in one weekend; desktop/mobile editor up within a week\nCPO · Kynk (04/2019 — 08/2020)\n- Product discovery and UX; user research into user stories and flows\n- Pitched scope and product vision to engineering team\n- Explored payments for creator‑safe platform\nWordPress Developer · Lella.co (01/2019 — 06/2019)\n- Built and maintained the site; advised on product positioning\n\n\nSelected projects\nMarketplace buildout — ultra‑fast marketplaces with non‑technical creator tools\nStack: Medusa.js + Next.js; Vendure; later Qwik; Docker; Railway; PostgreSQL\n- Rebuilt three times to optimize DX/perf; pitched to VCs; interviewed early users\nEngineer Learning Roadmap — curated learning hub\nPrior: Clerk auth, Next.js + TypeScript, Payload CMS (MongoDB, PlanetScale, Prisma)\nCurrent: Docusaurus; planned progression tracking and starter code\nSocial media prototype — deep dive first project\nGetStream‑based social feeds; Amplify, Serverless, Heroku, Mantine, Next.js\n- Solved media‑hosting costs via IPFS/web3.storage; working prototype with auth/feeds\nRealtime character rig — motion‑tracked 3D avatar\nUnity, Blender, Rokoko; custom retargeting, facial rig mapping, live web integration\n\n\nCurrently exploring\n\nAI companion to support autistic people (React/Next.js)\n\nIn‑browser OS (Tauri/React Native)\n\nAliveUI — rapidly composable UI library\n\nProduct planning with Figma and Linear\n\n\n\nGet in touch\nIf you’re building ambitious, design‑led products and want to ship fast with quality, let’s talk."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog. I write about frontend engineering, product development, and building with AI.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\nNo matching items"
  }
]