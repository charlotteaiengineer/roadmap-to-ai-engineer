---
title: "Lecture 2 - Multi-Layer Perceptrons"
date: 2025-10-27
jupyter: python3
categories: []
image: https://cdn-images-1.medium.com/v2/resize:fit:1400/1*qA_APGgbbh0QfRNsRyMaJg.png
---
# Perceptrons

<iframe width="560" height="315" src="https://www.youtube.com/embed/NXYrIEP1LRs?si=95pjt-OfNECOQArL&amp;start=3534" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

![](/images/logicgates.png)

perceptrons do have logic gates for AND, OR, and NOT. 

However...there is no solution for XOR gate.

![](/images/xor.png)

Until we thought about **Networks of Perceptrons**. Networked elements are required.

There is a need for three perceptrons to solve the XOR gate.

![](https://cdn-images-1.medium.com/v2/resize:fit:1400/1*qA_APGgbbh0QfRNsRyMaJg.png)

When the inidividual outputs of a layer of perceptrons is not needed to be visuallised, we call it a **Hidden Layer**.

Once you begin networking the perceptions, you can perform any boolean function. 

This is deemed a multi-layer perceptron. Perceptrons are arranged in layers.

### Linear Classifier

A perceptron operates on real-valued vectors.

There is a boundary where all inputs are classified as 0 or 1.

![](https://i.sstatic.net/epSZC.png)

A perceptron defines a boundary (the line and the area) where on the graph its a 0 or 1 on each side of that linear classifier line.

You can create a shape with many perceptrons with their own linear classifiers. You create a **boundary**.

So you create a region where all perceptrons must output a 1.

#### Decision Boundaries

<iframe width="560" height="315" src="https://www.youtube.com/embed/KJJFfRXFbuc?si=FMy6iF6RVHI2bz0D" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

> Finding and fitting a decision boundary to the data is one of the main objectives of Machine Learning

<iframe width="560" height="315" src="https://www.youtube.com/embed/4Gac5I64LM4?si=7oRb00AooUKyx3qE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

`Converges`, `Coeficiants`

For each misclassifcation, we adjust the coeficiants to move the boundary in the direction of the misclassification. If still not classified correctly, we adjust the coeficiants again. This process is called `Gradient Descent`.

#### [TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.89612&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)

> Build a network that isolates the region within the region we wish to classify.

> Individual perceptrons capture linear boundaries