---
title: "Transformers and Attention"
date: 2025-10-20
jupyter: python3
categories: [transformers, attention]
image: https://i.ytimg.com/vi/KJtZARuO3JY/maxresdefault.jpg
---

This video is very information dense and heavy and far beyond currrent full understanding. Just watch to familiarise yourself with the concepts loosely.

<iframe width="560" height="315" src="https://www.youtube.com/embed/KJtZARuO3JY?si=OPYkvBBVcuYc5muq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

`Attention pattern`, `Weighted Sum`, `Dot Product`, `Sequence of Vectors`, `Multi-Headed Attention`, `Multilayer Perceptron`

Context is revelant to updating meanings.

Why is this technique as effective as it is? One lesson is that **scale alone matters**. Simply making things bigger and simply giving them more training data can sometimes give **qualititivie improvements to the model performance**.

For a given size of the model, for a given amount of training that you do --- what's the cost function going to look like? If that cost function is going down, that typically responds to improvements in the model performance that are qualitativley visable - a chatbot that behaves better.

The attention mechanism allows things to talk to eachother without sequential processing - it means they can take in the whole text passage at once, letting all the different embeddings talk to eachother as it does. 
This way it can effectively do **more floating point operations in a given amount of time**.

You can alos train on a huge amount of data that doesnt require human labelling. You can just give it massive amounts of data wihtout being restrained by human feedback.

**You can Tokenise essentially anything and then embed those as vectors. This means you can have lots of distinct data types working in conjunction with each other.**


<iframe width="560" height="315" src="https://www.youtube.com/embed/UZDiGooFs54?si=5tcj1lYP05y_5w9z" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

#### Under the hood of ChatGP(Transformer)

**Each Transformer performs a set of fixed matrix operations on an input matrix of data and typically returns an output matrix of the same size.**

To figure out what it's going to say next, ChatGPT breaks apart what you ask it into words and word fragments, **maps each of these to a vector** and stacks all these vectors together into a matrix.

This matrix is then passed into the first transformer block which returns a new matrix of the same size. This operation is repeated again and again. 

The next word is the final column of its final output matrix mapped from a vector back to text.

Then this final word is appended to the end of the input sequence and the process is repeated again and again. With one new column appended to the end of the input matrix each time.

ChatGPT slowly morphs the input you give it into the output it returns.

`Convolutional Blocks`, `Kernel`, `Activation Maps`. Activations maps are stacked together to form a `Tensor` that become the input to the Convolutional Compute Block.

Dot Product can be thought of as a similarity score.

![Convolutional Block Architecture](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzsy3fP0QZLP53OxNTQPF66jPfXXxokJunOA1UuLf6qpJyt1d29NDEnlFVXyz8H-13RFNAf120mg6MkrWH_Tkl1UgEOqFcRwg_lPPJ4p2T4NaGuZvzUeuSOIZf6U62Ma7Y2-Ma5xQvAUTSYQ7qooYsbuo0E-QWR6GG0HpkDYs12UsfpBg58gSEDZfiyHwF/w582-h328-rw/CNN%20Architecutre%20(1).png)

`Latent/Embedding Space` is the space of all the possible vectors that the model can output. Distance but Directionality in these latent spaces is meaningful.

To de-age an image for example, you can use the latent space to find the closest image to the original image and then apply a transformation (literally moving the point in the age direction) to the latent space to make the image younger/older. Then mapping the modified vector back into an image.

